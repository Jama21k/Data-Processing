{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:58:27.778970Z",
     "iopub.status.busy": "2025-04-15T12:58:27.778601Z",
     "iopub.status.idle": "2025-04-15T12:58:27.784802Z",
     "shell.execute_reply": "2025-04-15T12:58:27.783818Z",
     "shell.execute_reply.started": "2025-04-15T12:58:27.778946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"GNN Model.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1pSO-VEyn5Cywjw9sXKn2fjXdVbI3hAsH\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Untitled2.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/193StgLnr4doKklAxwBiQsVX3njEfb1oa\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\hu4227mo-s\\OneDrive - Lund University\\Merged_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:58:32.355467Z",
     "iopub.status.busy": "2025-04-15T12:58:32.354707Z",
     "iopub.status.idle": "2025-04-15T12:58:36.162939Z",
     "shell.execute_reply": "2025-04-15T12:58:36.161897Z",
     "shell.execute_reply.started": "2025-04-15T12:58:32.355432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: vmdpy in c:\\users\\hu4227mo-s\\appdata\\roaming\\python\\python312\\site-packages (0.2)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from vmdpy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install vmdpy\n",
    "from vmdpy import VMD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:01:50.336244Z",
     "iopub.status.busy": "2025-04-15T13:01:50.335695Z",
     "iopub.status.idle": "2025-04-15T13:01:50.818103Z",
     "shell.execute_reply": "2025-04-15T13:01:50.817094Z",
     "shell.execute_reply.started": "2025-04-15T13:01:50.336204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/vmdpy')  # Adjust the path if needed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Check for GPU and set device appropriately for Kaggle T4\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set optimized CUDA options for T4 GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False  # Better performance but less reproducible\n",
    "    # Set to float16 precision for faster computation on T4 GPU\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# ============================================================\n",
    "# MODIFIED DATA LOADING AND PROCESSING FOR TARGETED SEASONAL ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load weather data, clean missing values, and filter to San Jose only.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine file type and read\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    # Convert timestamp\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    elif 'DATE' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['DATE'])\n",
    "        df = df.rename(columns={'DATE': 'date_original'})\n",
    "\n",
    "    # Ensure time-based ordering before interpolation\n",
    "    df = df.sort_values(by='timestamp')\n",
    "\n",
    "    # Add hour of day feature - sine/cosine encoding for cyclical pattern\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "\n",
    "    # Add day of year feature - sine/cosine encoding for cyclical pattern\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n",
    "\n",
    "    # Define region mapping for LA Downtown only\n",
    "    region_mapping = {\n",
    "        'San Jose': 'urban'\n",
    "    }\n",
    "\n",
    "    # Filter to keep only LA Downtown\n",
    "    if 'station_id' in df.columns:\n",
    "        # Filter to LA Downtown only\n",
    "        df = df[df['station_id'] == 'San Jose']\n",
    "        \n",
    "        # Add region information\n",
    "        df['region'] = df['station_id'].map(region_mapping)\n",
    "\n",
    "        # Convert region to numerical encoding\n",
    "        region_to_num = {region: i for i, region in enumerate(df['region'].unique())}\n",
    "        df['region_code'] = df['region'].map(region_to_num)\n",
    "\n",
    "        # Add elevation for LA Downtown\n",
    "        elevation_mapping = {\n",
    "            'San Jose': 93         # meters\n",
    "        }\n",
    "        df['elevation'] = df['station_id'].map(elevation_mapping)\n",
    "        # Normalize elevation (will be constant for single station)\n",
    "        df['elevation_norm'] = 0.0  # Since we only have one station, just use 0 as normalized value\n",
    "\n",
    "        print(f\"Filtered data to San Jose only\")\n",
    "\n",
    "    # Interpolate missing values along the time dimension\n",
    "    df.interpolate(method='linear', limit_direction='both', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# VMD DECOMPOSITION\n",
    "# ============================================================\n",
    "\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def perform_vmd_decomposition(signal, alpha=2000, tau=0, K=10, DC=0, init=1, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Perform VMD decomposition on a signal.\n",
    "    \"\"\"\n",
    "    # Run VMD\n",
    "    u, u_hat, omega = VMD(signal, alpha, tau, K, DC, init, tol)\n",
    "    \n",
    "    # Calculate reconstruction residual\n",
    "    recon = np.sum(u, axis=0)\n",
    "    rres = np.sqrt(np.mean((signal - recon)**2)) / np.sqrt(np.mean(signal**2)) * 100\n",
    "    \n",
    "    return u, rres\n",
    "\n",
    "def find_optimal_k(signal, k_range=range(6, 15)):\n",
    "    \"\"\"\n",
    "    Find optimal K where rres < 3% with no sharp drop.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        print(f\"  Testing K={k}\")\n",
    "        _, rres = perform_vmd_decomposition(signal, K=k)\n",
    "        results.append((k, rres))\n",
    "        print(f\"  K={k}, rres={rres:.2f}%\")\n",
    "        if rres < 3.0:\n",
    "            return k, rres\n",
    "            \n",
    "    # If no K achieves rres < 3%, return the K with lowest rres\n",
    "    return min(results, key=lambda x: x[1])\n",
    "\n",
    "def decompose_station_data(data, feature_name, station_id, cache_dir='vmd_cache'):\n",
    "    \"\"\"\n",
    "    Decompose time series data for a single station.\n",
    "    \"\"\"\n",
    "    print(f\"Starting decomposition for {station_id} - {feature_name}\")\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a unique cache filename\n",
    "    cache_file = f\"{cache_dir}/vmd_{station_id}_{feature_name}.joblib\"\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading cached VMD decomposition for {station_id} - {feature_name}\")\n",
    "        return joblib.load(cache_file)\n",
    "    \n",
    "    # Get the time series to decompose\n",
    "    print(f\"  Extracting signal for {station_id}\")\n",
    "    signal = data[feature_name].values\n",
    "    \n",
    "    # Normalize signal\n",
    "    print(f\"  Normalizing signal\")\n",
    "    signal_norm = (signal - np.mean(signal)) / np.std(signal)\n",
    "    \n",
    "    # Find optimal K\n",
    "    print(f\"  Finding optimal K\")\n",
    "    k_opt, rres = find_optimal_k(signal_norm)\n",
    "    print(f\"Station {station_id}, feature {feature_name}: Optimal K={k_opt}, rres={rres:.2f}%\")\n",
    "    \n",
    "    # Perform VMD with optimal K\n",
    "    print(f\"  Performing VMD with K={k_opt}\")\n",
    "    u, _ = perform_vmd_decomposition(signal_norm, K=k_opt)\n",
    "    \n",
    "    # Denormalize modes\n",
    "    print(f\"  Denormalizing modes\")\n",
    "    u_denorm = u * np.std(signal) + np.mean(signal) / k_opt\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"  Saving to cache: {cache_file}\")\n",
    "    joblib.dump(u_denorm, cache_file)\n",
    "    \n",
    "    print(f\"Completed decomposition for {station_id} - {feature_name}\")\n",
    "    return u_denorm\n",
    "\n",
    "def parallel_vmd_decomposition(df, feature_cols_to_decompose=['Temperature_C']):\n",
    "    \"\"\"\n",
    "    Apply VMD decomposition to multiple stations serially to avoid multiprocessing issues.\n",
    "    \"\"\"\n",
    "    # Get the unique station IDs \n",
    "    station_ids = ['San Jose']  # We only have LA Downtown in this code\n",
    "    \n",
    "    decomposed_data = {}\n",
    "    \n",
    "    for feature in feature_cols_to_decompose:\n",
    "        print(f\"Performing VMD decomposition for feature: {feature}\")\n",
    "        \n",
    "        # Process each station serially instead of in parallel\n",
    "        results = []\n",
    "        for station_id in station_ids:\n",
    "            print(f\"  Processing station: {station_id}\")\n",
    "            result = decompose_station_data(df, feature, station_id)\n",
    "            results.append(result)\n",
    "            \n",
    "        # Store results\n",
    "        for i, station_id in enumerate(station_ids):\n",
    "            if station_id not in decomposed_data:\n",
    "                decomposed_data[station_id] = {}\n",
    "            decomposed_data[station_id][feature] = results[i]\n",
    "    \n",
    "    return decomposed_data\n",
    "\n",
    "def extract_target_days(df):\n",
    "    \"\"\"\n",
    "    Extract the 4 specific target days (one per season) for predictions\n",
    "    \"\"\"\n",
    "    # Define target days\n",
    "    target_days = [\n",
    "        {'season': 'Spring', 'month': 4, 'day': 15},  # April 15\n",
    "        {'season': 'Summer', 'month': 7, 'day': 20},  # July 20\n",
    "        {'season': 'Fall', 'month': 10, 'day': 10},   # October 10\n",
    "        {'season': 'Winter', 'month': 1, 'day': 15}   # January 15\n",
    "    ]\n",
    "\n",
    "    # Filter for each target day\n",
    "    target_data = {}\n",
    "    for target in target_days:\n",
    "        # Filter by month and day\n",
    "        day_data = df[(df['timestamp'].dt.month == target['month']) &\n",
    "                       (df['timestamp'].dt.day == target['day'])]\n",
    "\n",
    "        # Get the most recent year that has data for this day\n",
    "        if not day_data.empty:\n",
    "            latest_year = day_data['timestamp'].dt.year.max()\n",
    "            target_day_data = day_data[day_data['timestamp'].dt.year == latest_year]\n",
    "            target_data[target['season']] = target_day_data\n",
    "            print(f\"Found {len(target_day_data)} records for {target['season']} target day ({target['month']}/{target['day']}/{latest_year})\")\n",
    "        else:\n",
    "            print(f\"WARNING: No data found for {target['season']} target day\")\n",
    "\n",
    "    return target_data\n",
    "\n",
    "def prepare_seasonal_training_data(df, target_days):\n",
    "    \"\"\"\n",
    "    For each target day, prepare all historical data for training\n",
    "    \"\"\"\n",
    "    training_sets = {}\n",
    "\n",
    "    for season, target_day_data in target_days.items():\n",
    "        if target_day_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Get the date of this target\n",
    "        sample_date = target_day_data['timestamp'].iloc[0]\n",
    "        target_year = sample_date.year\n",
    "\n",
    "        # Use all historical data prior to the target year\n",
    "        historical_data = df[df['timestamp'].dt.year < target_year]\n",
    "\n",
    "        training_sets[season] = historical_data\n",
    "        print(f\"{season} training set: {len(historical_data)} samples from all historical data\")\n",
    "\n",
    "    return training_sets\n",
    "\n",
    "def split_data_by_years(df):\n",
    "    \"\"\"\n",
    "    Split the data into training, validation and test sets based on years.\n",
    "    - Training: 2021-2022\n",
    "    - Validation: 2023\n",
    "    - Test: 2024\n",
    "    \"\"\"\n",
    "    print(\"Splitting data by years...\")\n",
    "    \n",
    "    # Create the splits\n",
    "    train_df = df[(df['timestamp'].dt.year >= 2021) & (df['timestamp'].dt.year <= 2022)]\n",
    "    val_df = df[df['timestamp'].dt.year == 2023]\n",
    "    test_df = df[df['timestamp'].dt.year == 2024]\n",
    "    \n",
    "    # Print split sizes\n",
    "    print(f\"Training data (2021-2022): {len(train_df)} samples\")\n",
    "    print(f\"Validation data (2023): {len(val_df)} samples\")\n",
    "    print(f\"Test data (2024): {len(test_df)} samples\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def normalize_features(train_df, val_df, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalize features using StandardScaler fitted on training data\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit on training data\n",
    "    scaler.fit(train_df[feature_cols])\n",
    "\n",
    "    # Transform datasets\n",
    "    train_scaled = scaler.transform(train_df[feature_cols])\n",
    "    val_scaled = scaler.transform(val_df[feature_cols])\n",
    "\n",
    "    # Convert back to DataFrames\n",
    "    train_norm = pd.DataFrame(train_scaled, columns=feature_cols, index=train_df.index)\n",
    "    val_norm = pd.DataFrame(val_scaled, columns=feature_cols, index=val_df.index)\n",
    "\n",
    "    return train_norm, val_norm, scaler\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for weather forecasting with sliding window approach.\n",
    "    Modified to work efficiently with single station San Jose.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, station_ids, feature_cols, seq_length=24, forecast_horizon=24):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with weather data (filtered to San Jose only)\n",
    "            station_ids: List containing only 'San Jose'\n",
    "            feature_cols: List of feature columns to use as input\n",
    "            seq_length: Length of input sequence (in hours)\n",
    "            forecast_horizon: How many hours ahead to predict\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.station_ids = station_ids\n",
    "        self.feature_cols = feature_cols\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.n_stations = len(station_ids)  # Should be 1\n",
    "\n",
    "        # Get unique timestamps (now all from one station)\n",
    "        self.timestamps = sorted(df['timestamp'].unique())\n",
    "\n",
    "        # Filter valid timestamps (those that have enough history and future data)\n",
    "        valid_idx = []\n",
    "        for i in range(len(self.timestamps) - (seq_length + forecast_horizon - 1)):\n",
    "            # Check if we have continuous data for this window\n",
    "            current_time = self.timestamps[i]\n",
    "            end_time = self.timestamps[i + seq_length + forecast_horizon - 1]\n",
    "            expected_duration = timedelta(hours=seq_length + forecast_horizon - 1)\n",
    "\n",
    "            if (end_time - current_time) == expected_duration:\n",
    "                valid_idx.append(i)\n",
    "\n",
    "        self.valid_indices = valid_idx\n",
    "\n",
    "        # Add a safety check to ensure we have at least one valid window\n",
    "        if len(self.valid_indices) == 0:\n",
    "            print(f\"WARNING: No valid windows found in dataset. Using reduced requirements.\")\n",
    "            # Fall back to allowing any windows where we have both input and output data\n",
    "            valid_idx = []\n",
    "            for i in range(len(self.timestamps) - seq_length):\n",
    "                if i + seq_length < len(self.timestamps):\n",
    "                    valid_idx.append(i)\n",
    "            self.valid_indices = valid_idx\n",
    "            self.fallback_mode = True\n",
    "            print(f\"Found {len(self.valid_indices)} windows with relaxed continuity requirements\")\n",
    "        else:\n",
    "            self.fallback_mode = False\n",
    "            print(f\"Created dataset with {len(self.valid_indices)} valid windows\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.valid_indices))  # Ensure length is at least 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.valid_indices) == 0:\n",
    "            # Return dummy data if no valid indices\n",
    "            X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))\n",
    "            y = np.zeros((self.n_stations, self.forecast_horizon))\n",
    "            static_features = np.zeros((self.n_stations, 2))  # region_code and elevation\n",
    "            return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n",
    "\n",
    "        # Get actual data when possible\n",
    "        start_idx = self.valid_indices[idx % len(self.valid_indices)]\n",
    "\n",
    "        # Get timestamps for input and output windows\n",
    "        input_timestamps = self.timestamps[start_idx:start_idx + self.seq_length]\n",
    "        output_timestamps = self.timestamps[start_idx + self.seq_length:\n",
    "                                            start_idx + self.seq_length + self.forecast_horizon]\n",
    "\n",
    "        # Handle potential shortfall in output window (fallback mode)\n",
    "        if self.fallback_mode and len(output_timestamps) < self.forecast_horizon:\n",
    "            # Pad with repetition of last timestamp if needed\n",
    "            last_time = output_timestamps[-1] if len(output_timestamps) > 0 else input_timestamps[-1]\n",
    "            padding = [last_time] * (self.forecast_horizon - len(output_timestamps))\n",
    "            output_timestamps = list(output_timestamps) + padding\n",
    "\n",
    "        # Initialize tensors - simpler now with just one station\n",
    "        X = np.zeros((len(self.feature_cols), 1, self.seq_length))  # Only one station\n",
    "        y = np.zeros((1, self.forecast_horizon))  # Only one station\n",
    "        static_features = np.zeros((1, 2))  # region_code and elevation\n",
    "\n",
    "        # LA Downtown is our only station\n",
    "        station_id = 'San Jose'\n",
    "        \n",
    "        # Input sequence\n",
    "        for t_idx, ts in enumerate(input_timestamps):\n",
    "            station_data = self.df[self.df['timestamp'] == ts]\n",
    "\n",
    "            if not station_data.empty:\n",
    "                for f_idx, feat in enumerate(self.feature_cols):\n",
    "                    X[f_idx, 0, t_idx] = station_data[feat].values[0]\n",
    "\n",
    "                # Store static features (same for all timestamps)\n",
    "                if t_idx == 0:\n",
    "                    static_features[0, 0] = station_data['region_code'].values[0]\n",
    "                    static_features[0, 1] = station_data['elevation_norm'].values[0]\n",
    "\n",
    "        # Target sequence (temperature only)\n",
    "        for t_idx, ts in enumerate(output_timestamps):\n",
    "            if t_idx < self.forecast_horizon:  # Safety check\n",
    "                station_data = self.df[self.df['timestamp'] == ts]\n",
    "\n",
    "                if not station_data.empty:\n",
    "                    y[0, t_idx] = station_data['Temperature_C'].values[0]\n",
    "\n",
    "        return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n",
    "# ============================================================\n",
    "# TEMPORAL FUSION TRANSFORMER IMPLEMENTATION\n",
    "# ============================================================\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention layer for temporal data.\n",
    "    Simplified from the original TFT paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=2, dropout=0.1):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        # Linear projections\n",
    "        queries = self.query(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = self.key(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        values = self.value(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "        # Final linear layer\n",
    "        return self.out(out)\n",
    "\n",
    "class GatedResidualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Residual Network as described in the TFT paper.\n",
    "    Simplified version with fewer layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
    "        super(GatedResidualNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # If input and output sizes are different, apply a skip connection\n",
    "        self.skip_layer = None\n",
    "        if input_size != output_size:\n",
    "            self.skip_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "        # Main layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.gate = nn.Linear(input_size + output_size, output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch\n",
    "        hidden = F.elu(self.fc1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "\n",
    "        # Skip connection\n",
    "        if self.skip_layer is not None:\n",
    "            skip = self.skip_layer(x)\n",
    "        else:\n",
    "            skip = x\n",
    "\n",
    "        # Gate mechanism\n",
    "        gate_input = torch.cat([x, hidden], dim=-1)\n",
    "        gate = torch.sigmoid(self.gate(gate_input))\n",
    "\n",
    "        # Combine using gate\n",
    "        output = gate * hidden + (1 - gate) * skip\n",
    "\n",
    "        # Layer normalization\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class VariableSelectionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Variable Selection Network for TFT.\n",
    "    Simplified version with fewer layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size_per_var, num_vars, hidden_size, output_size, dropout=0.1):\n",
    "        super(VariableSelectionNetwork, self).__init__()\n",
    "        self.input_size_per_var = input_size_per_var\n",
    "        self.num_vars = num_vars\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # GRN for variable weights\n",
    "        self.weight_grn = GatedResidualNetwork(\n",
    "            input_size=input_size_per_var * num_vars,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=num_vars,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # GRN for each variable\n",
    "        self.var_grns = nn.ModuleList([\n",
    "            GatedResidualNetwork(\n",
    "                input_size=input_size_per_var,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_vars)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_vars, input_size_per_var]\n",
    "        batch_size = x.size(0)\n",
    "        flat_x = x.view(batch_size, -1)\n",
    "\n",
    "        # Calculate variable weights\n",
    "        var_weights = self.weight_grn(flat_x)\n",
    "        var_weights = F.softmax(var_weights, dim=-1).unsqueeze(-1)  # [batch_size, num_vars, 1]\n",
    "\n",
    "        # Transform each variable\n",
    "        var_outputs = []\n",
    "        for i in range(self.num_vars):\n",
    "            var_outputs.append(self.var_grns[i](x[:, i]))\n",
    "\n",
    "        var_outputs = torch.stack(var_outputs, dim=1)  # [batch_size, num_vars, output_size]\n",
    "\n",
    "        # Weighted combination\n",
    "        outputs = torch.sum(var_outputs * var_weights, dim=1)  # [batch_size, output_size]\n",
    "\n",
    "        return outputs, var_weights\n",
    "\n",
    "class TemporalFusionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Fusion Transformer with single predictions (no quantiles).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_stations, hidden_size=64, num_heads=1, \n",
    "                 dropout=0.1, forecast_horizon=24, hidden_layers=2):\n",
    "        super(TemporalFusionTransformer, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_stations = num_stations\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Static variable processing\n",
    "        self.static_var_processor = GatedResidualNetwork(\n",
    "            input_size=2,  # region_code, elevation\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Variable selection for time-varying features\n",
    "        self.temporal_var_selection = VariableSelectionNetwork(\n",
    "            input_size_per_var=24,  # Sequence length per feature\n",
    "            num_vars=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # LSTM encoder layers\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size=hidden_size if i == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "                batch_first=True\n",
    "            ) for i in range(hidden_layers)\n",
    "        ])\n",
    "        \n",
    "        # Temporal self-attention\n",
    "        self.self_attention = TemporalSelfAttention(\n",
    "            d_model=hidden_size,\n",
    "            n_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Final output layer for forecasting (single prediction)\n",
    "        self.forecast_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, forecast_horizon)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Unpack inputs\n",
    "        temporal_features, static_features = inputs\n",
    "        batch_size = temporal_features.size(0)\n",
    "        \n",
    "        # [batch, features, stations, time] -> [batch*stations, features, time]\n",
    "        temporal_features = temporal_features.permute(0, 2, 1, 3)\n",
    "        temporal_features = temporal_features.reshape(batch_size * self.num_stations, self.num_features, -1)\n",
    "        \n",
    "        # Static features: [batch, stations, static_dims] -> [batch*stations, static_dims]\n",
    "        static_features = static_features.reshape(batch_size * self.num_stations, -1)\n",
    "        \n",
    "        # Process static features\n",
    "        static_embeddings = self.static_var_processor(static_features)\n",
    "        \n",
    "        # Process temporal features with variable selection\n",
    "        temporal_embeddings, temporal_weights = self.temporal_var_selection(temporal_features)\n",
    "        \n",
    "        # Reshape to [batch*stations, seq_len, hidden]\n",
    "        temporal_embeddings = temporal_embeddings.unsqueeze(1).expand(-1, 24, -1)\n",
    "        \n",
    "        # Add static embeddings to each timestep\n",
    "        temporal_embeddings = temporal_embeddings + static_embeddings.unsqueeze(1)\n",
    "        \n",
    "        # Pass through LSTM layers\n",
    "        lstm_out = temporal_embeddings\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            lstm_out, _ = lstm_layer(lstm_out)\n",
    "        \n",
    "        # Self-attention\n",
    "        attention_out = self.self_attention(lstm_out)\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = self.forecast_projection(attention_out)\n",
    "        \n",
    "        # Take the last timesteps for the forecast horizon\n",
    "        forecast = forecast[:, -self.forecast_horizon:, 0]\n",
    "        \n",
    "        # Reshape back to [batch, stations, horizon]\n",
    "        forecast = forecast.reshape(batch_size, self.num_stations, -1)\n",
    "        \n",
    "        return forecast\n",
    "        \n",
    "# ============================================================\n",
    "# ADE OPTIMIZATION\n",
    "# ============================================================\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "    \n",
    "def train_model(model, train_loader, val_loader, learning_rate=0.001, epochs=30, patience=5, use_checkpoint=True):\n",
    "    \"\"\"\n",
    "    Train the model with MSE loss and early stopping.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoint_path = 'checkpoint.pth'\n",
    "    # Add a model version identifier to prevent future mismatches\n",
    "    model_version = \"single_output_v1\"  # Change this when architecture changes\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Simple checkpoint loading\n",
    "    if os.path.exists(checkpoint_path) and use_checkpoint:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    \n",
    "    # Add mixed precision training with autocast\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f\"Starting training for {epochs} epochs with patience {patience}...\")\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move to device\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = F.mse_loss(outputs, targets)\n",
    "\n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "        avg_train_loss = train_loss / max(1, train_batches)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                # Move to device\n",
    "                if isinstance(inputs, tuple):\n",
    "                    inputs = tuple(x.to(device) for x in inputs)\n",
    "                elif isinstance(inputs, list):\n",
    "                    inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "                else:\n",
    "                    inputs = inputs.to(device)\n",
    "                    \n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = F.mse_loss(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / max(1, val_batches)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if use_checkpoint and avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'model_version': model_version\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  Saved best model with val loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "    \n",
    "    # Load best model\n",
    "    if use_checkpoint and os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded best model\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading best model: {e}\")\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, data_loader, station_ids, regions):\n",
    "    \"\"\"\n",
    "    Evaluate the model and calculate metrics with robust dimension handling.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            # Move to device - handle ALL possible input types\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision for evaluation\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            # Me i changed the median preds with preds instead \n",
    "            preds = outputs.detach().cpu().numpy()\n",
    "            targets_cpu = targets.numpy()\n",
    "            \n",
    "            # Append batch predictions and targets\n",
    "            all_predictions.append(preds)\n",
    "            all_actuals.append(targets_cpu)\n",
    "            \n",
    "            # Clear GPU cache periodically\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Process predictions in chunks to save memory\n",
    "    rmse_sum = 0\n",
    "    mae_sum = 0\n",
    "    r2_sum = 0\n",
    "    sample_count = 0\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    for preds, acts in zip(all_predictions, all_actuals):\n",
    "        # Flatten current batch\n",
    "        preds_flat = preds.flatten()  \n",
    "        acts_flat = acts.flatten()\n",
    "        \n",
    "        # Update metrics\n",
    "        rmse_sum += np.sum((preds_flat - acts_flat) ** 2)\n",
    "        mae_sum += np.sum(np.abs(preds_flat - acts_flat))\n",
    "        sample_count += len(preds_flat)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    rmse = np.sqrt(rmse_sum / sample_count)\n",
    "    mae = mae_sum / sample_count\n",
    "    \n",
    "    # For R², we need all data (this is an approximation)\n",
    "    all_preds_concat = np.concatenate([p.flatten() for p in all_predictions])\n",
    "    all_acts_concat = np.concatenate([a.flatten() for a in all_actuals])\n",
    "    r2 = r2_score(all_acts_concat, all_preds_concat)\n",
    "\n",
    "    print(f\"Overall Metrics - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # Since we only have one station San Jose, simplify the station metrics\n",
    "    station_metrics = {\n",
    "        station_ids[0]: {\n",
    "            'region': regions.get(station_ids[0], 'Unknown'),\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Station {station_ids[0]} ({regions.get(station_ids[0], 'Unknown')}) - \"\n",
    "          f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    return rmse, mae, r2, station_metrics\n",
    "\n",
    "def visualize_predictions(model, data_loader, station_ids, regions, season):\n",
    "    \"\"\"\n",
    "    Visualize predictions for each station.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if len(data_loader) == 0:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions\n",
    "    try:\n",
    "        for inputs, targets in data_loader:\n",
    "            # Only process one batch for visualization\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Move to CPU for plotting\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.numpy()\n",
    "            break\n",
    "\n",
    "        # Check if we have data to plot\n",
    "        if 'outputs' not in locals():\n",
    "            print(\"No data was loaded from the dataloader\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing visualization data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots for each station\n",
    "    fig, axes = plt.subplots(len(station_ids), 1, figsize=(12, 3*len(station_ids)))\n",
    "    if len(station_ids) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    hours = np.arange(24)\n",
    "\n",
    "    for i, station in enumerate(station_ids):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Plot actual vs predicted\n",
    "        ax.plot(hours, targets[0, i, :], 'b-', label='Actual')\n",
    "        ax.plot(hours, outputs[0, i, :], 'r--', label='Predicted')\n",
    "\n",
    "        ax.set_title(f\"{station} ({regions.get(station, 'Unknown')}) - {season}\")\n",
    "        ax.set_xlabel('Hour of Day')\n",
    "        ax.set_ylabel('Temperature (°C)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{season}_predictions.png\")\n",
    "    plt.show()  # Add this line to display the plot\n",
    "    plt.close()\n",
    "\n",
    "def predict_day_temperatures(model, day_data, feature_cols):\n",
    "    \"\"\"\n",
    "    Predict temperatures for a single day (24 hours).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained TFT model\n",
    "        day_data: DataFrame with data for the target day\n",
    "        feature_cols: List of feature columns used by the model\n",
    "        \n",
    "    Returns:\n",
    "        Predicted temperatures for 24 hours\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input data format (same as in create_seasonal_datasets)\n",
    "    try:\n",
    "        # Format data for model input\n",
    "        temporal_data = np.zeros((1, len(feature_cols), 1, 24))  # [batch=1, features, stations=1, time=24]\n",
    "        \n",
    "        # Fill temporal features\n",
    "        for f_idx, feat in enumerate(feature_cols):\n",
    "            if feat in day_data.columns:\n",
    "                temporal_data[0, f_idx, 0, :] = day_data[feat].values[:24]\n",
    "        \n",
    "        # Create static features\n",
    "        static_data = np.zeros((1, 1, 2))  # [batch=1, stations=1, static_features=2]\n",
    "        static_data[0, 0, 0] = day_data['region_code'].iloc[0] \n",
    "        static_data[0, 0, 1] = day_data['elevation_norm'].iloc[0]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_temporal = torch.FloatTensor(temporal_data).to(device)\n",
    "        X_static = torch.FloatTensor(static_data).to(device)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = model((X_temporal, X_static))\n",
    "            \n",
    "        # Return prediction as numpy array\n",
    "        return prediction.cpu().numpy()[0, 0, :]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting temperatures: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "def plot_temperature_prediction(actual, predicted, date_str=None):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted temperatures.\n",
    "    \n",
    "    Args:\n",
    "        actual: Numpy array of actual temperatures (24 hours)\n",
    "        predicted: Numpy array of predicted temperatures (24 hours)\n",
    "        date_str: String representation of the date\n",
    "    \"\"\"\n",
    "    hours = np.arange(24)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(hours, actual, 'b-o', label='Actual Temperature', linewidth=2)\n",
    "    plt.plot(hours, predicted, 'r-o', label='Predicted Temperature', linewidth=2)\n",
    "    \n",
    "    title = f\"Daily Temperature Forecast\" \n",
    "    if date_str:\n",
    "        title += f\" for {date_str}\"\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Hour of Day', fontsize=14)\n",
    "    plt.ylabel('Temperature (°C)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.xticks(hours)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"temperature_forecast_{date_str.replace(' ', '_').replace(':', '-')}.png\" if date_str else \"temperature_forecast.png\"\n",
    "    plt.show()\n",
    "\n",
    "def create_mirror_plot(model, data_loader, station_ids, regions, season):\n",
    "    \"\"\"\n",
    "    Create a mirror plot with actual temperature on left side and predicted on right side.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if len(data_loader) == 0:\n",
    "        print(f\"No data available for {season} mirror plot\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions\n",
    "    try:\n",
    "        for inputs, targets in data_loader:\n",
    "            # Move inputs to device - handle ALL possible input types\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Move to CPU for plotting\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.numpy()\n",
    "            break\n",
    "            \n",
    "        if 'outputs' not in locals():\n",
    "            print(f\"No data was loaded for {season} mirror plot\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing mirror plot data for {season}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print full error details\n",
    "        return\n",
    "    \n",
    "    # Create a figure for each station\n",
    "    for i, station in enumerate(station_ids):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "        \n",
    "        hours = np.arange(24)\n",
    "        \n",
    "        # Left plot - Actual temperatures\n",
    "        ax1.plot(hours, targets[0, i, :], 'b-o', linewidth=2)\n",
    "        ax1.set_title(f\"Actual Temperature - {season}\", fontsize=14)\n",
    "        ax1.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax1.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "        ax1.set_xlim(0, 23)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.invert_xaxis()  # Invert x-axis for mirror effect\n",
    "        \n",
    "        # Right plot - Predicted temperatures\n",
    "        ax2.plot(hours, outputs[0, i, :], 'r-o', linewidth=2)\n",
    "        ax2.set_title(f\"Predicted Temperature - {season}\", fontsize=14)\n",
    "        ax2.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax2.set_xlim(0, 23)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add overall title\n",
    "        plt.suptitle(f\"{station} ({regions.get(station, 'Unknown')}) - {season} Day Comparison\", \n",
    "                    fontsize=16, y=1.05)\n",
    "        \n",
    "        # Add a line in the middle\n",
    "        fig.tight_layout()\n",
    "        plt.subplots_adjust(wspace=0.05)  # Reduce space between subplots\n",
    "        \n",
    "        # Save and display\n",
    "        plt.savefig(f\"{season}_mirror_comparison.png\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def create_seasonal_datasets(df, target_days, all_stations, feature_cols):\n",
    "    \"\"\"\n",
    "    Create datasets for each seasonal day and visualize predictions.\n",
    "    \"\"\"\n",
    "    seasonal_datasets = {}\n",
    "    \n",
    "    for season, day_data in target_days.items():\n",
    "        if day_data.empty:\n",
    "            print(f\"No data available for {season}\")\n",
    "            continue\n",
    "            \n",
    "        # Get the date of this target day\n",
    "        sample_date = day_data['timestamp'].iloc[0]\n",
    "        print(f\"Creating dataset for {season}: {sample_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare the data in the format expected by the model\n",
    "            # Format should be: temporal_features [batch, features, stations, time], static_features [batch, stations, static_features]\n",
    "            \n",
    "            # Reshape features to match expected format\n",
    "            temporal_data = np.zeros((1, len(feature_cols), 1, 24))  # [batch=1, features, stations=1, time=24]\n",
    "            \n",
    "            # Fill in the temporal features\n",
    "            for f_idx, feat in enumerate(feature_cols):\n",
    "                if feat in day_data.columns:\n",
    "                    temporal_data[0, f_idx, 0, :] = day_data[feat].values[:24]  # Using first 24 records\n",
    "            \n",
    "            # Create static features (region_code and elevation)\n",
    "            static_data = np.zeros((1, 1, 2))  # [batch=1, stations=1, static_features=2]\n",
    "            static_data[0, 0, 0] = day_data['region_code'].iloc[0] \n",
    "            static_data[0, 0, 1] = day_data['elevation_norm'].iloc[0]\n",
    "            \n",
    "            # Create target (actual temperatures)\n",
    "            target_data = np.zeros((1, 1, 24))  # [batch=1, stations=1, time=24]\n",
    "            target_data[0, 0, :] = day_data['Temperature_C'].values[:24]  # Using first 24 records\n",
    "            \n",
    "            # Convert to tensors\n",
    "            X_temporal = torch.FloatTensor(temporal_data)\n",
    "            X_static = torch.FloatTensor(static_data)\n",
    "            y = torch.FloatTensor(target_data)\n",
    "            \n",
    "            # Store as a list containing a single data point\n",
    "            seasonal_datasets[season] = [((X_temporal, X_static), y)]\n",
    "            print(f\"Created {season} dataset with shapes: X_temporal={X_temporal.shape}, X_static={X_static.shape}, y={y.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating {season} dataset: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return seasonal_datasets\n",
    "\n",
    "\n",
    "def analyze_topographic_performance(station_metrics, regions):\n",
    "    \"\"\"\n",
    "    Analyze model performance across different topographic regions.\n",
    "    \"\"\"\n",
    "    # Group metrics by region\n",
    "    region_metrics = {}\n",
    "    for station, metrics in station_metrics.items():\n",
    "        region = regions.get(station, 'Unknown')\n",
    "        if region not in region_metrics:\n",
    "            region_metrics[region] = []\n",
    "        region_metrics[region].append(metrics)\n",
    "\n",
    "    # Calculate average metrics by region\n",
    "    region_avg_metrics = {}\n",
    "    for region, metrics_list in region_metrics.items():\n",
    "        avg_rmse = np.mean([m['rmse'] for m in metrics_list])\n",
    "        avg_mae = np.mean([m['mae'] for m in metrics_list])\n",
    "        avg_r2 = np.mean([m['r2'] for m in metrics_list])\n",
    "\n",
    "        region_avg_metrics[region] = {\n",
    "            'avg_rmse': avg_rmse,\n",
    "            'avg_mae': avg_mae,\n",
    "            'avg_r2': avg_r2\n",
    "        }\n",
    "\n",
    "        print(f\"Region {region} - Avg RMSE: {avg_rmse:.4f}, Avg MAE: {avg_mae:.4f}, Avg R²: {avg_r2:.4f}\")\n",
    "\n",
    "        if not station_metrics:\n",
    "            print(\"No station metrics available for analysis\")\n",
    "            return {}\n",
    "\n",
    "    # Create bar chart comparing regions\n",
    "    regions = list(region_avg_metrics.keys())\n",
    "    rmse_values = [region_avg_metrics[r]['avg_rmse'] for r in regions]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(regions, rmse_values)\n",
    "\n",
    "    # Add styling\n",
    "    plt.title('RMSE by Topographic Region', fontsize=16)\n",
    "\n",
    "    plt.title('RMSE by Topographic Region', fontsize=16)\n",
    "    plt.ylabel('RMSE (°C)', fontsize=14)\n",
    "    plt.xlabel('Region', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'{height:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig('region_performance.png')\n",
    "    plt.close()\n",
    "\n",
    "    return region_avg_metrics\n",
    "\n",
    "def analyze_seasonal_performance(seasonal_results):\n",
    "    \"\"\"\n",
    "    Compare model performance across different seasons.\n",
    "    \"\"\"\n",
    "    seasons = list(seasonal_results.keys())\n",
    "    rmse_values = [results['rmse'] for results in seasonal_results.values()]\n",
    "    mae_values = [results['mae'] for results in seasonal_results.values()]\n",
    "\n",
    "    if not seasonal_results:\n",
    "        print(\"No seasonal results available for analysis\")\n",
    "        return [], [], []\n",
    "\n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(seasons))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width/2, rmse_values, width, label='RMSE')\n",
    "    ax.bar(x + width/2, mae_values, width, label='MAE')\n",
    "\n",
    "    ax.set_title('Model Performance by Season', fontsize=16)\n",
    "    ax.set_ylabel('Error (°C)', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(seasons)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(rmse_values):\n",
    "        ax.text(i - width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "    for i, v in enumerate(mae_values):\n",
    "        ax.text(i + width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig('seasonal_performance.png')\n",
    "    plt.close()\n",
    "\n",
    "    return seasons, rmse_values, mae_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:19:23.787517Z",
     "iopub.status.busy": "2025-04-15T08:19:23.787198Z",
     "iopub.status.idle": "2025-04-15T08:19:24.850884Z",
     "shell.execute_reply": "2025-04-15T08:19:24.849897Z",
     "shell.execute_reply.started": "2025-04-15T08:19:23.787492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Jose Weather Forecasting with VMD + TFT\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 1: INITIALIZATION AND DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "print(\"San Jose Weather Forecasting with VMD + TFT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_temperature_prediction(data_file, target_date=None):\n",
    "    \"\"\"\n",
    "    Run the VMD-TFT temperature prediction pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_file: Path to weather data file\n",
    "        target_date: Optional datetime object for prediction target \n",
    "                     (if None, will use the latest available date in data)\n",
    "    \"\"\"\n",
    "    print(\"Starting VMD-TFT temperature prediction pipeline...\")\n",
    "    \n",
    "    df = load_data(data_file)\n",
    "    print(f\"Loaded data with {len(df)} records\")\n",
    "    if df.empty:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Split data by years\n",
    "    train_df, val_df, test_df = split_data_by_years(df)\n",
    "    \n",
    "    # Define feature columns for the model\n",
    "    feature_cols = [\n",
    "    'Temperature_C',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyStationPressure',\n",
    "    'hour_sin', 'hour_cos',\n",
    "    'day_sin', 'day_cos'\n",
    "    ]\n",
    "    \n",
    "    # Perform VMD decomposition on temperature data\n",
    "    print(\"Performing VMD decomposition...\")\n",
    "    decomposed_data = parallel_vmd_decomposition(df, feature_cols_to_decompose=['Temperature_C'])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = WeatherDataset(train_df, ['San Jose'], feature_cols, seq_length=24, forecast_horizon=24)\n",
    "    val_dataset = WeatherDataset(val_df, ['San Jose'], feature_cols, seq_length=24, forecast_horizon=24)\n",
    "    test_dataset = WeatherDataset(test_df, ['San Jose'], feature_cols, seq_length=24, forecast_horizon=24)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    model = TemporalFusionTransformer(\n",
    "        num_features=len(feature_cols),\n",
    "        num_stations=1,  # Just San Jose\n",
    "        hidden_size=64,\n",
    "        num_heads=1,\n",
    "        dropout=0.1,\n",
    "        forecast_horizon=24,\n",
    "        hidden_layers=2\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        learning_rate=0.001,\n",
    "        epochs=30,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "        # Plot training and validation loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', marker='o')\n",
    "    plt.title(\"Training vs Validation Loss over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Evaluate model\n",
    "    rmse, mae, r2, station_metrics = evaluate_model(model, test_loader, ['San Jose'], {'San Jose': 'urban'})\n",
    "    \n",
    "    # Predict for target date\n",
    "    if target_date is None:\n",
    "        # Use last day in dataset\n",
    "        target_date = df['timestamp'].max().date()\n",
    "        print(f\"No target date specified. Using last day in dataset: {target_date}\")\n",
    "    \n",
    "    # Filter data for target day\n",
    "    target_day_data = df[df['timestamp'].dt.date == target_date].copy()\n",
    "    \n",
    "    if target_day_data.empty:\n",
    "        print(f\"No data available for date {target_date}\")\n",
    "        return\n",
    "        \n",
    "    # Ensure we have 24 hours of data\n",
    "    if len(target_day_data) < 24:\n",
    "        print(f\"Warning: Only {len(target_day_data)} hours available for {target_date}. Need 24 hours.\")\n",
    "        return\n",
    "        \n",
    "    # Make prediction\n",
    "    print(f\"Predicting temperatures for {target_date}...\")\n",
    "    predicted_temps = predict_day_temperatures(model, target_day_data, feature_cols)\n",
    "    \n",
    "    if predicted_temps is not None:\n",
    "        # Get actual temperatures\n",
    "        actual_temps = target_day_data['Temperature_C'].values[:24]\n",
    "        \n",
    "        # Plot results\n",
    "        plot_temperature_prediction(actual_temps, predicted_temps, str(target_date))\n",
    "        \n",
    "        # Print prediction results\n",
    "        print(\"\\nPrediction Results:\")\n",
    "        print(f\"{'Hour':<5} {'Actual':<10} {'Predicted':<10} {'Difference':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        for hour in range(24):\n",
    "            diff = predicted_temps[hour] - actual_temps[hour]\n",
    "            print(f\"{hour:<5} {actual_temps[hour]:<10.2f} {predicted_temps[hour]:<10.2f} {diff:<10.2f}\")\n",
    "            \n",
    "        # Calculate error metrics for this prediction\n",
    "        day_mse = np.mean((predicted_temps - actual_temps) ** 2)\n",
    "        day_rmse = np.sqrt(day_mse)\n",
    "        day_mae = np.mean(np.abs(predicted_temps - actual_temps))\n",
    "        \n",
    "        print(f\"\\nDay prediction errors - RMSE: {day_rmse:.2f}°C, MAE: {day_mae:.2f}°C\")\n",
    "    \n",
    "    print(\"VMD-TFT prediction pipeline completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VMD-TFT temperature prediction pipeline...\n",
      "Filtered data to LA Downtown only\n",
      "Loaded data with 35040 records\n",
      "Splitting data by years...\n",
      "Training data (2021-2022): 17496 samples\n",
      "Validation data (2023): 8759 samples\n",
      "Test data (2024): 8784 samples\n",
      "Performing VMD decomposition...\n",
      "Performing VMD decomposition for feature: Temperature_C\n",
      "  Processing station: LA Downtown\n",
      "Starting decomposition for LA Downtown - Temperature_C\n",
      "Loading cached VMD decomposition for LA Downtown - Temperature_C\n",
      "Created dataset with 17449 valid windows\n",
      "Created dataset with 8665 valid windows\n",
      "Created dataset with 8737 valid windows\n",
      "Resuming training from epoch 5\n",
      "Starting training for 5 epochs with patience 2...\n",
      "Training completed in 0.00 minutes\n",
      "Loaded best model\n",
      "Overall Metrics - RMSE: 2.5100, MAE: 2.0850, R²: -0.4614\n",
      "Station LA Downtown (urban) - RMSE: 2.5100, MAE: 2.0850, R²: -0.4614\n",
      "No target date specified. Using last day in dataset: 2025-01-01\n",
      "Warning: Only 1 hours available for 2025-01-01. Need 24 hours.\n"
     ]
    }
   ],
   "source": [
    "c = r\"C:\\Users\\hu4227mo-s\\OneDrive - Lund University\\Merged_Data.csv\"\n",
    "run_temperature_prediction(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VMD-TFT temperature prediction pipeline...\n",
      "Filtered data to San Jose only\n",
      "Loaded data with 35040 records\n",
      "Splitting data by years...\n",
      "Training data (2021-2022): 17496 samples\n",
      "Validation data (2023): 8759 samples\n",
      "Test data (2024): 8784 samples\n",
      "Performing VMD decomposition...\n",
      "Performing VMD decomposition for feature: Temperature_C\n",
      "  Processing station: San Jose\n",
      "Starting decomposition for San Jose - Temperature_C\n",
      "Loading cached VMD decomposition for San Jose - Temperature_C\n",
      "Created dataset with 17449 valid windows\n",
      "Created dataset with 8665 valid windows\n",
      "Created dataset with 8737 valid windows\n",
      "Resuming training from epoch 15\n",
      "Starting training for 30 epochs with patience 5...\n",
      "Epoch 16/30\n",
      "  Train Loss: 4.0516, Val Loss: 4.3692\n",
      "  Saved best model with val loss: 4.3692\n",
      "Epoch 17/30\n",
      "  Train Loss: 3.9172, Val Loss: 4.5488\n",
      "Epoch 18/30\n",
      "  Train Loss: 3.8059, Val Loss: 4.2257\n",
      "  Saved best model with val loss: 4.2257\n",
      "Epoch 19/30\n",
      "  Train Loss: 3.7346, Val Loss: 4.5248\n",
      "Epoch 20/30\n",
      "  Train Loss: 3.7808, Val Loss: 5.9106\n",
      "Epoch 21/30\n",
      "  Train Loss: 3.6605, Val Loss: 4.6821\n",
      "Epoch 22/30\n",
      "  Train Loss: 3.3627, Val Loss: 4.2857\n",
      "Epoch 23/30\n",
      "  Train Loss: 3.3262, Val Loss: 4.4594\n",
      "Early stopping at epoch 23\n",
      "Training completed in 41.12 minutes\n",
      "Loaded best model\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Metrics - RMSE: 2.1970, MAE: 1.6441, R²: 0.8642\n",
      "Station San Jose (urban) - RMSE: 2.1970, MAE: 1.6441, R²: 0.8642\n",
      "Predicting temperatures for 2024-04-15...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Results:\n",
      "Hour  Actual     Predicted  Difference\n",
      "----------------------------------------\n",
      "0     12.78      11.21      -1.57     \n",
      "1     12.78      10.54      -2.23     \n",
      "2     12.78      10.26      -2.52     \n",
      "3     12.22      10.09      -2.13     \n",
      "4     12.22      10.01      -2.22     \n",
      "5     12.22      10.06      -2.16     \n",
      "6     12.22      10.50      -1.72     \n",
      "7     13.33      11.84      -1.49     \n",
      "8     13.33      14.07      0.74      \n",
      "9     13.89      16.18      2.29      \n",
      "10    14.44      18.01      3.57      \n",
      "11    16.11      19.38      3.27      \n",
      "12    17.22      20.18      2.96      \n",
      "13    18.89      20.45      1.56      \n",
      "14    20.00      20.28      0.28      \n",
      "15    20.00      19.56      -0.44     \n",
      "16    20.00      18.25      -1.75     \n",
      "17    18.89      16.54      -2.35     \n",
      "18    17.78      15.02      -2.75     \n",
      "19    15.56      13.95      -1.61     \n",
      "20    15.00      13.25      -1.75     \n",
      "21    13.89      12.81      -1.08     \n",
      "22    13.33      12.51      -0.82     \n",
      "23    12.22      12.30      0.07      \n",
      "\n",
      "Day prediction errors - RMSE: 2.01°C, MAE: 1.81°C\n",
      "VMD-TFT prediction pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "run_temperature_prediction(c, datetime(2024, 4, 15).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:02:08.988737Z",
     "iopub.status.busy": "2025-04-15T13:02:08.988407Z",
     "iopub.status.idle": "2025-04-15T13:02:09.020953Z",
     "shell.execute_reply": "2025-04-15T13:02:09.020152Z",
     "shell.execute_reply.started": "2025-04-15T13:02:08.988712Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing VMD decomposition...\n",
      "Performing VMD decomposition for feature: Temperature_C\n",
      "  Processing station: LA Downtown\n",
      "Starting decomposition for LA Downtown - Temperature_C\n",
      "Loading cached VMD decomposition for LA Downtown - Temperature_C\n",
      "Found 14 VMD modes for Temperature_C\n",
      "BLOCK 2 COMPLETED: VMD decomposition successful.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 2: VMD DECOMPOSITION\n",
    "# ============================================================\n",
    "\n",
    "# Apply VMD decomposition to the Temperature_C feature\n",
    "print(\"Performing VMD decomposition...\")\n",
    "decomposed_data = parallel_vmd_decomposition(df, feature_cols_to_decompose=['Temperature_C'])\n",
    "\n",
    "# Expand feature columns with decomposed modes\n",
    "expanded_feature_cols = feature_cols.copy()\n",
    "\n",
    "# For LA Downtown, add decomposed temperature modes to feature columns\n",
    "la_downtown_modes = decomposed_data['LA Downtown']['Temperature_C']\n",
    "n_modes = la_downtown_modes.shape[0]  # Number of VMD modes\n",
    "\n",
    "print(f\"Found {n_modes} VMD modes for Temperature_C\")\n",
    "\n",
    "# Create expanded dataset with VMD modes\n",
    "vmd_df = df.copy()\n",
    "\n",
    "# Add VMD modes as new features\n",
    "for i in range(n_modes):\n",
    "    mode_name = f'Temp_Mode_{i+1}'\n",
    "    vmd_df[mode_name] = la_downtown_modes[i, :]\n",
    "    expanded_feature_cols.append(mode_name)\n",
    "\n",
    "print(\"BLOCK 2 COMPLETED: VMD decomposition successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:02:17.348627Z",
     "iopub.status.busy": "2025-04-15T13:02:17.348266Z",
     "iopub.status.idle": "2025-04-15T13:02:17.562474Z",
     "shell.execute_reply": "2025-04-15T13:02:17.561212Z",
     "shell.execute_reply.started": "2025-04-15T13:02:17.348604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 34946 valid windows\n",
      "Dataset split completed - Train: 24462, Validation: 5241, Test: 5243\n",
      "BLOCK 3 COMPLETED: Dataset creation and splitting successful.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 3: DATASET CREATION AND SPLITTING\n",
    "# ============================================================\n",
    "\n",
    "# Create dataset with expanded features\n",
    "full_dataset = WeatherDataset(\n",
    "    df=vmd_df,\n",
    "    station_ids=all_stations,\n",
    "    feature_cols=expanded_feature_cols,\n",
    "    seq_length=24,\n",
    "    forecast_horizon=24\n",
    ")\n",
    "\n",
    "# Split into train/val/test\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "val_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Dataset split completed - Train: {train_size}, Validation: {val_size}, Test: {test_size}\")\n",
    "\n",
    "# First, create data loaders with default batch size\n",
    "initial_batch_size = 32  # Default value before optimization\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=initial_batch_size, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=initial_batch_size, \n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(\"BLOCK 3 COMPLETED: Dataset creation and splitting successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:25:15.653642Z",
     "iopub.status.busy": "2025-04-15T08:25:15.653319Z",
     "iopub.status.idle": "2025-04-15T08:47:41.341130Z",
     "shell.execute_reply": "2025-04-15T08:47:41.339745Z",
     "shell.execute_reply.started": "2025-04-15T08:25:15.653616Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK 3.5: Running quick test before optimization...\n",
      "Running quick test to verify checkpoint compatibility...\n",
      "Starting training from scratch.\n",
      "Starting training for 2 epochs with patience 5...\n",
      "  Batch 10/765, Loss: 188.5364, Time: 0.060s\n",
      "  Batch 20/765, Loss: 24.3842, Time: 0.059s\n",
      "  Batch 30/765, Loss: 22.0760, Time: 0.061s\n",
      "  Batch 40/765, Loss: 20.5718, Time: 0.060s\n",
      "  Batch 50/765, Loss: 27.9817, Time: 0.061s\n",
      "  Batch 60/765, Loss: 21.1070, Time: 0.061s\n",
      "  Batch 70/765, Loss: 19.5703, Time: 0.062s\n",
      "  Batch 80/765, Loss: 24.9461, Time: 0.062s\n",
      "  Batch 90/765, Loss: 20.9401, Time: 0.063s\n",
      "  Batch 100/765, Loss: 23.7392, Time: 0.063s\n",
      "  Batch 110/765, Loss: 39.5088, Time: 0.061s\n",
      "  Batch 120/765, Loss: 20.0380, Time: 0.062s\n",
      "  Batch 130/765, Loss: 27.5298, Time: 0.062s\n",
      "  Batch 140/765, Loss: 21.4747, Time: 0.061s\n",
      "  Batch 150/765, Loss: 28.4103, Time: 0.061s\n",
      "  Batch 160/765, Loss: 24.9166, Time: 0.088s\n",
      "  Batch 170/765, Loss: 24.0036, Time: 0.041s\n",
      "  Batch 180/765, Loss: 20.0315, Time: 0.066s\n",
      "  Batch 190/765, Loss: 27.7392, Time: 0.066s\n",
      "  Batch 200/765, Loss: 21.2319, Time: 0.060s\n",
      "  Batch 210/765, Loss: 17.2498, Time: 0.063s\n",
      "  Batch 220/765, Loss: 36.8749, Time: 0.061s\n",
      "  Batch 230/765, Loss: 21.7890, Time: 0.063s\n",
      "  Batch 240/765, Loss: 24.5242, Time: 0.060s\n",
      "  Batch 250/765, Loss: 20.4561, Time: 0.062s\n",
      "  Batch 260/765, Loss: 20.0811, Time: 0.063s\n",
      "  Batch 270/765, Loss: 22.0361, Time: 0.060s\n",
      "  Batch 280/765, Loss: 23.1479, Time: 0.071s\n",
      "  Batch 290/765, Loss: 14.7952, Time: 0.061s\n",
      "  Batch 300/765, Loss: 12.7364, Time: 0.041s\n",
      "  Batch 310/765, Loss: 14.0309, Time: 0.061s\n",
      "  Batch 320/765, Loss: 14.2217, Time: 0.059s\n",
      "  Batch 330/765, Loss: 11.9675, Time: 0.060s\n",
      "  Batch 340/765, Loss: 11.2232, Time: 0.061s\n",
      "  Batch 350/765, Loss: 12.1830, Time: 0.060s\n",
      "  Batch 360/765, Loss: 14.7563, Time: 0.068s\n",
      "  Batch 370/765, Loss: 14.6650, Time: 0.062s\n",
      "  Batch 380/765, Loss: 14.0499, Time: 0.062s\n",
      "  Batch 390/765, Loss: 12.1958, Time: 0.062s\n",
      "  Batch 400/765, Loss: 13.6703, Time: 0.062s\n",
      "  Batch 410/765, Loss: 11.9901, Time: 0.062s\n",
      "  Batch 420/765, Loss: 12.3553, Time: 0.060s\n",
      "  Batch 430/765, Loss: 12.8899, Time: 0.062s\n",
      "  Batch 440/765, Loss: 11.1361, Time: 0.060s\n",
      "  Batch 450/765, Loss: 13.6977, Time: 0.061s\n",
      "  Batch 460/765, Loss: 10.6339, Time: 0.060s\n",
      "  Batch 470/765, Loss: 12.0050, Time: 0.061s\n",
      "  Batch 480/765, Loss: 10.4292, Time: 0.059s\n",
      "  Batch 490/765, Loss: 7.6244, Time: 0.060s\n",
      "  Batch 500/765, Loss: 12.9191, Time: 0.042s\n",
      "  Batch 510/765, Loss: 10.9665, Time: 0.061s\n",
      "  Batch 520/765, Loss: 11.4850, Time: 0.060s\n",
      "  Batch 530/765, Loss: 10.6138, Time: 0.061s\n",
      "  Batch 540/765, Loss: 10.9600, Time: 0.061s\n",
      "  Batch 550/765, Loss: 12.4348, Time: 0.062s\n",
      "  Batch 560/765, Loss: 8.4179, Time: 0.061s\n",
      "  Batch 570/765, Loss: 14.1186, Time: 0.060s\n",
      "  Batch 580/765, Loss: 10.3576, Time: 0.060s\n",
      "  Batch 590/765, Loss: 7.0853, Time: 0.061s\n",
      "  Batch 600/765, Loss: 9.9203, Time: 0.061s\n",
      "  Batch 610/765, Loss: 9.2733, Time: 0.060s\n",
      "  Batch 620/765, Loss: 9.5219, Time: 0.040s\n",
      "  Batch 630/765, Loss: 8.9881, Time: 0.061s\n",
      "  Batch 640/765, Loss: 10.0554, Time: 0.060s\n",
      "  Batch 650/765, Loss: 13.5823, Time: 0.061s\n",
      "  Batch 660/765, Loss: 12.8328, Time: 0.061s\n",
      "  Batch 670/765, Loss: 11.1290, Time: 0.060s\n",
      "  Batch 680/765, Loss: 10.9231, Time: 0.061s\n",
      "  Batch 690/765, Loss: 11.3044, Time: 0.062s\n",
      "  Batch 700/765, Loss: 11.8241, Time: 0.061s\n",
      "  Batch 710/765, Loss: 9.9241, Time: 0.062s\n",
      "  Batch 720/765, Loss: 10.0160, Time: 0.064s\n",
      "  Batch 730/765, Loss: 6.9604, Time: 0.056s\n",
      "  Batch 740/765, Loss: 7.2060, Time: 0.061s\n",
      "  Batch 750/765, Loss: 9.5212, Time: 0.061s\n",
      "  Batch 760/765, Loss: 10.3007, Time: 0.062s\n",
      "Epoch 1/2\n",
      "  Train Loss: 20.0783 (765 batches, avg batch time: 0.067s)\n",
      "  Val Loss: 8.9238 (validation time: 118.83s)\n",
      "  Epoch Time: 669.53s, Est. Remaining: 11.16 minutes\n",
      "  No improvement for 1/5 epochs\n",
      "  Batch 10/765, Loss: 6.1254, Time: 0.061s\n",
      "  Batch 20/765, Loss: 10.3201, Time: 0.143s\n",
      "  Batch 30/765, Loss: 8.4802, Time: 0.071s\n",
      "  Batch 40/765, Loss: 8.7689, Time: 0.061s\n",
      "  Batch 50/765, Loss: 11.2764, Time: 0.062s\n",
      "  Batch 60/765, Loss: 7.8157, Time: 0.062s\n",
      "  Batch 70/765, Loss: 9.9887, Time: 0.063s\n",
      "  Batch 80/765, Loss: 10.9238, Time: 0.061s\n",
      "  Batch 90/765, Loss: 9.3759, Time: 0.061s\n",
      "  Batch 100/765, Loss: 11.5638, Time: 0.063s\n",
      "  Batch 110/765, Loss: 9.7689, Time: 0.074s\n",
      "  Batch 120/765, Loss: 9.1039, Time: 0.062s\n",
      "  Batch 130/765, Loss: 8.4679, Time: 0.060s\n",
      "  Batch 140/765, Loss: 10.9059, Time: 0.060s\n",
      "  Batch 150/765, Loss: 10.1370, Time: 0.062s\n",
      "  Batch 160/765, Loss: 11.6547, Time: 0.061s\n",
      "  Batch 170/765, Loss: 9.9170, Time: 0.061s\n",
      "  Batch 180/765, Loss: 8.6545, Time: 0.040s\n",
      "  Batch 190/765, Loss: 9.9541, Time: 0.041s\n",
      "  Batch 200/765, Loss: 12.7833, Time: 0.100s\n",
      "  Batch 210/765, Loss: 11.2590, Time: 0.060s\n",
      "  Batch 220/765, Loss: 10.8670, Time: 0.062s\n",
      "  Batch 230/765, Loss: 10.9327, Time: 0.062s\n",
      "  Batch 240/765, Loss: 11.5773, Time: 0.062s\n",
      "  Batch 250/765, Loss: 11.6183, Time: 0.061s\n",
      "  Batch 260/765, Loss: 9.2786, Time: 0.062s\n",
      "  Batch 270/765, Loss: 8.6507, Time: 0.062s\n",
      "  Batch 280/765, Loss: 11.7974, Time: 0.062s\n",
      "  Batch 290/765, Loss: 9.3679, Time: 0.135s\n",
      "  Batch 300/765, Loss: 5.9865, Time: 0.067s\n",
      "  Batch 310/765, Loss: 9.5655, Time: 0.060s\n",
      "  Batch 320/765, Loss: 9.4570, Time: 0.061s\n",
      "  Batch 330/765, Loss: 9.4301, Time: 0.040s\n",
      "  Batch 340/765, Loss: 11.8770, Time: 0.060s\n",
      "  Batch 350/765, Loss: 9.1778, Time: 0.061s\n",
      "  Batch 360/765, Loss: 7.3999, Time: 0.060s\n",
      "  Batch 370/765, Loss: 8.0594, Time: 0.064s\n",
      "  Batch 380/765, Loss: 7.7690, Time: 0.121s\n",
      "  Batch 390/765, Loss: 11.4046, Time: 0.060s\n",
      "  Batch 400/765, Loss: 7.7047, Time: 0.062s\n",
      "  Batch 410/765, Loss: 9.9656, Time: 0.062s\n",
      "  Batch 420/765, Loss: 8.7147, Time: 0.065s\n",
      "  Batch 430/765, Loss: 10.5159, Time: 0.061s\n",
      "  Batch 440/765, Loss: 9.2661, Time: 0.063s\n",
      "  Batch 450/765, Loss: 11.2210, Time: 0.062s\n",
      "  Batch 460/765, Loss: 8.8498, Time: 0.060s\n",
      "  Batch 470/765, Loss: 10.9060, Time: 0.088s\n",
      "  Batch 480/765, Loss: 9.1847, Time: 0.073s\n",
      "  Batch 490/765, Loss: 11.0540, Time: 0.060s\n",
      "  Batch 500/765, Loss: 9.4857, Time: 0.061s\n",
      "  Batch 510/765, Loss: 8.4494, Time: 0.063s\n",
      "  Batch 520/765, Loss: 6.8609, Time: 0.062s\n",
      "  Batch 530/765, Loss: 11.0073, Time: 0.061s\n",
      "  Batch 540/765, Loss: 7.9460, Time: 0.063s\n",
      "  Batch 550/765, Loss: 7.3485, Time: 0.040s\n",
      "  Batch 560/765, Loss: 7.8606, Time: 0.100s\n",
      "  Batch 570/765, Loss: 9.4505, Time: 0.062s\n",
      "  Batch 580/765, Loss: 9.4922, Time: 0.061s\n",
      "  Batch 590/765, Loss: 8.6506, Time: 0.063s\n",
      "  Batch 600/765, Loss: 11.8238, Time: 0.061s\n",
      "  Batch 610/765, Loss: 8.5276, Time: 0.062s\n",
      "  Batch 620/765, Loss: 9.5523, Time: 0.062s\n",
      "  Batch 630/765, Loss: 7.9847, Time: 0.064s\n",
      "  Batch 640/765, Loss: 8.8047, Time: 0.063s\n",
      "  Batch 650/765, Loss: 8.5742, Time: 0.042s\n",
      "  Batch 660/765, Loss: 8.0943, Time: 0.062s\n",
      "  Batch 670/765, Loss: 9.8837, Time: 0.062s\n",
      "  Batch 680/765, Loss: 9.0827, Time: 0.061s\n",
      "  Batch 690/765, Loss: 10.0251, Time: 0.065s\n",
      "  Batch 700/765, Loss: 11.1201, Time: 0.042s\n",
      "  Batch 710/765, Loss: 8.7654, Time: 0.064s\n",
      "  Batch 720/765, Loss: 8.0157, Time: 0.062s\n",
      "  Batch 730/765, Loss: 7.6138, Time: 0.062s\n",
      "  Batch 740/765, Loss: 9.1480, Time: 0.066s\n",
      "  Batch 750/765, Loss: 9.4148, Time: 0.060s\n",
      "  Batch 760/765, Loss: 10.6229, Time: 0.042s\n",
      "Epoch 2/2\n",
      "  Train Loss: 9.6186 (765 batches, avg batch time: 0.064s)\n",
      "  Val Loss: 8.7535 (validation time: 119.14s)\n",
      "  Epoch Time: 673.13s, Est. Remaining: 0.00 minutes\n",
      "  No improvement for 2/5 epochs\n",
      "Training completed in 22.38 minutes (1342.7 seconds)\n",
      "Test run completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the quick test function\n",
    "def quick_test_run(train_loader, val_loader, feature_cols, num_stations):\n",
    "    \"\"\"Run a quick test to ensure training works.\"\"\"\n",
    "    print(\"Running quick test to verify checkpoint compatibility...\")\n",
    "    \n",
    "    # Create a small model with fixed parameters\n",
    "    test_model = TemporalFusionTransformer(\n",
    "        num_features=len(feature_cols),\n",
    "        num_stations=num_stations,\n",
    "        hidden_size=24,\n",
    "        num_heads=1,\n",
    "        dropout=0.1,\n",
    "        forecast_horizon=24,\n",
    "        hidden_layers=2\n",
    "    )\n",
    "    \n",
    "    # Train for only 2 epochs with early stopping disabled\n",
    "    model, train_losses, val_losses = train_model(\n",
    "        model=test_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        learning_rate=0.01,\n",
    "        epochs=2,  # Just 2 epochs for quick testing\n",
    "        patience=5,  # High patience to avoid early stopping\n",
    "        use_checkpoint=False  # Set to False to avoid checkpoint issues\n",
    "    )\n",
    "    \n",
    "    print(\"Test run completed successfully!\")\n",
    "    return True\n",
    "\n",
    "# First run the quick test to verify everything works\n",
    "print(\"BLOCK 3.5: Running quick test before optimization...\")\n",
    "quick_test_run(train_loader, val_loader, expanded_feature_cols, len(all_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:02:33.189875Z",
     "iopub.status.busy": "2025-04-15T13:02:33.189504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ADE optimization for hyperparameters...\n",
      "Removing incompatible checkpoint before optimization.\n",
      "Starting training from scratch.\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 60.4488, Time: 0.069s\n",
      "  Batch 20/765, Loss: 29.6770, Time: 0.084s\n",
      "  Batch 30/765, Loss: 18.9761, Time: 0.071s\n",
      "  Batch 40/765, Loss: 18.9913, Time: 0.086s\n",
      "  Batch 50/765, Loss: 19.8823, Time: 0.072s\n",
      "  Batch 60/765, Loss: 21.7851, Time: 0.078s\n",
      "  Batch 70/765, Loss: 22.8416, Time: 0.071s\n",
      "  Batch 80/765, Loss: 19.0067, Time: 0.072s\n",
      "  Batch 90/765, Loss: 30.1333, Time: 0.054s\n",
      "  Batch 100/765, Loss: 25.2879, Time: 0.068s\n",
      "  Batch 110/765, Loss: 31.4014, Time: 0.068s\n",
      "  Batch 120/765, Loss: 28.7064, Time: 0.072s\n",
      "  Batch 130/765, Loss: 28.7608, Time: 0.069s\n",
      "  Batch 140/765, Loss: 30.9138, Time: 0.044s\n",
      "  Batch 150/765, Loss: 25.6082, Time: 0.068s\n",
      "  Batch 160/765, Loss: 25.1586, Time: 0.071s\n",
      "  Batch 170/765, Loss: 25.9122, Time: 0.071s\n",
      "  Batch 180/765, Loss: 24.8534, Time: 0.071s\n",
      "  Batch 190/765, Loss: 26.1963, Time: 0.070s\n",
      "  Batch 200/765, Loss: 20.2368, Time: 0.070s\n",
      "  Batch 210/765, Loss: 30.5441, Time: 0.067s\n",
      "  Batch 220/765, Loss: 23.5357, Time: 0.071s\n",
      "  Batch 230/765, Loss: 30.4766, Time: 0.069s\n",
      "  Batch 240/765, Loss: 23.4015, Time: 0.089s\n",
      "  Batch 250/765, Loss: 28.4134, Time: 0.070s\n",
      "  Batch 260/765, Loss: 29.8147, Time: 0.067s\n",
      "  Batch 270/765, Loss: 27.0964, Time: 0.068s\n",
      "  Batch 280/765, Loss: 28.3097, Time: 0.069s\n",
      "  Batch 290/765, Loss: 21.0304, Time: 0.069s\n",
      "  Batch 300/765, Loss: 21.9782, Time: 0.071s\n",
      "  Batch 310/765, Loss: 19.0724, Time: 0.070s\n",
      "  Batch 320/765, Loss: 29.7988, Time: 0.068s\n",
      "  Batch 330/765, Loss: 21.6966, Time: 0.086s\n",
      "  Batch 340/765, Loss: 19.2451, Time: 0.067s\n",
      "  Batch 350/765, Loss: 30.1951, Time: 0.068s\n",
      "  Batch 360/765, Loss: 24.4531, Time: 0.067s\n",
      "  Batch 370/765, Loss: 22.3554, Time: 0.071s\n",
      "  Batch 380/765, Loss: 25.5658, Time: 0.068s\n",
      "  Batch 390/765, Loss: 27.6206, Time: 0.068s\n",
      "  Batch 400/765, Loss: 25.6305, Time: 0.068s\n",
      "  Batch 410/765, Loss: 20.8522, Time: 0.048s\n",
      "  Batch 420/765, Loss: 13.9580, Time: 0.069s\n",
      "  Batch 430/765, Loss: 26.4452, Time: 0.071s\n",
      "  Batch 440/765, Loss: 18.1891, Time: 0.069s\n",
      "  Batch 450/765, Loss: 12.7521, Time: 0.068s\n",
      "  Batch 460/765, Loss: 19.3154, Time: 0.070s\n",
      "  Batch 470/765, Loss: 12.6784, Time: 0.070s\n",
      "  Batch 480/765, Loss: 14.3989, Time: 0.071s\n",
      "  Batch 490/765, Loss: 18.7804, Time: 0.071s\n",
      "  Batch 500/765, Loss: 13.1153, Time: 0.070s\n",
      "  Batch 510/765, Loss: 9.9207, Time: 0.068s\n",
      "  Batch 520/765, Loss: 18.4215, Time: 0.070s\n",
      "  Batch 530/765, Loss: 17.5239, Time: 0.070s\n",
      "  Batch 540/765, Loss: 21.6746, Time: 0.092s\n",
      "  Batch 550/765, Loss: 13.1417, Time: 0.071s\n",
      "  Batch 560/765, Loss: 12.4777, Time: 0.084s\n",
      "  Batch 570/765, Loss: 14.0618, Time: 0.069s\n",
      "  Batch 580/765, Loss: 14.6083, Time: 0.071s\n",
      "  Batch 590/765, Loss: 12.3563, Time: 0.070s\n",
      "  Batch 600/765, Loss: 12.7236, Time: 0.071s\n",
      "  Batch 610/765, Loss: 9.9828, Time: 0.073s\n",
      "  Batch 620/765, Loss: 11.5078, Time: 0.074s\n",
      "  Batch 630/765, Loss: 14.9977, Time: 0.047s\n",
      "  Batch 640/765, Loss: 12.0098, Time: 0.069s\n",
      "  Batch 650/765, Loss: 11.5294, Time: 0.071s\n",
      "  Batch 660/765, Loss: 11.7418, Time: 0.073s\n",
      "  Batch 670/765, Loss: 11.3438, Time: 0.072s\n",
      "  Batch 680/765, Loss: 9.6422, Time: 0.080s\n",
      "  Batch 690/765, Loss: 13.0550, Time: 0.075s\n",
      "  Batch 700/765, Loss: 11.8902, Time: 0.143s\n",
      "  Batch 710/765, Loss: 15.6963, Time: 0.090s\n",
      "  Batch 720/765, Loss: 11.9993, Time: 0.072s\n",
      "  Batch 730/765, Loss: 10.3015, Time: 0.072s\n",
      "  Batch 740/765, Loss: 12.6119, Time: 0.071s\n",
      "  Batch 750/765, Loss: 8.8654, Time: 0.070s\n",
      "  Batch 760/765, Loss: 12.4726, Time: 0.071s\n",
      "Epoch 1/5\n",
      "  Train Loss: 21.2453 (765 batches, avg batch time: 0.073s)\n",
      "  Val Loss: 10.7155 (validation time: 75.89s)\n",
      "  Epoch Time: 657.45s, Est. Remaining: 43.83 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 10.7155\n",
      "  Batch 10/765, Loss: 12.1798, Time: 0.069s\n",
      "  Batch 20/765, Loss: 8.8881, Time: 0.044s\n",
      "  Batch 30/765, Loss: 10.8776, Time: 0.044s\n",
      "  Batch 40/765, Loss: 12.0932, Time: 0.071s\n",
      "  Batch 50/765, Loss: 11.0040, Time: 0.070s\n",
      "  Batch 60/765, Loss: 11.1475, Time: 0.071s\n",
      "  Batch 70/765, Loss: 8.5769, Time: 0.071s\n",
      "  Batch 80/765, Loss: 9.7896, Time: 0.069s\n",
      "  Batch 90/765, Loss: 12.4687, Time: 0.069s\n",
      "  Batch 100/765, Loss: 9.4855, Time: 0.159s\n",
      "  Batch 110/765, Loss: 15.2918, Time: 0.070s\n",
      "  Batch 120/765, Loss: 12.3475, Time: 0.070s\n",
      "  Batch 130/765, Loss: 10.6862, Time: 0.070s\n",
      "  Batch 140/765, Loss: 10.2746, Time: 0.072s\n",
      "  Batch 150/765, Loss: 12.8270, Time: 0.073s\n",
      "  Batch 160/765, Loss: 14.3445, Time: 0.068s\n",
      "  Batch 170/765, Loss: 13.3170, Time: 0.089s\n",
      "  Batch 180/765, Loss: 12.3551, Time: 0.070s\n",
      "  Batch 190/765, Loss: 9.8238, Time: 0.071s\n",
      "  Batch 200/765, Loss: 13.3285, Time: 0.071s\n",
      "  Batch 210/765, Loss: 12.1592, Time: 0.092s\n",
      "  Batch 220/765, Loss: 11.7179, Time: 0.077s\n",
      "  Batch 230/765, Loss: 12.4563, Time: 0.200s\n",
      "  Batch 240/765, Loss: 10.4855, Time: 0.071s\n",
      "  Batch 250/765, Loss: 12.5606, Time: 0.046s\n",
      "  Batch 260/765, Loss: 14.4668, Time: 0.070s\n",
      "  Batch 270/765, Loss: 11.0803, Time: 0.070s\n",
      "  Batch 280/765, Loss: 10.5269, Time: 0.069s\n",
      "  Batch 290/765, Loss: 12.6557, Time: 0.069s\n",
      "  Batch 300/765, Loss: 12.7374, Time: 0.070s\n",
      "  Batch 310/765, Loss: 10.4440, Time: 0.069s\n",
      "  Batch 320/765, Loss: 11.1202, Time: 0.070s\n",
      "  Batch 330/765, Loss: 11.0648, Time: 0.070s\n",
      "  Batch 340/765, Loss: 9.8892, Time: 0.049s\n",
      "  Batch 350/765, Loss: 12.8045, Time: 0.069s\n",
      "  Batch 360/765, Loss: 7.8830, Time: 0.071s\n",
      "  Batch 370/765, Loss: 10.9666, Time: 0.068s\n",
      "  Batch 380/765, Loss: 11.8933, Time: 0.071s\n",
      "  Batch 390/765, Loss: 9.4051, Time: 0.069s\n",
      "  Batch 400/765, Loss: 8.3322, Time: 0.070s\n",
      "  Batch 410/765, Loss: 14.7169, Time: 0.072s\n",
      "  Batch 420/765, Loss: 11.5249, Time: 0.069s\n",
      "  Batch 430/765, Loss: 15.2621, Time: 0.071s\n",
      "  Batch 440/765, Loss: 10.9798, Time: 0.070s\n",
      "  Batch 450/765, Loss: 10.5810, Time: 0.112s\n",
      "  Batch 460/765, Loss: 11.5740, Time: 0.069s\n",
      "  Batch 470/765, Loss: 12.3591, Time: 0.063s\n",
      "  Batch 480/765, Loss: 10.1535, Time: 0.073s\n",
      "  Batch 490/765, Loss: 11.7040, Time: 0.045s\n",
      "  Batch 500/765, Loss: 10.9326, Time: 0.045s\n",
      "  Batch 510/765, Loss: 10.3549, Time: 0.073s\n",
      "  Batch 520/765, Loss: 13.7479, Time: 0.069s\n",
      "  Batch 530/765, Loss: 10.1048, Time: 0.051s\n",
      "  Batch 540/765, Loss: 9.8892, Time: 0.068s\n",
      "  Batch 550/765, Loss: 11.4753, Time: 0.067s\n",
      "  Batch 560/765, Loss: 11.7503, Time: 0.069s\n",
      "  Batch 570/765, Loss: 10.3682, Time: 0.068s\n",
      "  Batch 580/765, Loss: 10.2084, Time: 0.070s\n",
      "  Batch 590/765, Loss: 9.0836, Time: 0.071s\n",
      "  Batch 600/765, Loss: 9.1695, Time: 0.070s\n",
      "  Batch 610/765, Loss: 10.9371, Time: 0.073s\n",
      "  Batch 620/765, Loss: 9.0700, Time: 0.046s\n",
      "  Batch 630/765, Loss: 10.7069, Time: 0.069s\n",
      "  Batch 640/765, Loss: 9.1222, Time: 0.069s\n",
      "  Batch 650/765, Loss: 11.6702, Time: 0.069s\n",
      "  Batch 660/765, Loss: 8.4352, Time: 0.049s\n",
      "  Batch 670/765, Loss: 10.4700, Time: 0.075s\n",
      "  Batch 680/765, Loss: 11.5852, Time: 0.080s\n",
      "  Batch 690/765, Loss: 7.9429, Time: 0.072s\n",
      "  Batch 700/765, Loss: 9.7683, Time: 0.225s\n",
      "  Batch 710/765, Loss: 8.3687, Time: 0.072s\n",
      "  Batch 720/765, Loss: 12.2186, Time: 0.046s\n",
      "  Batch 730/765, Loss: 11.7270, Time: 0.078s\n",
      "  Batch 740/765, Loss: 7.5706, Time: 0.073s\n",
      "  Batch 750/765, Loss: 10.4572, Time: 0.073s\n",
      "  Batch 760/765, Loss: 11.1078, Time: 0.073s\n",
      "Epoch 2/5\n",
      "  Train Loss: 10.7902 (765 batches, avg batch time: 0.073s)\n",
      "  Val Loss: 10.4972 (validation time: 75.22s)\n",
      "  Epoch Time: 651.37s, Est. Remaining: 32.72 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 10.4972\n",
      "  Batch 10/765, Loss: 11.6743, Time: 0.050s\n",
      "  Batch 20/765, Loss: 11.5366, Time: 0.081s\n",
      "  Batch 30/765, Loss: 9.3940, Time: 0.118s\n",
      "  Batch 40/765, Loss: 9.1982, Time: 0.074s\n",
      "  Batch 50/765, Loss: 9.3758, Time: 0.105s\n",
      "  Batch 60/765, Loss: 10.1516, Time: 0.074s\n",
      "  Batch 70/765, Loss: 10.1627, Time: 0.075s\n",
      "  Batch 80/765, Loss: 12.2645, Time: 0.075s\n",
      "  Batch 90/765, Loss: 10.8575, Time: 0.052s\n",
      "  Batch 100/765, Loss: 10.5034, Time: 0.074s\n",
      "  Batch 110/765, Loss: 8.1230, Time: 0.075s\n",
      "  Batch 120/765, Loss: 8.9230, Time: 0.052s\n",
      "  Batch 130/765, Loss: 11.3410, Time: 0.078s\n",
      "  Batch 140/765, Loss: 7.1985, Time: 0.076s\n",
      "  Batch 150/765, Loss: 9.7610, Time: 0.051s\n",
      "  Batch 160/765, Loss: 8.9487, Time: 0.077s\n",
      "  Batch 170/765, Loss: 8.5953, Time: 0.074s\n",
      "  Batch 180/765, Loss: 10.8941, Time: 0.050s\n",
      "  Batch 190/765, Loss: 9.7961, Time: 0.075s\n",
      "  Batch 200/765, Loss: 10.3948, Time: 0.072s\n",
      "  Batch 210/765, Loss: 8.8071, Time: 0.073s\n",
      "  Batch 220/765, Loss: 11.4575, Time: 0.125s\n",
      "  Batch 230/765, Loss: 10.0329, Time: 0.075s\n",
      "  Batch 240/765, Loss: 7.9754, Time: 0.076s\n",
      "  Batch 250/765, Loss: 9.6880, Time: 0.078s\n",
      "  Batch 260/765, Loss: 8.4553, Time: 0.074s\n",
      "  Batch 270/765, Loss: 7.6669, Time: 0.072s\n",
      "  Batch 280/765, Loss: 10.0569, Time: 0.076s\n",
      "  Batch 290/765, Loss: 7.4454, Time: 0.074s\n",
      "  Batch 300/765, Loss: 9.3566, Time: 0.074s\n",
      "  Batch 310/765, Loss: 10.8888, Time: 0.084s\n",
      "  Batch 320/765, Loss: 10.9202, Time: 0.049s\n",
      "  Batch 330/765, Loss: 9.1495, Time: 0.076s\n",
      "  Batch 340/765, Loss: 9.2258, Time: 0.080s\n",
      "  Batch 350/765, Loss: 11.5922, Time: 0.070s\n",
      "  Batch 360/765, Loss: 9.5944, Time: 0.047s\n",
      "  Batch 370/765, Loss: 7.8832, Time: 0.072s\n",
      "  Batch 380/765, Loss: 10.7281, Time: 0.071s\n",
      "  Batch 390/765, Loss: 10.1006, Time: 0.071s\n",
      "  Batch 400/765, Loss: 11.0236, Time: 0.072s\n",
      "  Batch 410/765, Loss: 11.1802, Time: 0.073s\n",
      "  Batch 420/765, Loss: 8.9834, Time: 0.071s\n",
      "  Batch 430/765, Loss: 8.6553, Time: 0.072s\n",
      "  Batch 440/765, Loss: 9.6766, Time: 0.071s\n",
      "  Batch 450/765, Loss: 11.1716, Time: 0.046s\n",
      "  Batch 460/765, Loss: 8.8064, Time: 0.076s\n",
      "  Batch 470/765, Loss: 10.6599, Time: 0.073s\n",
      "  Batch 480/765, Loss: 11.1431, Time: 0.072s\n",
      "  Batch 490/765, Loss: 11.3338, Time: 0.071s\n",
      "  Batch 500/765, Loss: 8.2404, Time: 0.071s\n",
      "  Batch 510/765, Loss: 11.6332, Time: 0.075s\n",
      "  Batch 520/765, Loss: 10.7276, Time: 0.074s\n",
      "  Batch 530/765, Loss: 9.6555, Time: 0.071s\n",
      "  Batch 540/765, Loss: 7.9665, Time: 0.077s\n",
      "  Batch 550/765, Loss: 9.6371, Time: 0.075s\n",
      "  Batch 560/765, Loss: 11.0955, Time: 0.086s\n",
      "  Batch 570/765, Loss: 7.1803, Time: 0.071s\n",
      "  Batch 580/765, Loss: 11.7022, Time: 0.087s\n",
      "  Batch 590/765, Loss: 11.3998, Time: 0.071s\n",
      "  Batch 600/765, Loss: 8.6768, Time: 0.222s\n",
      "  Batch 610/765, Loss: 9.6417, Time: 0.047s\n",
      "  Batch 620/765, Loss: 12.0752, Time: 0.092s\n",
      "  Batch 630/765, Loss: 9.0346, Time: 0.071s\n",
      "  Batch 640/765, Loss: 9.7008, Time: 0.071s\n",
      "  Batch 650/765, Loss: 9.8643, Time: 0.070s\n",
      "  Batch 660/765, Loss: 8.9796, Time: 0.072s\n",
      "  Batch 670/765, Loss: 10.0112, Time: 0.075s\n",
      "  Batch 680/765, Loss: 6.7119, Time: 0.047s\n",
      "  Batch 690/765, Loss: 8.7752, Time: 0.071s\n",
      "  Batch 700/765, Loss: 10.3938, Time: 0.071s\n",
      "  Batch 710/765, Loss: 11.1193, Time: 0.071s\n",
      "  Batch 720/765, Loss: 10.5008, Time: 0.072s\n",
      "  Batch 730/765, Loss: 10.7540, Time: 0.075s\n",
      "  Batch 740/765, Loss: 11.0747, Time: 0.073s\n",
      "  Batch 750/765, Loss: 9.9880, Time: 0.072s\n",
      "  Batch 760/765, Loss: 8.2510, Time: 0.070s\n",
      "Epoch 3/5\n",
      "  Train Loss: 10.0695 (765 batches, avg batch time: 0.077s)\n",
      "  Val Loss: 9.3564 (validation time: 76.30s)\n",
      "  Epoch Time: 661.25s, Est. Remaining: 21.89 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.3564\n",
      "  Batch 10/765, Loss: 6.9873, Time: 0.071s\n",
      "  Batch 20/765, Loss: 7.6070, Time: 0.070s\n",
      "  Batch 30/765, Loss: 13.3414, Time: 0.084s\n",
      "  Batch 40/765, Loss: 10.6351, Time: 0.072s\n",
      "  Batch 50/765, Loss: 8.4407, Time: 0.071s\n",
      "  Batch 60/765, Loss: 9.5212, Time: 0.070s\n",
      "  Batch 70/765, Loss: 9.2060, Time: 0.070s\n",
      "  Batch 80/765, Loss: 8.7516, Time: 0.070s\n",
      "  Batch 90/765, Loss: 9.2270, Time: 0.069s\n",
      "  Batch 100/765, Loss: 9.9854, Time: 0.070s\n",
      "  Batch 110/765, Loss: 9.2756, Time: 0.075s\n",
      "  Batch 120/765, Loss: 8.7726, Time: 0.096s\n",
      "  Batch 130/765, Loss: 10.6510, Time: 0.072s\n",
      "  Batch 140/765, Loss: 8.4536, Time: 0.072s\n",
      "  Batch 150/765, Loss: 10.1789, Time: 0.073s\n",
      "  Batch 160/765, Loss: 11.5696, Time: 0.130s\n",
      "  Batch 170/765, Loss: 7.6082, Time: 0.050s\n",
      "  Batch 180/765, Loss: 10.0456, Time: 0.070s\n",
      "  Batch 190/765, Loss: 8.2441, Time: 0.075s\n",
      "  Batch 200/765, Loss: 12.8203, Time: 0.104s\n",
      "  Batch 210/765, Loss: 10.3190, Time: 0.071s\n",
      "  Batch 220/765, Loss: 10.5711, Time: 0.073s\n",
      "  Batch 230/765, Loss: 11.6651, Time: 0.071s\n",
      "  Batch 240/765, Loss: 9.4822, Time: 0.163s\n",
      "  Batch 250/765, Loss: 6.8615, Time: 0.079s\n",
      "  Batch 260/765, Loss: 8.2160, Time: 0.070s\n",
      "  Batch 270/765, Loss: 7.4764, Time: 0.071s\n",
      "  Batch 280/765, Loss: 11.7747, Time: 0.071s\n",
      "  Batch 290/765, Loss: 9.0915, Time: 0.070s\n",
      "  Batch 300/765, Loss: 8.4153, Time: 0.070s\n",
      "  Batch 310/765, Loss: 10.8601, Time: 0.070s\n",
      "  Batch 320/765, Loss: 9.7982, Time: 0.069s\n",
      "  Batch 330/765, Loss: 8.0465, Time: 0.070s\n",
      "  Batch 340/765, Loss: 6.5432, Time: 0.070s\n",
      "  Batch 350/765, Loss: 8.8261, Time: 0.070s\n",
      "  Batch 360/765, Loss: 11.5908, Time: 0.072s\n",
      "  Batch 370/765, Loss: 13.2076, Time: 0.087s\n",
      "  Batch 380/765, Loss: 11.3408, Time: 0.070s\n",
      "  Batch 390/765, Loss: 12.1073, Time: 0.069s\n",
      "  Batch 400/765, Loss: 8.6036, Time: 0.071s\n",
      "  Batch 410/765, Loss: 9.1459, Time: 0.070s\n",
      "  Batch 420/765, Loss: 7.8514, Time: 0.087s\n",
      "  Batch 430/765, Loss: 8.9273, Time: 0.071s\n",
      "  Batch 440/765, Loss: 10.5147, Time: 0.093s\n",
      "  Batch 450/765, Loss: 9.8191, Time: 0.069s\n",
      "  Batch 460/765, Loss: 9.8031, Time: 0.057s\n",
      "  Batch 470/765, Loss: 9.7866, Time: 0.070s\n",
      "  Batch 480/765, Loss: 9.6730, Time: 0.069s\n",
      "  Batch 490/765, Loss: 12.9493, Time: 0.073s\n",
      "  Batch 500/765, Loss: 10.7546, Time: 0.077s\n",
      "  Batch 510/765, Loss: 11.5135, Time: 0.069s\n",
      "  Batch 520/765, Loss: 6.8875, Time: 0.081s\n",
      "  Batch 530/765, Loss: 8.7796, Time: 0.072s\n",
      "  Batch 540/765, Loss: 9.6346, Time: 0.124s\n",
      "  Batch 550/765, Loss: 9.7809, Time: 0.069s\n",
      "  Batch 560/765, Loss: 8.1661, Time: 0.070s\n",
      "  Batch 570/765, Loss: 8.7547, Time: 0.080s\n",
      "  Batch 580/765, Loss: 10.1201, Time: 0.072s\n",
      "  Batch 590/765, Loss: 8.2472, Time: 0.069s\n",
      "  Batch 600/765, Loss: 8.3053, Time: 0.070s\n",
      "  Batch 610/765, Loss: 12.2918, Time: 0.072s\n",
      "  Batch 620/765, Loss: 10.9859, Time: 0.072s\n",
      "  Batch 630/765, Loss: 10.7304, Time: 0.070s\n",
      "  Batch 640/765, Loss: 8.1775, Time: 0.079s\n",
      "  Batch 650/765, Loss: 8.5558, Time: 0.069s\n",
      "  Batch 660/765, Loss: 9.3174, Time: 0.070s\n",
      "  Batch 670/765, Loss: 11.2211, Time: 0.070s\n",
      "  Batch 680/765, Loss: 11.0414, Time: 0.072s\n",
      "  Batch 690/765, Loss: 10.1308, Time: 0.071s\n",
      "  Batch 700/765, Loss: 9.2808, Time: 0.048s\n",
      "  Batch 710/765, Loss: 9.0557, Time: 0.106s\n",
      "  Batch 720/765, Loss: 10.2089, Time: 0.072s\n",
      "  Batch 730/765, Loss: 9.3465, Time: 0.068s\n",
      "  Batch 740/765, Loss: 9.2294, Time: 0.070s\n",
      "  Batch 750/765, Loss: 8.1258, Time: 0.071s\n",
      "  Batch 760/765, Loss: 10.5444, Time: 0.072s\n",
      "Epoch 4/5\n",
      "  Train Loss: 9.5844 (765 batches, avg batch time: 0.075s)\n",
      "  Val Loss: 9.1731 (validation time: 76.81s)\n",
      "  Epoch Time: 658.24s, Est. Remaining: 10.95 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.1731\n",
      "  Batch 10/765, Loss: 7.7009, Time: 0.072s\n",
      "  Batch 20/765, Loss: 9.7043, Time: 0.073s\n",
      "  Batch 30/765, Loss: 9.2675, Time: 0.073s\n",
      "  Batch 40/765, Loss: 12.2135, Time: 0.070s\n",
      "  Batch 50/765, Loss: 11.3325, Time: 0.069s\n",
      "  Batch 60/765, Loss: 9.2753, Time: 0.069s\n",
      "  Batch 70/765, Loss: 8.7382, Time: 0.070s\n",
      "  Batch 80/765, Loss: 9.7211, Time: 0.070s\n",
      "  Batch 90/765, Loss: 10.5278, Time: 0.073s\n",
      "  Batch 100/765, Loss: 9.2177, Time: 0.070s\n",
      "  Batch 110/765, Loss: 9.9712, Time: 0.080s\n",
      "  Batch 120/765, Loss: 10.7498, Time: 0.074s\n",
      "  Batch 130/765, Loss: 11.1582, Time: 0.074s\n",
      "  Batch 140/765, Loss: 9.9067, Time: 0.091s\n",
      "  Batch 150/765, Loss: 9.8082, Time: 0.071s\n",
      "  Batch 160/765, Loss: 7.7951, Time: 0.078s\n",
      "  Batch 170/765, Loss: 10.6078, Time: 0.070s\n",
      "  Batch 180/765, Loss: 9.3386, Time: 0.068s\n",
      "  Batch 190/765, Loss: 12.2657, Time: 0.069s\n",
      "  Batch 200/765, Loss: 11.4274, Time: 0.070s\n",
      "  Batch 210/765, Loss: 7.2031, Time: 0.069s\n",
      "  Batch 220/765, Loss: 9.3813, Time: 0.070s\n",
      "  Batch 230/765, Loss: 8.9290, Time: 0.069s\n",
      "  Batch 240/765, Loss: 9.5546, Time: 0.069s\n",
      "  Batch 250/765, Loss: 10.4301, Time: 0.070s\n",
      "  Batch 260/765, Loss: 9.9420, Time: 0.070s\n",
      "  Batch 270/765, Loss: 6.7892, Time: 0.082s\n",
      "  Batch 280/765, Loss: 9.2017, Time: 0.069s\n",
      "  Batch 290/765, Loss: 8.6691, Time: 0.078s\n",
      "  Batch 300/765, Loss: 10.2559, Time: 0.071s\n",
      "  Batch 310/765, Loss: 8.1168, Time: 0.071s\n",
      "  Batch 320/765, Loss: 9.3018, Time: 0.072s\n",
      "  Batch 330/765, Loss: 9.9589, Time: 0.070s\n",
      "  Batch 340/765, Loss: 7.8931, Time: 0.071s\n",
      "  Batch 350/765, Loss: 10.0640, Time: 0.073s\n",
      "  Batch 360/765, Loss: 10.4461, Time: 0.069s\n",
      "  Batch 370/765, Loss: 9.7706, Time: 0.069s\n",
      "  Batch 380/765, Loss: 6.3515, Time: 0.071s\n",
      "  Batch 390/765, Loss: 8.7222, Time: 0.045s\n",
      "  Batch 400/765, Loss: 10.7580, Time: 0.069s\n",
      "  Batch 410/765, Loss: 10.1771, Time: 0.070s\n",
      "  Batch 420/765, Loss: 10.4611, Time: 0.069s\n",
      "  Batch 430/765, Loss: 9.9875, Time: 0.077s\n",
      "  Batch 440/765, Loss: 6.9981, Time: 0.203s\n",
      "  Batch 450/765, Loss: 9.9834, Time: 0.068s\n",
      "  Batch 460/765, Loss: 9.7457, Time: 0.071s\n",
      "  Batch 470/765, Loss: 8.1205, Time: 0.071s\n",
      "  Batch 480/765, Loss: 8.9498, Time: 0.072s\n",
      "  Batch 490/765, Loss: 10.0042, Time: 0.073s\n",
      "  Batch 500/765, Loss: 9.2050, Time: 0.070s\n",
      "  Batch 510/765, Loss: 10.5037, Time: 0.069s\n",
      "  Batch 520/765, Loss: 11.2742, Time: 0.069s\n",
      "  Batch 530/765, Loss: 11.9246, Time: 0.070s\n",
      "  Batch 540/765, Loss: 9.1862, Time: 0.069s\n",
      "  Batch 550/765, Loss: 10.2104, Time: 0.072s\n",
      "  Batch 560/765, Loss: 9.1224, Time: 0.071s\n",
      "  Batch 570/765, Loss: 12.8888, Time: 0.081s\n",
      "  Batch 580/765, Loss: 12.4687, Time: 0.072s\n",
      "  Batch 590/765, Loss: 10.6584, Time: 0.071s\n",
      "  Batch 600/765, Loss: 7.7310, Time: 0.068s\n",
      "  Batch 610/765, Loss: 9.6319, Time: 0.069s\n",
      "  Batch 620/765, Loss: 8.0956, Time: 0.069s\n",
      "  Batch 630/765, Loss: 9.2739, Time: 0.046s\n",
      "  Batch 640/765, Loss: 6.9123, Time: 0.069s\n",
      "  Batch 650/765, Loss: 11.2991, Time: 0.087s\n",
      "  Batch 660/765, Loss: 6.7525, Time: 0.074s\n",
      "  Batch 670/765, Loss: 10.5507, Time: 0.072s\n",
      "  Batch 680/765, Loss: 10.3142, Time: 0.069s\n",
      "  Batch 690/765, Loss: 10.7291, Time: 0.068s\n",
      "  Batch 700/765, Loss: 7.7689, Time: 0.094s\n",
      "  Batch 710/765, Loss: 10.7281, Time: 0.049s\n",
      "  Batch 720/765, Loss: 8.8179, Time: 0.069s\n",
      "  Batch 730/765, Loss: 10.2516, Time: 0.104s\n",
      "  Batch 740/765, Loss: 8.4660, Time: 0.072s\n",
      "  Batch 750/765, Loss: 10.3492, Time: 0.072s\n",
      "  Batch 760/765, Loss: 9.1804, Time: 0.071s\n",
      "Epoch 5/5\n",
      "  Train Loss: 9.3915 (765 batches, avg batch time: 0.074s)\n",
      "  Val Loss: 9.4533 (validation time: 76.95s)\n",
      "  Epoch Time: 656.34s, Est. Remaining: 0.00 minutes\n",
      "  No improvement for 1/2 epochs\n",
      "Training completed in 54.75 minutes (3284.8 seconds)\n",
      "Loaded best model (version single_output_v1)\n",
      "Params: [2.09894729e+01 4.42327400e+01 3.24787652e-02 1.73104562e+01\n",
      " 2.23418722e+00], Validation MAPE: 14.3235%\n",
      "Resuming training from epoch 4\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 11.2939, Time: 0.072s\n",
      "  Batch 20/765, Loss: 11.6169, Time: 0.072s\n",
      "  Batch 30/765, Loss: 11.2774, Time: 0.071s\n",
      "  Batch 40/765, Loss: 9.9644, Time: 0.075s\n",
      "  Batch 50/765, Loss: 8.5508, Time: 0.083s\n",
      "  Batch 60/765, Loss: 11.8884, Time: 0.078s\n",
      "  Batch 70/765, Loss: 7.2328, Time: 0.073s\n",
      "  Batch 80/765, Loss: 9.9160, Time: 0.074s\n",
      "  Batch 90/765, Loss: 7.3848, Time: 0.047s\n",
      "  Batch 100/765, Loss: 8.2954, Time: 0.071s\n",
      "  Batch 110/765, Loss: 6.1651, Time: 0.104s\n",
      "  Batch 120/765, Loss: 12.8563, Time: 0.085s\n",
      "  Batch 130/765, Loss: 9.9097, Time: 0.076s\n",
      "  Batch 140/765, Loss: 6.7204, Time: 0.071s\n",
      "  Batch 150/765, Loss: 10.7533, Time: 0.048s\n",
      "  Batch 160/765, Loss: 8.0757, Time: 0.074s\n",
      "  Batch 170/765, Loss: 6.9149, Time: 0.072s\n",
      "  Batch 180/765, Loss: 12.0520, Time: 0.072s\n",
      "  Batch 190/765, Loss: 7.7016, Time: 0.073s\n",
      "  Batch 200/765, Loss: 7.7780, Time: 0.072s\n",
      "  Batch 210/765, Loss: 9.0754, Time: 0.078s\n",
      "  Batch 220/765, Loss: 9.5153, Time: 0.074s\n",
      "  Batch 230/765, Loss: 9.0610, Time: 0.179s\n",
      "  Batch 240/765, Loss: 11.5099, Time: 0.077s\n",
      "  Batch 250/765, Loss: 10.0610, Time: 0.070s\n",
      "  Batch 260/765, Loss: 10.1740, Time: 0.074s\n",
      "  Batch 270/765, Loss: 9.9317, Time: 0.070s\n",
      "  Batch 280/765, Loss: 9.6133, Time: 0.072s\n",
      "  Batch 290/765, Loss: 9.1140, Time: 0.073s\n",
      "  Batch 300/765, Loss: 8.5319, Time: 0.072s\n",
      "  Batch 310/765, Loss: 7.4152, Time: 0.075s\n",
      "  Batch 320/765, Loss: 8.0760, Time: 0.069s\n",
      "  Batch 330/765, Loss: 8.9879, Time: 0.047s\n",
      "  Batch 340/765, Loss: 11.6836, Time: 0.072s\n",
      "  Batch 350/765, Loss: 8.6498, Time: 0.079s\n",
      "  Batch 360/765, Loss: 7.1785, Time: 0.071s\n",
      "  Batch 370/765, Loss: 6.7745, Time: 0.071s\n",
      "  Batch 380/765, Loss: 8.7625, Time: 0.076s\n",
      "  Batch 390/765, Loss: 8.4021, Time: 0.070s\n",
      "  Batch 400/765, Loss: 9.2404, Time: 0.073s\n",
      "  Batch 410/765, Loss: 12.0833, Time: 0.076s\n",
      "  Batch 420/765, Loss: 8.1791, Time: 0.070s\n",
      "  Batch 430/765, Loss: 7.2325, Time: 0.071s\n",
      "  Batch 440/765, Loss: 9.8601, Time: 0.131s\n",
      "  Batch 450/765, Loss: 7.0640, Time: 0.075s\n",
      "  Batch 460/765, Loss: 11.7111, Time: 0.078s\n",
      "  Batch 470/765, Loss: 9.0741, Time: 0.071s\n",
      "  Batch 480/765, Loss: 11.4202, Time: 0.072s\n",
      "  Batch 490/765, Loss: 7.2144, Time: 0.072s\n",
      "  Batch 500/765, Loss: 7.0669, Time: 0.071s\n",
      "  Batch 510/765, Loss: 10.1913, Time: 0.071s\n",
      "  Batch 520/765, Loss: 10.7821, Time: 0.073s\n",
      "  Batch 530/765, Loss: 9.1376, Time: 0.069s\n",
      "  Batch 540/765, Loss: 9.2182, Time: 0.072s\n",
      "  Batch 550/765, Loss: 9.0404, Time: 0.071s\n",
      "  Batch 560/765, Loss: 7.6404, Time: 0.071s\n",
      "  Batch 570/765, Loss: 8.2506, Time: 0.071s\n",
      "  Batch 580/765, Loss: 10.4395, Time: 0.071s\n",
      "  Batch 590/765, Loss: 9.2275, Time: 0.071s\n",
      "  Batch 600/765, Loss: 9.5616, Time: 0.070s\n",
      "  Batch 610/765, Loss: 6.7583, Time: 0.049s\n",
      "  Batch 620/765, Loss: 7.4016, Time: 0.070s\n",
      "  Batch 630/765, Loss: 9.4364, Time: 0.073s\n",
      "  Batch 640/765, Loss: 10.7356, Time: 0.071s\n",
      "  Batch 650/765, Loss: 12.2716, Time: 0.081s\n",
      "  Batch 660/765, Loss: 9.5894, Time: 0.070s\n",
      "  Batch 670/765, Loss: 8.9833, Time: 0.074s\n",
      "  Batch 680/765, Loss: 9.3328, Time: 0.069s\n",
      "  Batch 690/765, Loss: 8.7482, Time: 0.072s\n",
      "  Batch 700/765, Loss: 7.9867, Time: 0.072s\n",
      "  Batch 710/765, Loss: 10.3816, Time: 0.046s\n",
      "  Batch 720/765, Loss: 6.0373, Time: 0.069s\n",
      "  Batch 730/765, Loss: 10.3913, Time: 0.073s\n",
      "  Batch 740/765, Loss: 7.8628, Time: 0.069s\n",
      "  Batch 750/765, Loss: 10.8852, Time: 0.069s\n",
      "  Batch 760/765, Loss: 14.6462, Time: 0.070s\n",
      "Epoch 5/5\n",
      "  Train Loss: 9.4228 (765 batches, avg batch time: 0.075s)\n",
      "  Val Loss: 9.2235 (validation time: 75.74s)\n",
      "  Epoch Time: 669.88s, Est. Remaining: 0.00 minutes\n",
      "  No improvement for 1/2 epochs\n",
      "Training completed in 11.16 minutes (669.9 seconds)\n",
      "Loaded best model (version single_output_v1)\n",
      "Params: [18.29659782 33.76749702  0.04846934 17.58042272  2.91651477], Validation MAPE: 14.3235%\n",
      "Model architecture has changed - detected parameter size mismatch.\n",
      "Removing incompatible checkpoint and starting fresh training.\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 51.9950, Time: 0.074s\n",
      "  Batch 20/765, Loss: 32.1764, Time: 0.085s\n",
      "  Batch 30/765, Loss: 23.6355, Time: 0.073s\n",
      "  Batch 40/765, Loss: 36.1736, Time: 0.072s\n",
      "  Batch 50/765, Loss: 27.0274, Time: 0.072s\n",
      "  Batch 60/765, Loss: 14.0929, Time: 0.072s\n",
      "  Batch 70/765, Loss: 17.2827, Time: 0.094s\n",
      "  Batch 80/765, Loss: 16.4952, Time: 0.048s\n",
      "  Batch 90/765, Loss: 25.8291, Time: 0.079s\n",
      "  Batch 100/765, Loss: 14.8279, Time: 0.083s\n",
      "  Batch 110/765, Loss: 12.2600, Time: 0.080s\n",
      "  Batch 120/765, Loss: 10.6005, Time: 0.076s\n",
      "  Batch 130/765, Loss: 9.5815, Time: 0.076s\n",
      "  Batch 140/765, Loss: 9.2576, Time: 0.072s\n",
      "  Batch 150/765, Loss: 13.3987, Time: 0.075s\n",
      "  Batch 160/765, Loss: 13.3520, Time: 0.072s\n",
      "  Batch 170/765, Loss: 13.4368, Time: 0.075s\n",
      "  Batch 180/765, Loss: 9.8105, Time: 0.052s\n",
      "  Batch 190/765, Loss: 9.1486, Time: 0.079s\n",
      "  Batch 200/765, Loss: 12.4060, Time: 0.073s\n",
      "  Batch 210/765, Loss: 11.3558, Time: 0.081s\n",
      "  Batch 220/765, Loss: 11.9628, Time: 0.074s\n",
      "  Batch 230/765, Loss: 8.9270, Time: 0.074s\n",
      "  Batch 240/765, Loss: 10.6850, Time: 0.078s\n",
      "  Batch 250/765, Loss: 11.2604, Time: 0.072s\n",
      "  Batch 260/765, Loss: 9.6825, Time: 0.074s\n",
      "  Batch 270/765, Loss: 7.5744, Time: 0.219s\n",
      "  Batch 280/765, Loss: 9.7583, Time: 0.075s\n",
      "  Batch 290/765, Loss: 9.8541, Time: 0.081s\n",
      "  Batch 300/765, Loss: 9.1008, Time: 0.078s\n",
      "  Batch 310/765, Loss: 9.0505, Time: 0.110s\n",
      "  Batch 320/765, Loss: 9.9630, Time: 0.084s\n",
      "  Batch 330/765, Loss: 11.7268, Time: 0.077s\n",
      "  Batch 340/765, Loss: 10.4232, Time: 0.051s\n",
      "  Batch 350/765, Loss: 11.2916, Time: 0.095s\n",
      "  Batch 360/765, Loss: 6.9037, Time: 0.077s\n",
      "  Batch 370/765, Loss: 7.4140, Time: 0.077s\n",
      "  Batch 380/765, Loss: 6.8462, Time: 0.076s\n",
      "  Batch 390/765, Loss: 11.7847, Time: 0.079s\n",
      "  Batch 400/765, Loss: 8.8679, Time: 0.084s\n",
      "  Batch 410/765, Loss: 7.8188, Time: 0.083s\n",
      "  Batch 420/765, Loss: 8.6925, Time: 0.074s\n",
      "  Batch 430/765, Loss: 9.2609, Time: 0.075s\n",
      "  Batch 440/765, Loss: 11.0708, Time: 0.080s\n",
      "  Batch 450/765, Loss: 8.3983, Time: 0.077s\n",
      "  Batch 460/765, Loss: 9.9600, Time: 0.074s\n",
      "  Batch 470/765, Loss: 11.8156, Time: 0.076s\n",
      "  Batch 480/765, Loss: 11.7137, Time: 0.074s\n",
      "  Batch 490/765, Loss: 8.6961, Time: 0.074s\n",
      "  Batch 500/765, Loss: 9.8419, Time: 0.076s\n",
      "  Batch 510/765, Loss: 10.4092, Time: 0.075s\n",
      "  Batch 520/765, Loss: 9.8327, Time: 0.051s\n",
      "  Batch 530/765, Loss: 9.4566, Time: 0.075s\n",
      "  Batch 540/765, Loss: 8.5941, Time: 0.084s\n",
      "  Batch 550/765, Loss: 9.9697, Time: 0.072s\n",
      "  Batch 560/765, Loss: 7.3827, Time: 0.071s\n",
      "  Batch 570/765, Loss: 10.0858, Time: 0.073s\n",
      "  Batch 580/765, Loss: 12.1875, Time: 0.074s\n",
      "  Batch 590/765, Loss: 9.0092, Time: 0.073s\n",
      "  Batch 600/765, Loss: 9.2420, Time: 0.074s\n",
      "  Batch 610/765, Loss: 11.9305, Time: 0.072s\n",
      "  Batch 620/765, Loss: 11.1867, Time: 0.073s\n",
      "  Batch 630/765, Loss: 11.6748, Time: 0.074s\n",
      "  Batch 640/765, Loss: 7.9771, Time: 0.114s\n",
      "  Batch 650/765, Loss: 6.6666, Time: 0.075s\n",
      "  Batch 660/765, Loss: 10.3145, Time: 0.072s\n",
      "  Batch 670/765, Loss: 11.4737, Time: 0.096s\n",
      "  Batch 680/765, Loss: 8.3364, Time: 0.071s\n",
      "  Batch 690/765, Loss: 9.0988, Time: 0.044s\n",
      "  Batch 700/765, Loss: 7.3088, Time: 0.043s\n",
      "  Batch 710/765, Loss: 10.3627, Time: 0.078s\n",
      "  Batch 720/765, Loss: 8.5324, Time: 0.079s\n",
      "  Batch 730/765, Loss: 9.1427, Time: 0.048s\n",
      "  Batch 740/765, Loss: 7.9506, Time: 0.047s\n",
      "  Batch 750/765, Loss: 10.3139, Time: 0.044s\n",
      "  Batch 760/765, Loss: 9.1113, Time: 0.070s\n",
      "Epoch 1/5\n",
      "  Train Loss: 13.3992 (765 batches, avg batch time: 0.076s)\n",
      "  Val Loss: 9.2609 (validation time: 75.81s)\n",
      "  Epoch Time: 674.58s, Est. Remaining: 44.97 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.2609\n",
      "  Batch 10/765, Loss: 8.5612, Time: 0.072s\n",
      "  Batch 20/765, Loss: 10.1628, Time: 0.076s\n",
      "  Batch 30/765, Loss: 7.9897, Time: 0.075s\n",
      "  Batch 40/765, Loss: 9.1956, Time: 0.074s\n",
      "  Batch 50/765, Loss: 8.2045, Time: 0.072s\n",
      "  Batch 60/765, Loss: 8.3778, Time: 0.073s\n",
      "  Batch 70/765, Loss: 8.5697, Time: 0.082s\n",
      "  Batch 80/765, Loss: 8.9167, Time: 0.073s\n",
      "  Batch 90/765, Loss: 9.4361, Time: 0.072s\n",
      "  Batch 100/765, Loss: 9.9310, Time: 0.071s\n",
      "  Batch 110/765, Loss: 10.3076, Time: 0.073s\n",
      "  Batch 120/765, Loss: 9.9941, Time: 0.077s\n",
      "  Batch 130/765, Loss: 7.6678, Time: 0.074s\n",
      "  Batch 140/765, Loss: 9.3975, Time: 0.073s\n",
      "  Batch 150/765, Loss: 12.1197, Time: 0.073s\n",
      "  Batch 160/765, Loss: 10.6587, Time: 0.084s\n",
      "  Batch 170/765, Loss: 11.2806, Time: 0.076s\n",
      "  Batch 180/765, Loss: 8.8893, Time: 0.073s\n",
      "  Batch 190/765, Loss: 9.8648, Time: 0.073s\n",
      "  Batch 200/765, Loss: 9.2266, Time: 0.049s\n",
      "  Batch 210/765, Loss: 9.4682, Time: 0.073s\n",
      "  Batch 220/765, Loss: 8.5095, Time: 0.074s\n",
      "  Batch 230/765, Loss: 8.9734, Time: 0.053s\n",
      "  Batch 240/765, Loss: 6.2574, Time: 0.073s\n",
      "  Batch 250/765, Loss: 7.7899, Time: 0.078s\n",
      "  Batch 260/765, Loss: 13.1216, Time: 0.076s\n",
      "  Batch 270/765, Loss: 9.3180, Time: 0.076s\n",
      "  Batch 280/765, Loss: 10.2479, Time: 0.073s\n",
      "  Batch 290/765, Loss: 10.6092, Time: 0.075s\n",
      "  Batch 300/765, Loss: 10.4375, Time: 0.075s\n",
      "  Batch 310/765, Loss: 10.1732, Time: 0.074s\n",
      "  Batch 320/765, Loss: 12.9071, Time: 0.074s\n",
      "  Batch 330/765, Loss: 9.3105, Time: 0.077s\n",
      "  Batch 340/765, Loss: 8.1994, Time: 0.049s\n",
      "  Batch 350/765, Loss: 8.4261, Time: 0.076s\n",
      "  Batch 360/765, Loss: 8.4746, Time: 0.073s\n",
      "  Batch 370/765, Loss: 8.5207, Time: 0.072s\n",
      "  Batch 380/765, Loss: 8.9069, Time: 0.050s\n",
      "  Batch 390/765, Loss: 7.1919, Time: 0.072s\n",
      "  Batch 400/765, Loss: 11.4627, Time: 0.073s\n",
      "  Batch 410/765, Loss: 10.4551, Time: 0.201s\n",
      "  Batch 420/765, Loss: 9.9176, Time: 0.077s\n",
      "  Batch 430/765, Loss: 7.9487, Time: 0.074s\n",
      "  Batch 440/765, Loss: 10.5776, Time: 0.074s\n",
      "  Batch 450/765, Loss: 5.5387, Time: 0.076s\n",
      "  Batch 460/765, Loss: 8.9371, Time: 0.076s\n",
      "  Batch 470/765, Loss: 11.1669, Time: 0.051s\n",
      "  Batch 480/765, Loss: 7.3868, Time: 0.074s\n",
      "  Batch 490/765, Loss: 8.7283, Time: 0.073s\n",
      "  Batch 500/765, Loss: 7.6708, Time: 0.085s\n",
      "  Batch 510/765, Loss: 9.7946, Time: 0.073s\n",
      "  Batch 520/765, Loss: 8.8324, Time: 0.073s\n",
      "  Batch 530/765, Loss: 12.1200, Time: 0.072s\n",
      "  Batch 540/765, Loss: 10.3484, Time: 0.075s\n",
      "  Batch 550/765, Loss: 8.6921, Time: 0.073s\n",
      "  Batch 560/765, Loss: 9.0136, Time: 0.075s\n",
      "  Batch 570/765, Loss: 10.2969, Time: 0.073s\n",
      "  Batch 580/765, Loss: 8.9840, Time: 0.073s\n",
      "  Batch 590/765, Loss: 10.4184, Time: 0.079s\n",
      "  Batch 600/765, Loss: 8.6659, Time: 0.075s\n",
      "  Batch 610/765, Loss: 10.3810, Time: 0.074s\n",
      "  Batch 620/765, Loss: 11.4317, Time: 0.072s\n",
      "  Batch 630/765, Loss: 9.2806, Time: 0.073s\n",
      "  Batch 640/765, Loss: 10.2972, Time: 0.117s\n",
      "  Batch 650/765, Loss: 6.8664, Time: 0.072s\n",
      "  Batch 660/765, Loss: 8.8072, Time: 0.075s\n",
      "  Batch 670/765, Loss: 9.3188, Time: 0.049s\n",
      "  Batch 680/765, Loss: 8.8760, Time: 0.072s\n",
      "  Batch 690/765, Loss: 8.2905, Time: 0.074s\n",
      "  Batch 700/765, Loss: 9.3715, Time: 0.073s\n",
      "  Batch 710/765, Loss: 10.0086, Time: 0.075s\n",
      "  Batch 720/765, Loss: 11.7781, Time: 0.080s\n",
      "  Batch 730/765, Loss: 12.0713, Time: 0.074s\n",
      "  Batch 740/765, Loss: 8.7043, Time: 0.076s\n",
      "  Batch 750/765, Loss: 12.6081, Time: 0.098s\n",
      "  Batch 760/765, Loss: 9.6994, Time: 0.075s\n",
      "Epoch 2/5\n",
      "  Train Loss: 9.2853 (765 batches, avg batch time: 0.077s)\n",
      "  Val Loss: 8.9425 (validation time: 78.43s)\n",
      "  Epoch Time: 661.78s, Est. Remaining: 33.41 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.9425\n",
      "  Batch 10/765, Loss: 10.5924, Time: 0.081s\n",
      "  Batch 20/765, Loss: 9.3521, Time: 0.076s\n",
      "  Batch 30/765, Loss: 10.3318, Time: 0.077s\n",
      "  Batch 40/765, Loss: 12.0081, Time: 0.076s\n",
      "  Batch 50/765, Loss: 9.5082, Time: 0.077s\n",
      "  Batch 60/765, Loss: 5.2435, Time: 0.049s\n",
      "  Batch 70/765, Loss: 9.7338, Time: 0.072s\n",
      "  Batch 80/765, Loss: 10.2487, Time: 0.075s\n",
      "  Batch 90/765, Loss: 8.9459, Time: 0.075s\n",
      "  Batch 100/765, Loss: 12.7282, Time: 0.072s\n",
      "  Batch 110/765, Loss: 9.4644, Time: 0.075s\n",
      "  Batch 120/765, Loss: 7.8722, Time: 0.071s\n",
      "  Batch 130/765, Loss: 11.7595, Time: 0.101s\n",
      "  Batch 140/765, Loss: 6.4934, Time: 0.072s\n",
      "  Batch 150/765, Loss: 8.8900, Time: 0.073s\n",
      "  Batch 160/765, Loss: 11.8171, Time: 0.101s\n",
      "  Batch 170/765, Loss: 9.2072, Time: 0.075s\n",
      "  Batch 180/765, Loss: 7.5883, Time: 0.074s\n",
      "  Batch 190/765, Loss: 8.3189, Time: 0.073s\n",
      "  Batch 200/765, Loss: 9.5570, Time: 0.075s\n",
      "  Batch 210/765, Loss: 8.9878, Time: 0.075s\n",
      "  Batch 220/765, Loss: 9.0940, Time: 0.075s\n",
      "  Batch 230/765, Loss: 10.0710, Time: 0.076s\n",
      "  Batch 240/765, Loss: 9.5448, Time: 0.130s\n",
      "  Batch 250/765, Loss: 11.1133, Time: 0.093s\n",
      "  Batch 260/765, Loss: 8.8499, Time: 0.074s\n",
      "  Batch 270/765, Loss: 5.7266, Time: 0.073s\n",
      "  Batch 280/765, Loss: 8.5806, Time: 0.074s\n",
      "  Batch 290/765, Loss: 8.9040, Time: 0.072s\n",
      "  Batch 300/765, Loss: 9.5914, Time: 0.075s\n",
      "  Batch 310/765, Loss: 7.8741, Time: 0.082s\n",
      "  Batch 320/765, Loss: 9.2125, Time: 0.076s\n",
      "  Batch 330/765, Loss: 9.7248, Time: 0.074s\n",
      "  Batch 340/765, Loss: 7.3056, Time: 0.076s\n",
      "  Batch 350/765, Loss: 6.4707, Time: 0.051s\n",
      "  Batch 360/765, Loss: 7.6953, Time: 0.075s\n",
      "  Batch 370/765, Loss: 9.5436, Time: 0.083s\n",
      "  Batch 380/765, Loss: 10.7589, Time: 0.053s\n",
      "  Batch 390/765, Loss: 6.2262, Time: 0.072s\n",
      "  Batch 400/765, Loss: 8.8738, Time: 0.074s\n",
      "  Batch 410/765, Loss: 9.5569, Time: 0.072s\n",
      "  Batch 420/765, Loss: 9.2052, Time: 0.092s\n",
      "  Batch 430/765, Loss: 9.6642, Time: 0.095s\n",
      "  Batch 440/765, Loss: 10.0158, Time: 0.073s\n",
      "  Batch 450/765, Loss: 9.2268, Time: 0.073s\n",
      "  Batch 460/765, Loss: 8.9075, Time: 0.076s\n",
      "  Batch 470/765, Loss: 9.7451, Time: 0.073s\n",
      "  Batch 480/765, Loss: 8.4838, Time: 0.073s\n",
      "  Batch 490/765, Loss: 9.8200, Time: 0.080s\n",
      "  Batch 500/765, Loss: 9.1228, Time: 0.074s\n",
      "  Batch 510/765, Loss: 9.9556, Time: 0.074s\n",
      "  Batch 520/765, Loss: 8.1249, Time: 0.073s\n",
      "  Batch 530/765, Loss: 8.2084, Time: 0.075s\n",
      "  Batch 540/765, Loss: 10.7529, Time: 0.075s\n",
      "  Batch 550/765, Loss: 10.2931, Time: 0.073s\n",
      "  Batch 560/765, Loss: 10.5997, Time: 0.112s\n",
      "  Batch 570/765, Loss: 11.2981, Time: 0.071s\n",
      "  Batch 580/765, Loss: 9.4685, Time: 0.050s\n",
      "  Batch 590/765, Loss: 8.5683, Time: 0.074s\n",
      "  Batch 600/765, Loss: 7.8261, Time: 0.073s\n",
      "  Batch 610/765, Loss: 10.1475, Time: 0.072s\n",
      "  Batch 620/765, Loss: 7.8412, Time: 0.072s\n",
      "  Batch 630/765, Loss: 8.0169, Time: 0.073s\n",
      "  Batch 640/765, Loss: 10.1846, Time: 0.073s\n",
      "  Batch 650/765, Loss: 8.4277, Time: 0.073s\n",
      "  Batch 660/765, Loss: 7.8411, Time: 0.085s\n",
      "  Batch 670/765, Loss: 9.5602, Time: 0.077s\n",
      "  Batch 680/765, Loss: 8.1969, Time: 0.074s\n",
      "  Batch 690/765, Loss: 7.9829, Time: 0.130s\n",
      "  Batch 700/765, Loss: 8.3915, Time: 0.076s\n",
      "  Batch 710/765, Loss: 9.6685, Time: 0.073s\n",
      "  Batch 720/765, Loss: 6.7874, Time: 0.075s\n",
      "  Batch 730/765, Loss: 8.6915, Time: 0.076s\n",
      "  Batch 740/765, Loss: 10.2353, Time: 0.073s\n",
      "  Batch 750/765, Loss: 8.5661, Time: 0.080s\n",
      "  Batch 760/765, Loss: 7.8140, Time: 0.074s\n",
      "Epoch 3/5\n",
      "  Train Loss: 9.1375 (765 batches, avg batch time: 0.078s)\n",
      "  Val Loss: 8.8333 (validation time: 75.59s)\n",
      "  Epoch Time: 657.50s, Est. Remaining: 22.15 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.8333\n",
      "  Batch 10/765, Loss: 10.2514, Time: 0.072s\n",
      "  Batch 20/765, Loss: 12.2487, Time: 0.075s\n",
      "  Batch 30/765, Loss: 9.5180, Time: 0.073s\n",
      "  Batch 40/765, Loss: 9.2198, Time: 0.084s\n",
      "  Batch 50/765, Loss: 11.4903, Time: 0.076s\n",
      "  Batch 60/765, Loss: 8.5176, Time: 0.048s\n",
      "  Batch 70/765, Loss: 7.8036, Time: 0.048s\n",
      "  Batch 80/765, Loss: 9.4127, Time: 0.054s\n",
      "  Batch 90/765, Loss: 10.5124, Time: 0.097s\n",
      "  Batch 100/765, Loss: 8.1029, Time: 0.073s\n",
      "  Batch 110/765, Loss: 11.2737, Time: 0.072s\n",
      "  Batch 120/765, Loss: 11.4596, Time: 0.073s\n",
      "  Batch 130/765, Loss: 8.3172, Time: 0.108s\n",
      "  Batch 140/765, Loss: 6.8657, Time: 0.072s\n",
      "  Batch 150/765, Loss: 8.5589, Time: 0.073s\n",
      "  Batch 160/765, Loss: 9.0329, Time: 0.072s\n",
      "  Batch 170/765, Loss: 7.7219, Time: 0.075s\n",
      "  Batch 180/765, Loss: 7.2564, Time: 0.072s\n",
      "  Batch 190/765, Loss: 6.2059, Time: 0.075s\n",
      "  Batch 200/765, Loss: 10.3924, Time: 0.078s\n",
      "  Batch 210/765, Loss: 10.6242, Time: 0.048s\n",
      "  Batch 220/765, Loss: 7.6417, Time: 0.073s\n",
      "  Batch 230/765, Loss: 6.8538, Time: 0.047s\n",
      "  Batch 240/765, Loss: 9.7399, Time: 0.072s\n",
      "  Batch 250/765, Loss: 10.5553, Time: 0.072s\n",
      "  Batch 260/765, Loss: 11.5439, Time: 0.073s\n",
      "  Batch 270/765, Loss: 10.8382, Time: 0.047s\n",
      "  Batch 280/765, Loss: 10.6404, Time: 0.073s\n",
      "  Batch 290/765, Loss: 8.6650, Time: 0.072s\n",
      "  Batch 300/765, Loss: 11.3079, Time: 0.072s\n",
      "  Batch 310/765, Loss: 8.6366, Time: 0.072s\n",
      "  Batch 320/765, Loss: 10.6752, Time: 0.073s\n",
      "  Batch 330/765, Loss: 9.4796, Time: 0.072s\n",
      "  Batch 340/765, Loss: 11.1331, Time: 0.072s\n",
      "  Batch 350/765, Loss: 8.5672, Time: 0.073s\n",
      "  Batch 360/765, Loss: 9.6634, Time: 0.074s\n",
      "  Batch 370/765, Loss: 9.6080, Time: 0.071s\n",
      "  Batch 380/765, Loss: 9.6183, Time: 0.074s\n",
      "  Batch 390/765, Loss: 10.5783, Time: 0.075s\n",
      "  Batch 400/765, Loss: 7.5699, Time: 0.073s\n",
      "  Batch 410/765, Loss: 8.9368, Time: 0.073s\n",
      "  Batch 420/765, Loss: 8.6383, Time: 0.315s\n",
      "  Batch 430/765, Loss: 7.4898, Time: 0.074s\n",
      "  Batch 440/765, Loss: 8.3191, Time: 0.075s\n",
      "  Batch 450/765, Loss: 10.0162, Time: 0.075s\n",
      "  Batch 460/765, Loss: 8.6802, Time: 0.085s\n",
      "  Batch 470/765, Loss: 8.9506, Time: 0.077s\n",
      "  Batch 480/765, Loss: 6.4749, Time: 0.074s\n",
      "  Batch 490/765, Loss: 10.3621, Time: 0.072s\n",
      "  Batch 500/765, Loss: 8.4518, Time: 0.048s\n",
      "  Batch 510/765, Loss: 9.9976, Time: 0.073s\n",
      "  Batch 520/765, Loss: 8.3906, Time: 0.072s\n",
      "  Batch 530/765, Loss: 7.7340, Time: 0.072s\n",
      "  Batch 540/765, Loss: 8.2736, Time: 0.071s\n",
      "  Batch 550/765, Loss: 10.3392, Time: 0.073s\n",
      "  Batch 560/765, Loss: 9.8778, Time: 0.076s\n",
      "  Batch 570/765, Loss: 10.3813, Time: 0.055s\n",
      "  Batch 580/765, Loss: 8.1303, Time: 0.072s\n",
      "  Batch 590/765, Loss: 7.2984, Time: 0.127s\n",
      "  Batch 600/765, Loss: 6.6171, Time: 0.074s\n",
      "  Batch 610/765, Loss: 8.0289, Time: 0.073s\n",
      "  Batch 620/765, Loss: 12.6064, Time: 0.073s\n",
      "  Batch 630/765, Loss: 7.1981, Time: 0.073s\n",
      "  Batch 640/765, Loss: 11.2505, Time: 0.082s\n",
      "  Batch 650/765, Loss: 8.6306, Time: 0.049s\n",
      "  Batch 660/765, Loss: 9.8893, Time: 0.074s\n",
      "  Batch 670/765, Loss: 8.8027, Time: 0.074s\n",
      "  Batch 680/765, Loss: 11.1011, Time: 0.074s\n",
      "  Batch 690/765, Loss: 8.0578, Time: 0.075s\n",
      "  Batch 700/765, Loss: 9.1842, Time: 0.074s\n",
      "  Batch 710/765, Loss: 9.1543, Time: 0.078s\n",
      "  Batch 720/765, Loss: 9.4188, Time: 0.077s\n",
      "  Batch 730/765, Loss: 8.7997, Time: 0.077s\n",
      "  Batch 740/765, Loss: 8.7043, Time: 0.077s\n",
      "  Batch 750/765, Loss: 11.5531, Time: 0.058s\n",
      "  Batch 760/765, Loss: 8.9795, Time: 0.073s\n",
      "Epoch 4/5\n",
      "  Train Loss: 9.0266 (765 batches, avg batch time: 0.077s)\n",
      "  Val Loss: 8.8284 (validation time: 78.57s)\n",
      "  Epoch Time: 659.54s, Est. Remaining: 11.06 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.8284\n",
      "  Batch 10/765, Loss: 11.8664, Time: 0.083s\n",
      "  Batch 20/765, Loss: 9.1643, Time: 0.078s\n",
      "  Batch 30/765, Loss: 9.5089, Time: 0.074s\n",
      "  Batch 40/765, Loss: 14.6653, Time: 0.075s\n",
      "  Batch 50/765, Loss: 12.6343, Time: 0.075s\n",
      "  Batch 60/765, Loss: 9.7003, Time: 0.074s\n",
      "  Batch 70/765, Loss: 8.0188, Time: 0.074s\n",
      "  Batch 80/765, Loss: 5.7658, Time: 0.047s\n",
      "  Batch 90/765, Loss: 9.8492, Time: 0.072s\n",
      "  Batch 100/765, Loss: 9.4603, Time: 0.082s\n",
      "  Batch 110/765, Loss: 7.5338, Time: 0.053s\n",
      "  Batch 120/765, Loss: 9.9710, Time: 0.073s\n",
      "  Batch 130/765, Loss: 9.8320, Time: 0.073s\n",
      "  Batch 140/765, Loss: 8.9081, Time: 0.074s\n",
      "  Batch 150/765, Loss: 10.7256, Time: 0.072s\n",
      "  Batch 160/765, Loss: 7.4379, Time: 0.072s\n",
      "  Batch 170/765, Loss: 6.2861, Time: 0.073s\n",
      "  Batch 180/765, Loss: 7.8892, Time: 0.074s\n",
      "  Batch 190/765, Loss: 10.1473, Time: 0.094s\n",
      "  Batch 200/765, Loss: 7.7224, Time: 0.085s\n",
      "  Batch 210/765, Loss: 7.1095, Time: 0.072s\n",
      "  Batch 220/765, Loss: 7.5452, Time: 0.100s\n",
      "  Batch 230/765, Loss: 8.1476, Time: 0.076s\n",
      "  Batch 240/765, Loss: 9.0107, Time: 0.073s\n",
      "  Batch 250/765, Loss: 7.8301, Time: 0.072s\n",
      "  Batch 260/765, Loss: 8.4849, Time: 0.072s\n",
      "  Batch 270/765, Loss: 7.3223, Time: 0.074s\n",
      "  Batch 280/765, Loss: 8.4672, Time: 0.073s\n",
      "  Batch 290/765, Loss: 8.4928, Time: 0.078s\n",
      "  Batch 300/765, Loss: 8.7367, Time: 0.079s\n",
      "  Batch 310/765, Loss: 9.3010, Time: 0.077s\n",
      "  Batch 320/765, Loss: 11.1194, Time: 0.072s\n",
      "  Batch 330/765, Loss: 9.2285, Time: 0.071s\n",
      "  Batch 340/765, Loss: 7.8477, Time: 0.071s\n",
      "  Batch 350/765, Loss: 9.0822, Time: 0.076s\n",
      "  Batch 360/765, Loss: 7.4729, Time: 0.074s\n",
      "  Batch 370/765, Loss: 7.0000, Time: 0.091s\n",
      "  Batch 380/765, Loss: 7.6797, Time: 0.090s\n",
      "  Batch 390/765, Loss: 5.1258, Time: 0.089s\n",
      "  Batch 400/765, Loss: 11.0170, Time: 0.094s\n",
      "  Batch 410/765, Loss: 12.9317, Time: 0.079s\n",
      "  Batch 420/765, Loss: 7.9493, Time: 0.072s\n",
      "  Batch 430/765, Loss: 11.5835, Time: 0.053s\n",
      "  Batch 440/765, Loss: 7.7962, Time: 0.101s\n",
      "  Batch 450/765, Loss: 9.8067, Time: 0.073s\n",
      "  Batch 460/765, Loss: 9.9821, Time: 0.074s\n",
      "  Batch 470/765, Loss: 8.8279, Time: 0.052s\n",
      "  Batch 480/765, Loss: 9.5017, Time: 0.073s\n",
      "  Batch 490/765, Loss: 8.6705, Time: 0.073s\n",
      "  Batch 500/765, Loss: 11.3515, Time: 0.074s\n",
      "  Batch 510/765, Loss: 8.6893, Time: 0.075s\n",
      "  Batch 520/765, Loss: 9.4822, Time: 0.071s\n",
      "  Batch 530/765, Loss: 7.9626, Time: 0.050s\n",
      "  Batch 540/765, Loss: 10.6816, Time: 0.076s\n",
      "  Batch 550/765, Loss: 7.1992, Time: 0.084s\n",
      "  Batch 560/765, Loss: 10.0645, Time: 0.073s\n",
      "  Batch 570/765, Loss: 7.8592, Time: 0.086s\n",
      "  Batch 580/765, Loss: 6.7736, Time: 0.073s\n",
      "  Batch 590/765, Loss: 10.9450, Time: 0.075s\n",
      "  Batch 600/765, Loss: 7.8120, Time: 0.074s\n",
      "  Batch 610/765, Loss: 7.5009, Time: 0.092s\n",
      "  Batch 620/765, Loss: 10.2667, Time: 0.075s\n",
      "  Batch 630/765, Loss: 8.1258, Time: 0.074s\n",
      "  Batch 640/765, Loss: 8.3304, Time: 0.074s\n",
      "  Batch 650/765, Loss: 9.3935, Time: 0.074s\n",
      "  Batch 660/765, Loss: 7.2003, Time: 0.048s\n",
      "  Batch 670/765, Loss: 11.1198, Time: 0.078s\n",
      "  Batch 680/765, Loss: 7.6845, Time: 0.074s\n",
      "  Batch 690/765, Loss: 9.2523, Time: 0.075s\n",
      "  Batch 700/765, Loss: 9.8346, Time: 0.090s\n",
      "  Batch 710/765, Loss: 7.9728, Time: 0.072s\n",
      "  Batch 720/765, Loss: 8.0615, Time: 0.073s\n",
      "  Batch 730/765, Loss: 9.5352, Time: 0.074s\n",
      "  Batch 740/765, Loss: 10.5881, Time: 0.076s\n",
      "  Batch 750/765, Loss: 6.9417, Time: 0.072s\n",
      "  Batch 760/765, Loss: 7.6231, Time: 0.073s\n",
      "Epoch 5/5\n",
      "  Train Loss: 9.0710 (765 batches, avg batch time: 0.077s)\n",
      "  Val Loss: 8.8005 (validation time: 75.73s)\n",
      "  Epoch Time: 655.87s, Est. Remaining: 0.00 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.8005\n",
      "Training completed in 55.16 minutes (3309.5 seconds)\n",
      "Loaded best model (version single_output_v1)\n",
      "Params: [1.83896252e+01 4.97870778e+01 2.45246325e-02 2.21847065e+01\n",
      " 2.57622793e+00], Validation MAPE: 13.5046%\n",
      "Model architecture has changed - detected parameter size mismatch.\n",
      "Removing incompatible checkpoint and starting fresh training.\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 21.7754, Time: 0.074s\n",
      "  Batch 20/765, Loss: 17.4635, Time: 0.072s\n",
      "  Batch 30/765, Loss: 26.9054, Time: 0.074s\n",
      "  Batch 40/765, Loss: 17.1485, Time: 0.079s\n",
      "  Batch 50/765, Loss: 27.2690, Time: 0.067s\n",
      "  Batch 60/765, Loss: 18.0303, Time: 0.074s\n",
      "  Batch 70/765, Loss: 18.9542, Time: 0.083s\n",
      "  Batch 80/765, Loss: 13.3502, Time: 0.065s\n",
      "  Batch 90/765, Loss: 11.2790, Time: 0.074s\n",
      "  Batch 100/765, Loss: 14.7190, Time: 0.134s\n",
      "  Batch 110/765, Loss: 15.3994, Time: 0.074s\n",
      "  Batch 120/765, Loss: 12.8650, Time: 0.078s\n",
      "  Batch 130/765, Loss: 10.9229, Time: 0.073s\n",
      "  Batch 140/765, Loss: 10.1432, Time: 0.075s\n",
      "  Batch 150/765, Loss: 14.5896, Time: 0.076s\n",
      "  Batch 160/765, Loss: 14.3326, Time: 0.073s\n",
      "  Batch 170/765, Loss: 15.4690, Time: 0.072s\n",
      "  Batch 180/765, Loss: 7.1032, Time: 0.075s\n",
      "  Batch 190/765, Loss: 9.7195, Time: 0.075s\n",
      "  Batch 200/765, Loss: 7.9978, Time: 0.076s\n",
      "  Batch 210/765, Loss: 8.8636, Time: 0.089s\n",
      "  Batch 220/765, Loss: 9.3107, Time: 0.073s\n",
      "  Batch 230/765, Loss: 9.0607, Time: 0.075s\n",
      "  Batch 240/765, Loss: 8.2426, Time: 0.151s\n",
      "  Batch 250/765, Loss: 10.9247, Time: 0.078s\n",
      "  Batch 260/765, Loss: 9.9727, Time: 0.075s\n",
      "  Batch 270/765, Loss: 10.5756, Time: 0.074s\n",
      "  Batch 280/765, Loss: 11.0320, Time: 0.075s\n",
      "  Batch 290/765, Loss: 9.6672, Time: 0.076s\n",
      "  Batch 300/765, Loss: 13.3550, Time: 0.096s\n",
      "  Batch 310/765, Loss: 10.6548, Time: 0.073s\n",
      "  Batch 320/765, Loss: 10.6375, Time: 0.073s\n",
      "  Batch 330/765, Loss: 9.0018, Time: 0.072s\n",
      "  Batch 340/765, Loss: 8.5024, Time: 0.075s\n",
      "  Batch 350/765, Loss: 9.1530, Time: 0.071s\n",
      "  Batch 360/765, Loss: 9.9786, Time: 0.072s\n",
      "  Batch 370/765, Loss: 8.4081, Time: 0.074s\n",
      "  Batch 380/765, Loss: 10.2097, Time: 0.074s\n",
      "  Batch 390/765, Loss: 7.7136, Time: 0.074s\n",
      "  Batch 400/765, Loss: 9.7705, Time: 0.048s\n",
      "  Batch 410/765, Loss: 11.5404, Time: 0.135s\n",
      "  Batch 420/765, Loss: 11.3845, Time: 0.077s\n",
      "  Batch 430/765, Loss: 10.3839, Time: 0.081s\n",
      "  Batch 440/765, Loss: 8.4182, Time: 0.075s\n",
      "  Batch 450/765, Loss: 9.8153, Time: 0.074s\n",
      "  Batch 460/765, Loss: 9.9028, Time: 0.074s\n",
      "  Batch 470/765, Loss: 10.8534, Time: 0.096s\n",
      "  Batch 480/765, Loss: 13.6452, Time: 0.083s\n",
      "  Batch 490/765, Loss: 11.8967, Time: 0.093s\n",
      "  Batch 500/765, Loss: 8.9357, Time: 0.072s\n",
      "  Batch 510/765, Loss: 8.3255, Time: 0.075s\n",
      "  Batch 520/765, Loss: 9.7001, Time: 0.073s\n",
      "  Batch 530/765, Loss: 14.0027, Time: 0.074s\n",
      "  Batch 540/765, Loss: 9.5527, Time: 0.074s\n",
      "  Batch 550/765, Loss: 8.5141, Time: 0.075s\n",
      "  Batch 560/765, Loss: 9.9080, Time: 0.076s\n",
      "  Batch 570/765, Loss: 9.0600, Time: 0.078s\n",
      "  Batch 580/765, Loss: 9.3653, Time: 0.079s\n",
      "  Batch 590/765, Loss: 8.9034, Time: 0.076s\n",
      "  Batch 600/765, Loss: 8.6747, Time: 0.079s\n",
      "  Batch 610/765, Loss: 9.3476, Time: 0.088s\n",
      "  Batch 620/765, Loss: 5.8631, Time: 0.075s\n",
      "  Batch 630/765, Loss: 10.5298, Time: 0.073s\n",
      "  Batch 640/765, Loss: 11.6162, Time: 0.077s\n",
      "  Batch 650/765, Loss: 12.4762, Time: 0.075s\n",
      "  Batch 660/765, Loss: 10.0337, Time: 0.108s\n",
      "  Batch 670/765, Loss: 9.0445, Time: 0.077s\n",
      "  Batch 680/765, Loss: 9.0151, Time: 0.076s\n",
      "  Batch 690/765, Loss: 7.3001, Time: 0.074s\n",
      "  Batch 700/765, Loss: 9.2195, Time: 0.079s\n",
      "  Batch 710/765, Loss: 9.5859, Time: 0.079s\n",
      "  Batch 720/765, Loss: 9.9649, Time: 0.074s\n",
      "  Batch 730/765, Loss: 7.6412, Time: 0.077s\n",
      "  Batch 740/765, Loss: 10.3227, Time: 0.075s\n",
      "  Batch 750/765, Loss: 9.4088, Time: 0.076s\n",
      "  Batch 760/765, Loss: 8.9871, Time: 0.074s\n",
      "Epoch 1/5\n",
      "  Train Loss: 12.8178 (765 batches, avg batch time: 0.081s)\n",
      "  Val Loss: 9.0389 (validation time: 76.15s)\n",
      "  Epoch Time: 663.61s, Est. Remaining: 44.24 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.0389\n",
      "  Batch 10/765, Loss: 12.3524, Time: 0.074s\n",
      "  Batch 20/765, Loss: 10.3879, Time: 0.078s\n",
      "  Batch 30/765, Loss: 10.6670, Time: 0.074s\n",
      "  Batch 40/765, Loss: 8.5937, Time: 0.074s\n",
      "  Batch 50/765, Loss: 9.8633, Time: 0.099s\n",
      "  Batch 60/765, Loss: 8.6899, Time: 0.082s\n",
      "  Batch 70/765, Loss: 10.9560, Time: 0.102s\n",
      "  Batch 80/765, Loss: 11.4043, Time: 0.080s\n",
      "  Batch 90/765, Loss: 9.1736, Time: 0.077s\n",
      "  Batch 100/765, Loss: 11.0960, Time: 0.078s\n",
      "  Batch 110/765, Loss: 9.9295, Time: 0.096s\n",
      "  Batch 120/765, Loss: 9.4786, Time: 0.076s\n",
      "  Batch 130/765, Loss: 10.7124, Time: 0.076s\n",
      "  Batch 140/765, Loss: 7.4468, Time: 0.079s\n",
      "  Batch 150/765, Loss: 7.3832, Time: 0.074s\n",
      "  Batch 160/765, Loss: 10.3316, Time: 0.075s\n",
      "  Batch 170/765, Loss: 7.6732, Time: 0.088s\n",
      "  Batch 180/765, Loss: 8.7814, Time: 0.158s\n",
      "  Batch 190/765, Loss: 10.0753, Time: 0.074s\n",
      "  Batch 200/765, Loss: 9.7625, Time: 0.076s\n",
      "  Batch 210/765, Loss: 8.5284, Time: 0.099s\n",
      "  Batch 220/765, Loss: 10.1638, Time: 0.076s\n",
      "  Batch 230/765, Loss: 7.7936, Time: 0.074s\n",
      "  Batch 240/765, Loss: 7.9595, Time: 0.075s\n",
      "  Batch 250/765, Loss: 9.6101, Time: 0.074s\n",
      "  Batch 260/765, Loss: 7.0220, Time: 0.075s\n",
      "  Batch 270/765, Loss: 6.2685, Time: 0.075s\n",
      "  Batch 280/765, Loss: 12.3563, Time: 0.073s\n",
      "  Batch 290/765, Loss: 10.2167, Time: 0.074s\n",
      "  Batch 300/765, Loss: 10.4483, Time: 0.073s\n",
      "  Batch 310/765, Loss: 9.2702, Time: 0.227s\n",
      "  Batch 320/765, Loss: 7.3335, Time: 0.074s\n",
      "  Batch 330/765, Loss: 8.3452, Time: 0.077s\n",
      "  Batch 340/765, Loss: 8.7052, Time: 0.076s\n",
      "  Batch 350/765, Loss: 8.9085, Time: 0.048s\n",
      "  Batch 360/765, Loss: 8.0024, Time: 0.073s\n",
      "  Batch 370/765, Loss: 10.3824, Time: 0.072s\n",
      "  Batch 380/765, Loss: 9.1313, Time: 0.074s\n",
      "  Batch 390/765, Loss: 9.3137, Time: 0.072s\n",
      "  Batch 400/765, Loss: 7.5380, Time: 0.075s\n",
      "  Batch 410/765, Loss: 10.0881, Time: 0.074s\n",
      "  Batch 420/765, Loss: 9.1418, Time: 0.074s\n",
      "  Batch 430/765, Loss: 8.7617, Time: 0.100s\n",
      "  Batch 440/765, Loss: 11.3858, Time: 0.076s\n",
      "  Batch 450/765, Loss: 9.6202, Time: 0.076s\n",
      "  Batch 460/765, Loss: 7.2796, Time: 0.097s\n",
      "  Batch 470/765, Loss: 6.7426, Time: 0.073s\n",
      "  Batch 480/765, Loss: 9.8486, Time: 0.119s\n",
      "  Batch 490/765, Loss: 8.8925, Time: 0.088s\n",
      "  Batch 500/765, Loss: 10.0091, Time: 0.074s\n",
      "  Batch 510/765, Loss: 8.2231, Time: 0.075s\n",
      "  Batch 520/765, Loss: 9.4296, Time: 0.051s\n",
      "  Batch 530/765, Loss: 10.6052, Time: 0.054s\n",
      "  Batch 540/765, Loss: 7.6678, Time: 0.050s\n",
      "  Batch 550/765, Loss: 9.3723, Time: 0.076s\n",
      "  Batch 560/765, Loss: 9.6455, Time: 0.074s\n",
      "  Batch 570/765, Loss: 9.3540, Time: 0.052s\n",
      "  Batch 580/765, Loss: 8.9265, Time: 0.074s\n",
      "  Batch 590/765, Loss: 7.8084, Time: 0.075s\n",
      "  Batch 600/765, Loss: 13.7026, Time: 0.075s\n",
      "  Batch 610/765, Loss: 10.0274, Time: 0.126s\n",
      "  Batch 620/765, Loss: 8.4171, Time: 0.076s\n",
      "  Batch 630/765, Loss: 10.5696, Time: 0.073s\n",
      "  Batch 640/765, Loss: 10.5127, Time: 0.074s\n",
      "  Batch 650/765, Loss: 8.6368, Time: 0.075s\n",
      "  Batch 660/765, Loss: 9.8473, Time: 0.077s\n",
      "  Batch 670/765, Loss: 10.5781, Time: 0.076s\n",
      "  Batch 680/765, Loss: 6.6102, Time: 0.074s\n",
      "  Batch 690/765, Loss: 8.3893, Time: 0.076s\n",
      "  Batch 700/765, Loss: 13.3574, Time: 0.077s\n",
      "  Batch 710/765, Loss: 10.1652, Time: 0.049s\n",
      "  Batch 720/765, Loss: 9.7574, Time: 0.076s\n",
      "  Batch 730/765, Loss: 6.9937, Time: 0.073s\n",
      "  Batch 740/765, Loss: 12.4578, Time: 0.104s\n",
      "  Batch 750/765, Loss: 6.3066, Time: 0.074s\n",
      "  Batch 760/765, Loss: 10.5608, Time: 0.076s\n",
      "Epoch 2/5\n",
      "  Train Loss: 9.2666 (765 batches, avg batch time: 0.079s)\n",
      "  Val Loss: 9.0053 (validation time: 75.97s)\n",
      "  Epoch Time: 651.77s, Est. Remaining: 32.88 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.0053\n",
      "  Batch 10/765, Loss: 9.0920, Time: 0.075s\n",
      "  Batch 20/765, Loss: 10.2140, Time: 0.074s\n",
      "  Batch 30/765, Loss: 8.1233, Time: 0.076s\n",
      "  Batch 40/765, Loss: 9.2757, Time: 0.078s\n",
      "  Batch 50/765, Loss: 9.0282, Time: 0.075s\n",
      "  Batch 60/765, Loss: 11.3313, Time: 0.072s\n",
      "  Batch 70/765, Loss: 5.9685, Time: 0.049s\n",
      "  Batch 80/765, Loss: 11.0436, Time: 0.054s\n",
      "  Batch 90/765, Loss: 9.6116, Time: 0.076s\n",
      "  Batch 100/765, Loss: 8.6772, Time: 0.073s\n",
      "  Batch 110/765, Loss: 9.7554, Time: 0.074s\n",
      "  Batch 120/765, Loss: 10.1437, Time: 0.075s\n",
      "  Batch 130/765, Loss: 11.4951, Time: 0.211s\n",
      "  Batch 140/765, Loss: 9.3810, Time: 0.076s\n",
      "  Batch 150/765, Loss: 9.4553, Time: 0.077s\n",
      "  Batch 160/765, Loss: 8.6359, Time: 0.049s\n",
      "  Batch 170/765, Loss: 11.4463, Time: 0.078s\n",
      "  Batch 180/765, Loss: 9.1161, Time: 0.075s\n",
      "  Batch 190/765, Loss: 10.2753, Time: 0.080s\n",
      "  Batch 200/765, Loss: 8.8772, Time: 0.075s\n",
      "  Batch 210/765, Loss: 9.6692, Time: 0.076s\n",
      "  Batch 220/765, Loss: 9.4622, Time: 0.075s\n",
      "  Batch 230/765, Loss: 9.4284, Time: 0.079s\n",
      "  Batch 240/765, Loss: 10.3889, Time: 0.114s\n",
      "  Batch 250/765, Loss: 10.0956, Time: 0.076s\n",
      "  Batch 260/765, Loss: 8.6142, Time: 0.096s\n",
      "  Batch 270/765, Loss: 8.3608, Time: 0.074s\n",
      "  Batch 280/765, Loss: 7.1612, Time: 0.112s\n",
      "  Batch 290/765, Loss: 11.3877, Time: 0.076s\n",
      "  Batch 300/765, Loss: 9.2210, Time: 0.074s\n",
      "  Batch 310/765, Loss: 7.6094, Time: 0.073s\n",
      "  Batch 320/765, Loss: 9.0894, Time: 0.086s\n",
      "  Batch 330/765, Loss: 8.8486, Time: 0.073s\n",
      "  Batch 340/765, Loss: 9.4630, Time: 0.076s\n",
      "  Batch 350/765, Loss: 9.3944, Time: 0.074s\n",
      "  Batch 360/765, Loss: 9.0181, Time: 0.073s\n",
      "  Batch 370/765, Loss: 9.2265, Time: 0.076s\n",
      "  Batch 380/765, Loss: 7.2792, Time: 0.074s\n",
      "  Batch 390/765, Loss: 8.6615, Time: 0.075s\n",
      "  Batch 400/765, Loss: 8.4337, Time: 0.080s\n",
      "  Batch 410/765, Loss: 10.9024, Time: 0.076s\n",
      "  Batch 420/765, Loss: 10.0179, Time: 0.075s\n",
      "  Batch 430/765, Loss: 11.0487, Time: 0.143s\n",
      "  Batch 440/765, Loss: 7.5878, Time: 0.049s\n",
      "  Batch 450/765, Loss: 8.3752, Time: 0.075s\n",
      "  Batch 460/765, Loss: 8.1096, Time: 0.074s\n",
      "  Batch 470/765, Loss: 6.4059, Time: 0.078s\n",
      "  Batch 480/765, Loss: 11.0115, Time: 0.076s\n",
      "  Batch 490/765, Loss: 10.1953, Time: 0.049s\n",
      "  Batch 500/765, Loss: 8.0141, Time: 0.073s\n",
      "  Batch 510/765, Loss: 6.8099, Time: 0.074s\n",
      "  Batch 520/765, Loss: 7.7654, Time: 0.078s\n",
      "  Batch 530/765, Loss: 10.0271, Time: 0.076s\n",
      "  Batch 540/765, Loss: 10.2001, Time: 0.077s\n",
      "  Batch 550/765, Loss: 9.4380, Time: 0.075s\n",
      "  Batch 560/765, Loss: 7.9577, Time: 0.148s\n",
      "  Batch 570/765, Loss: 8.2034, Time: 0.074s\n",
      "  Batch 580/765, Loss: 10.8928, Time: 0.078s\n",
      "  Batch 590/765, Loss: 7.9875, Time: 0.077s\n",
      "  Batch 600/765, Loss: 11.6144, Time: 0.076s\n",
      "  Batch 610/765, Loss: 9.8450, Time: 0.083s\n",
      "  Batch 620/765, Loss: 8.6670, Time: 0.078s\n",
      "  Batch 630/765, Loss: 6.4478, Time: 0.076s\n",
      "  Batch 640/765, Loss: 9.4335, Time: 0.077s\n",
      "  Batch 650/765, Loss: 11.1480, Time: 0.052s\n",
      "  Batch 660/765, Loss: 11.1528, Time: 0.075s\n",
      "  Batch 670/765, Loss: 9.1298, Time: 0.083s\n",
      "  Batch 680/765, Loss: 7.5504, Time: 0.077s\n",
      "  Batch 690/765, Loss: 11.3972, Time: 0.075s\n",
      "  Batch 700/765, Loss: 7.5403, Time: 0.074s\n",
      "  Batch 710/765, Loss: 12.1493, Time: 0.084s\n",
      "  Batch 720/765, Loss: 9.6980, Time: 0.075s\n",
      "  Batch 730/765, Loss: 5.5391, Time: 0.178s\n",
      "  Batch 740/765, Loss: 8.9225, Time: 0.075s\n",
      "  Batch 750/765, Loss: 8.8669, Time: 0.075s\n",
      "  Batch 760/765, Loss: 8.4506, Time: 0.051s\n",
      "Epoch 3/5\n",
      "  Train Loss: 9.1328 (765 batches, avg batch time: 0.080s)\n",
      "  Val Loss: 8.8502 (validation time: 76.06s)\n",
      "  Epoch Time: 654.46s, Est. Remaining: 21.89 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.8502\n",
      "  Batch 10/765, Loss: 9.9718, Time: 0.074s\n",
      "  Batch 20/765, Loss: 9.2262, Time: 0.074s\n",
      "  Batch 30/765, Loss: 10.3007, Time: 0.078s\n",
      "  Batch 40/765, Loss: 7.3678, Time: 0.076s\n",
      "  Batch 50/765, Loss: 8.4314, Time: 0.079s\n",
      "  Batch 60/765, Loss: 8.8301, Time: 0.049s\n",
      "  Batch 70/765, Loss: 11.6185, Time: 0.074s\n",
      "  Batch 80/765, Loss: 8.9708, Time: 0.075s\n",
      "  Batch 90/765, Loss: 11.0978, Time: 0.074s\n",
      "  Batch 100/765, Loss: 9.4143, Time: 0.083s\n",
      "  Batch 110/765, Loss: 9.3115, Time: 0.078s\n",
      "  Batch 120/765, Loss: 8.2292, Time: 0.207s\n",
      "  Batch 130/765, Loss: 10.5146, Time: 0.075s\n",
      "  Batch 140/765, Loss: 9.9436, Time: 0.077s\n",
      "  Batch 150/765, Loss: 7.3644, Time: 0.076s\n",
      "  Batch 160/765, Loss: 7.2540, Time: 0.076s\n",
      "  Batch 170/765, Loss: 10.4140, Time: 0.079s\n",
      "  Batch 180/765, Loss: 6.1258, Time: 0.116s\n",
      "  Batch 190/765, Loss: 8.8864, Time: 0.073s\n",
      "  Batch 200/765, Loss: 9.8066, Time: 0.078s\n",
      "  Batch 210/765, Loss: 8.0989, Time: 0.076s\n",
      "  Batch 220/765, Loss: 7.9249, Time: 0.073s\n",
      "  Batch 230/765, Loss: 8.6467, Time: 0.073s\n",
      "  Batch 240/765, Loss: 9.3600, Time: 0.077s\n",
      "  Batch 250/765, Loss: 9.1577, Time: 0.075s\n",
      "  Batch 260/765, Loss: 8.1913, Time: 0.078s\n",
      "  Batch 270/765, Loss: 10.4520, Time: 0.050s\n",
      "  Batch 280/765, Loss: 8.1243, Time: 0.048s\n",
      "  Batch 290/765, Loss: 7.7551, Time: 0.076s\n",
      "  Batch 300/765, Loss: 9.0440, Time: 0.106s\n",
      "  Batch 310/765, Loss: 8.0239, Time: 0.076s\n",
      "  Batch 320/765, Loss: 9.6571, Time: 0.076s\n",
      "  Batch 330/765, Loss: 9.7928, Time: 0.075s\n",
      "  Batch 340/765, Loss: 11.0603, Time: 0.074s\n",
      "  Batch 350/765, Loss: 8.8220, Time: 0.075s\n",
      "  Batch 360/765, Loss: 9.6078, Time: 0.074s\n",
      "  Batch 370/765, Loss: 6.3589, Time: 0.076s\n",
      "  Batch 380/765, Loss: 9.7610, Time: 0.051s\n",
      "  Batch 390/765, Loss: 13.1264, Time: 0.079s\n",
      "  Batch 400/765, Loss: 10.2573, Time: 0.076s\n",
      "  Batch 410/765, Loss: 13.3103, Time: 0.074s\n",
      "  Batch 420/765, Loss: 11.4708, Time: 0.091s\n",
      "  Batch 430/765, Loss: 10.6098, Time: 0.077s\n",
      "  Batch 440/765, Loss: 7.3198, Time: 0.073s\n",
      "  Batch 450/765, Loss: 7.7991, Time: 0.077s\n",
      "  Batch 460/765, Loss: 7.8794, Time: 0.049s\n",
      "  Batch 470/765, Loss: 8.6162, Time: 0.076s\n",
      "  Batch 480/765, Loss: 9.9938, Time: 0.080s\n",
      "  Batch 490/765, Loss: 8.9689, Time: 0.075s\n",
      "  Batch 500/765, Loss: 9.8021, Time: 0.077s\n",
      "  Batch 510/765, Loss: 9.6808, Time: 0.079s\n",
      "  Batch 520/765, Loss: 10.4804, Time: 0.078s\n",
      "  Batch 530/765, Loss: 8.8929, Time: 0.076s\n",
      "  Batch 540/765, Loss: 8.9971, Time: 0.077s\n",
      "  Batch 550/765, Loss: 8.7990, Time: 0.080s\n",
      "  Batch 560/765, Loss: 8.1391, Time: 0.075s\n",
      "  Batch 570/765, Loss: 9.1378, Time: 0.075s\n",
      "  Batch 580/765, Loss: 7.6254, Time: 0.051s\n",
      "  Batch 590/765, Loss: 9.8801, Time: 0.092s\n",
      "  Batch 600/765, Loss: 10.2524, Time: 0.079s\n",
      "  Batch 610/765, Loss: 8.3931, Time: 0.074s\n",
      "  Batch 620/765, Loss: 6.8872, Time: 0.074s\n",
      "  Batch 630/765, Loss: 7.5930, Time: 0.119s\n",
      "  Batch 640/765, Loss: 8.0490, Time: 0.076s\n",
      "  Batch 650/765, Loss: 7.4894, Time: 0.074s\n",
      "  Batch 660/765, Loss: 8.3134, Time: 0.076s\n",
      "  Batch 670/765, Loss: 7.6330, Time: 0.074s\n",
      "  Batch 680/765, Loss: 8.2034, Time: 0.075s\n",
      "  Batch 690/765, Loss: 9.2573, Time: 0.074s\n",
      "  Batch 700/765, Loss: 6.4354, Time: 0.074s\n",
      "  Batch 710/765, Loss: 10.9258, Time: 0.086s\n",
      "  Batch 720/765, Loss: 8.5213, Time: 0.076s\n",
      "  Batch 730/765, Loss: 8.7701, Time: 0.099s\n",
      "  Batch 740/765, Loss: 10.2239, Time: 0.077s\n",
      "  Batch 750/765, Loss: 7.1785, Time: 0.080s\n",
      "  Batch 760/765, Loss: 6.7540, Time: 0.083s\n",
      "Epoch 4/5\n",
      "  Train Loss: 9.1648 (765 batches, avg batch time: 0.080s)\n",
      "  Val Loss: 8.8777 (validation time: 77.98s)\n",
      "  Epoch Time: 662.55s, Est. Remaining: 10.97 minutes\n",
      "  No improvement for 1/2 epochs\n",
      "  Batch 10/765, Loss: 8.5521, Time: 0.085s\n",
      "  Batch 20/765, Loss: 9.0291, Time: 0.089s\n",
      "  Batch 30/765, Loss: 8.2833, Time: 0.066s\n",
      "  Batch 40/765, Loss: 9.8088, Time: 0.075s\n",
      "  Batch 50/765, Loss: 7.9477, Time: 0.076s\n",
      "  Batch 60/765, Loss: 11.2969, Time: 0.075s\n",
      "  Batch 70/765, Loss: 10.3669, Time: 0.074s\n",
      "  Batch 80/765, Loss: 7.0863, Time: 0.075s\n",
      "  Batch 90/765, Loss: 9.2913, Time: 0.074s\n",
      "  Batch 100/765, Loss: 7.9549, Time: 0.075s\n",
      "  Batch 110/765, Loss: 9.3773, Time: 0.075s\n",
      "  Batch 120/765, Loss: 7.7609, Time: 0.075s\n",
      "  Batch 130/765, Loss: 10.6744, Time: 0.078s\n",
      "  Batch 140/765, Loss: 10.2164, Time: 0.090s\n",
      "  Batch 150/765, Loss: 8.3169, Time: 0.075s\n",
      "  Batch 160/765, Loss: 9.0309, Time: 0.074s\n",
      "  Batch 170/765, Loss: 9.0635, Time: 0.075s\n",
      "  Batch 180/765, Loss: 7.5668, Time: 0.075s\n",
      "  Batch 190/765, Loss: 9.3139, Time: 0.052s\n",
      "  Batch 200/765, Loss: 6.9490, Time: 0.077s\n",
      "  Batch 210/765, Loss: 8.7856, Time: 0.074s\n",
      "  Batch 220/765, Loss: 10.4496, Time: 0.083s\n",
      "  Batch 230/765, Loss: 9.3887, Time: 0.076s\n",
      "  Batch 240/765, Loss: 8.8122, Time: 0.076s\n",
      "  Batch 250/765, Loss: 8.2806, Time: 0.074s\n",
      "  Batch 260/765, Loss: 8.6923, Time: 0.075s\n",
      "  Batch 270/765, Loss: 8.0286, Time: 0.074s\n",
      "  Batch 280/765, Loss: 8.3214, Time: 0.079s\n",
      "  Batch 290/765, Loss: 8.9448, Time: 0.074s\n",
      "  Batch 300/765, Loss: 8.4798, Time: 0.049s\n",
      "  Batch 310/765, Loss: 12.3334, Time: 0.074s\n",
      "  Batch 320/765, Loss: 11.7276, Time: 0.074s\n",
      "  Batch 330/765, Loss: 8.9182, Time: 0.075s\n",
      "  Batch 340/765, Loss: 10.2920, Time: 0.075s\n",
      "  Batch 350/765, Loss: 12.5119, Time: 0.075s\n",
      "  Batch 360/765, Loss: 9.4120, Time: 0.074s\n",
      "  Batch 370/765, Loss: 10.2653, Time: 0.085s\n",
      "  Batch 380/765, Loss: 9.1302, Time: 0.074s\n",
      "  Batch 390/765, Loss: 10.3924, Time: 0.075s\n",
      "  Batch 400/765, Loss: 7.4458, Time: 0.074s\n",
      "  Batch 410/765, Loss: 8.4245, Time: 0.073s\n",
      "  Batch 420/765, Loss: 9.7507, Time: 0.075s\n",
      "  Batch 430/765, Loss: 11.2696, Time: 0.076s\n",
      "  Batch 440/765, Loss: 7.0593, Time: 0.201s\n",
      "  Batch 450/765, Loss: 9.2542, Time: 0.075s\n",
      "  Batch 460/765, Loss: 9.3179, Time: 0.077s\n",
      "  Batch 470/765, Loss: 9.1306, Time: 0.074s\n",
      "  Batch 480/765, Loss: 8.4834, Time: 0.074s\n",
      "  Batch 490/765, Loss: 7.5505, Time: 0.083s\n",
      "  Batch 500/765, Loss: 9.5703, Time: 0.076s\n",
      "  Batch 510/765, Loss: 8.1920, Time: 0.074s\n",
      "  Batch 520/765, Loss: 10.3081, Time: 0.050s\n",
      "  Batch 530/765, Loss: 7.7799, Time: 0.074s\n",
      "  Batch 540/765, Loss: 8.0439, Time: 0.076s\n",
      "  Batch 550/765, Loss: 9.2680, Time: 0.075s\n",
      "  Batch 560/765, Loss: 10.1416, Time: 0.075s\n",
      "  Batch 570/765, Loss: 10.3473, Time: 0.074s\n",
      "  Batch 580/765, Loss: 8.3638, Time: 0.076s\n",
      "  Batch 590/765, Loss: 10.9950, Time: 0.075s\n",
      "  Batch 600/765, Loss: 9.4354, Time: 0.073s\n",
      "  Batch 610/765, Loss: 10.0765, Time: 0.075s\n",
      "  Batch 620/765, Loss: 8.7745, Time: 0.077s\n",
      "  Batch 630/765, Loss: 8.4498, Time: 0.075s\n",
      "  Batch 640/765, Loss: 8.8330, Time: 0.074s\n",
      "  Batch 650/765, Loss: 8.8176, Time: 0.076s\n",
      "  Batch 660/765, Loss: 9.9015, Time: 0.076s\n",
      "  Batch 670/765, Loss: 7.2821, Time: 0.076s\n",
      "  Batch 680/765, Loss: 11.4710, Time: 0.074s\n",
      "  Batch 690/765, Loss: 9.1444, Time: 0.075s\n",
      "  Batch 700/765, Loss: 7.8851, Time: 0.080s\n",
      "  Batch 710/765, Loss: 6.1676, Time: 0.075s\n",
      "  Batch 720/765, Loss: 9.4555, Time: 0.077s\n",
      "  Batch 730/765, Loss: 8.8088, Time: 0.074s\n",
      "  Batch 740/765, Loss: 8.9874, Time: 0.074s\n",
      "  Batch 750/765, Loss: 7.1527, Time: 0.050s\n",
      "  Batch 760/765, Loss: 7.4814, Time: 0.075s\n",
      "Epoch 5/5\n",
      "  Train Loss: 9.0506 (765 batches, avg batch time: 0.078s)\n",
      "  Val Loss: 8.9056 (validation time: 75.59s)\n",
      "  Epoch Time: 654.16s, Est. Remaining: 0.00 minutes\n",
      "  No improvement for 2/2 epochs\n",
      "Early stopping at epoch 5\n",
      "Training completed in 54.78 minutes (3286.7 seconds)\n",
      "Loaded best model (version single_output_v1)\n",
      "Params: [2.29677418e+01 6.35056601e+01 2.04272411e-02 2.79373497e+01\n",
      " 2.05715977e+00], Validation MAPE: 13.5584%\n",
      "Model architecture has changed - detected parameter size mismatch.\n",
      "Removing incompatible checkpoint and starting fresh training.\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 100.5721, Time: 0.094s\n",
      "  Batch 20/765, Loss: 34.6541, Time: 0.079s\n",
      "  Batch 30/765, Loss: 32.6211, Time: 0.068s\n",
      "  Batch 40/765, Loss: 21.7688, Time: 0.072s\n",
      "  Batch 50/765, Loss: 15.0459, Time: 0.080s\n",
      "  Batch 60/765, Loss: 26.8934, Time: 0.072s\n",
      "  Batch 70/765, Loss: 29.9735, Time: 0.078s\n",
      "  Batch 80/765, Loss: 24.5138, Time: 0.071s\n",
      "  Batch 90/765, Loss: 26.4800, Time: 0.075s\n",
      "  Batch 100/765, Loss: 21.5311, Time: 0.089s\n",
      "  Batch 110/765, Loss: 26.1330, Time: 0.074s\n",
      "  Batch 120/765, Loss: 22.1820, Time: 0.070s\n",
      "  Batch 130/765, Loss: 20.0499, Time: 0.045s\n",
      "  Batch 140/765, Loss: 17.5519, Time: 0.071s\n",
      "  Batch 150/765, Loss: 10.6652, Time: 0.072s\n",
      "  Batch 160/765, Loss: 12.4776, Time: 0.071s\n",
      "  Batch 170/765, Loss: 10.4776, Time: 0.070s\n",
      "  Batch 180/765, Loss: 10.9551, Time: 0.082s\n",
      "  Batch 190/765, Loss: 11.1330, Time: 0.074s\n",
      "  Batch 200/765, Loss: 15.2021, Time: 0.102s\n",
      "  Batch 210/765, Loss: 10.2178, Time: 0.078s\n",
      "  Batch 220/765, Loss: 13.6555, Time: 0.072s\n",
      "  Batch 230/765, Loss: 12.6000, Time: 0.049s\n",
      "  Batch 240/765, Loss: 11.5569, Time: 0.071s\n",
      "  Batch 250/765, Loss: 10.8484, Time: 0.148s\n",
      "  Batch 260/765, Loss: 11.9629, Time: 0.072s\n",
      "  Batch 270/765, Loss: 8.1666, Time: 0.073s\n",
      "  Batch 280/765, Loss: 8.8328, Time: 0.071s\n",
      "  Batch 290/765, Loss: 10.6021, Time: 0.071s\n",
      "  Batch 300/765, Loss: 9.4220, Time: 0.072s\n",
      "  Batch 310/765, Loss: 9.2713, Time: 0.047s\n",
      "  Batch 320/765, Loss: 7.6086, Time: 0.072s\n",
      "  Batch 330/765, Loss: 9.7735, Time: 0.071s\n",
      "  Batch 340/765, Loss: 11.0354, Time: 0.069s\n",
      "  Batch 350/765, Loss: 10.2986, Time: 0.065s\n",
      "  Batch 360/765, Loss: 8.5028, Time: 0.071s\n",
      "  Batch 370/765, Loss: 12.7941, Time: 0.075s\n",
      "  Batch 380/765, Loss: 11.5982, Time: 0.072s\n",
      "  Batch 390/765, Loss: 11.3686, Time: 0.070s\n",
      "  Batch 400/765, Loss: 7.9941, Time: 0.073s\n",
      "  Batch 410/765, Loss: 9.2013, Time: 0.072s\n",
      "  Batch 420/765, Loss: 11.9144, Time: 0.073s\n",
      "  Batch 430/765, Loss: 10.2637, Time: 0.071s\n",
      "  Batch 440/765, Loss: 10.2546, Time: 0.072s\n",
      "  Batch 450/765, Loss: 9.3648, Time: 0.071s\n",
      "  Batch 460/765, Loss: 12.4738, Time: 0.072s\n",
      "  Batch 470/765, Loss: 12.4525, Time: 0.097s\n",
      "  Batch 480/765, Loss: 12.0429, Time: 0.070s\n",
      "  Batch 490/765, Loss: 8.5578, Time: 0.071s\n",
      "  Batch 500/765, Loss: 8.9602, Time: 0.074s\n",
      "  Batch 510/765, Loss: 10.1592, Time: 0.076s\n",
      "  Batch 520/765, Loss: 9.3718, Time: 0.235s\n",
      "  Batch 530/765, Loss: 12.6044, Time: 0.047s\n",
      "  Batch 540/765, Loss: 11.0792, Time: 0.071s\n",
      "  Batch 550/765, Loss: 8.1622, Time: 0.071s\n",
      "  Batch 560/765, Loss: 9.6057, Time: 0.069s\n",
      "  Batch 570/765, Loss: 7.7961, Time: 0.069s\n",
      "  Batch 580/765, Loss: 10.2883, Time: 0.045s\n",
      "  Batch 590/765, Loss: 9.2413, Time: 0.071s\n",
      "  Batch 600/765, Loss: 9.7798, Time: 0.070s\n",
      "  Batch 610/765, Loss: 7.4616, Time: 0.072s\n",
      "  Batch 620/765, Loss: 7.2525, Time: 0.069s\n",
      "  Batch 630/765, Loss: 6.4222, Time: 0.071s\n",
      "  Batch 640/765, Loss: 8.1128, Time: 0.069s\n",
      "  Batch 650/765, Loss: 9.2899, Time: 0.103s\n",
      "  Batch 660/765, Loss: 8.7319, Time: 0.071s\n",
      "  Batch 670/765, Loss: 10.5969, Time: 0.071s\n",
      "  Batch 680/765, Loss: 9.2922, Time: 0.070s\n",
      "  Batch 690/765, Loss: 9.7540, Time: 0.048s\n",
      "  Batch 700/765, Loss: 8.8823, Time: 0.072s\n",
      "  Batch 710/765, Loss: 12.1866, Time: 0.069s\n",
      "  Batch 720/765, Loss: 8.2750, Time: 0.069s\n",
      "  Batch 730/765, Loss: 8.4429, Time: 0.069s\n",
      "  Batch 740/765, Loss: 11.0929, Time: 0.071s\n",
      "  Batch 750/765, Loss: 10.7221, Time: 0.050s\n",
      "  Batch 760/765, Loss: 9.8206, Time: 0.071s\n",
      "Epoch 1/5\n",
      "  Train Loss: 15.0046 (765 batches, avg batch time: 0.074s)\n",
      "  Val Loss: 9.0210 (validation time: 75.37s)\n",
      "  Epoch Time: 665.85s, Est. Remaining: 44.39 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.0210\n",
      "  Batch 10/765, Loss: 9.4016, Time: 0.072s\n",
      "  Batch 20/765, Loss: 12.4118, Time: 0.045s\n",
      "  Batch 30/765, Loss: 8.5826, Time: 0.069s\n",
      "  Batch 40/765, Loss: 7.3645, Time: 0.069s\n",
      "  Batch 50/765, Loss: 8.9906, Time: 0.070s\n",
      "  Batch 60/765, Loss: 8.9096, Time: 0.070s\n",
      "  Batch 70/765, Loss: 9.8397, Time: 0.069s\n",
      "  Batch 80/765, Loss: 9.4423, Time: 0.071s\n",
      "  Batch 90/765, Loss: 10.6277, Time: 0.045s\n",
      "  Batch 100/765, Loss: 9.5722, Time: 0.072s\n",
      "  Batch 110/765, Loss: 10.4376, Time: 0.069s\n",
      "  Batch 120/765, Loss: 8.2886, Time: 0.071s\n",
      "  Batch 130/765, Loss: 8.4165, Time: 0.085s\n",
      "  Batch 140/765, Loss: 8.7464, Time: 0.069s\n",
      "  Batch 150/765, Loss: 9.4870, Time: 0.070s\n",
      "  Batch 160/765, Loss: 9.8824, Time: 0.071s\n",
      "  Batch 170/765, Loss: 7.6594, Time: 0.071s\n",
      "  Batch 180/765, Loss: 8.1729, Time: 0.080s\n",
      "  Batch 190/765, Loss: 8.6313, Time: 0.077s\n",
      "  Batch 200/765, Loss: 8.8754, Time: 0.072s\n",
      "  Batch 210/765, Loss: 9.4001, Time: 0.074s\n",
      "  Batch 220/765, Loss: 9.6313, Time: 0.091s\n",
      "  Batch 230/765, Loss: 8.6791, Time: 0.070s\n",
      "  Batch 240/765, Loss: 9.4108, Time: 0.070s\n",
      "  Batch 250/765, Loss: 9.1519, Time: 0.071s\n",
      "  Batch 260/765, Loss: 7.5312, Time: 0.080s\n",
      "  Batch 270/765, Loss: 8.4183, Time: 0.070s\n",
      "  Batch 280/765, Loss: 8.0670, Time: 0.071s\n",
      "  Batch 290/765, Loss: 8.8283, Time: 0.070s\n",
      "  Batch 300/765, Loss: 9.6744, Time: 0.074s\n",
      "  Batch 310/765, Loss: 11.5785, Time: 0.086s\n",
      "  Batch 320/765, Loss: 8.8153, Time: 0.070s\n",
      "  Batch 330/765, Loss: 8.7979, Time: 0.072s\n",
      "  Batch 340/765, Loss: 9.7363, Time: 0.075s\n",
      "  Batch 350/765, Loss: 12.9320, Time: 0.070s\n",
      "  Batch 360/765, Loss: 11.6953, Time: 0.070s\n",
      "  Batch 370/765, Loss: 8.4078, Time: 0.069s\n",
      "  Batch 380/765, Loss: 8.8808, Time: 0.070s\n",
      "  Batch 390/765, Loss: 10.7803, Time: 0.071s\n",
      "  Batch 400/765, Loss: 10.7721, Time: 0.071s\n",
      "  Batch 410/765, Loss: 10.3127, Time: 0.070s\n",
      "  Batch 420/765, Loss: 7.2313, Time: 0.070s\n",
      "  Batch 430/765, Loss: 11.8235, Time: 0.072s\n",
      "  Batch 440/765, Loss: 10.5455, Time: 0.095s\n",
      "  Batch 450/765, Loss: 9.1834, Time: 0.070s\n",
      "  Batch 460/765, Loss: 6.8297, Time: 0.071s\n",
      "  Batch 470/765, Loss: 10.4161, Time: 0.078s\n",
      "  Batch 480/765, Loss: 7.6050, Time: 0.074s\n",
      "  Batch 490/765, Loss: 9.7806, Time: 0.071s\n",
      "  Batch 500/765, Loss: 6.5401, Time: 0.072s\n",
      "  Batch 510/765, Loss: 8.3742, Time: 0.070s\n",
      "  Batch 520/765, Loss: 9.6879, Time: 0.076s\n",
      "  Batch 530/765, Loss: 9.5587, Time: 0.073s\n",
      "  Batch 540/765, Loss: 9.5603, Time: 0.072s\n",
      "  Batch 550/765, Loss: 9.1580, Time: 0.070s\n",
      "  Batch 560/765, Loss: 9.4925, Time: 0.069s\n",
      "  Batch 570/765, Loss: 9.8503, Time: 0.069s\n",
      "  Batch 580/765, Loss: 9.9259, Time: 0.077s\n",
      "  Batch 590/765, Loss: 8.2517, Time: 0.070s\n",
      "  Batch 600/765, Loss: 9.7610, Time: 0.071s\n",
      "  Batch 610/765, Loss: 8.5891, Time: 0.075s\n",
      "  Batch 620/765, Loss: 10.7263, Time: 0.071s\n",
      "  Batch 630/765, Loss: 9.1325, Time: 0.070s\n",
      "  Batch 640/765, Loss: 7.4625, Time: 0.072s\n",
      "  Batch 650/765, Loss: 9.5241, Time: 0.070s\n",
      "  Batch 660/765, Loss: 8.5624, Time: 0.070s\n",
      "  Batch 670/765, Loss: 9.0321, Time: 0.071s\n",
      "  Batch 680/765, Loss: 6.9542, Time: 0.082s\n",
      "  Batch 690/765, Loss: 8.8222, Time: 0.069s\n",
      "  Batch 700/765, Loss: 8.9713, Time: 0.070s\n",
      "  Batch 710/765, Loss: 10.4947, Time: 0.096s\n",
      "  Batch 720/765, Loss: 10.2704, Time: 0.076s\n",
      "  Batch 730/765, Loss: 8.6052, Time: 0.092s\n",
      "  Batch 740/765, Loss: 9.6996, Time: 0.049s\n",
      "  Batch 750/765, Loss: 9.1463, Time: 0.069s\n",
      "  Batch 760/765, Loss: 6.4840, Time: 0.075s\n",
      "Epoch 2/5\n",
      "  Train Loss: 9.1614 (765 batches, avg batch time: 0.073s)\n",
      "  Val Loss: 9.0974 (validation time: 75.43s)\n",
      "  Epoch Time: 653.11s, Est. Remaining: 32.97 minutes\n",
      "  No improvement for 1/2 epochs\n",
      "  Batch 10/765, Loss: 6.9560, Time: 0.084s\n",
      "  Batch 20/765, Loss: 8.1346, Time: 0.087s\n",
      "  Batch 30/765, Loss: 8.4381, Time: 0.091s\n",
      "  Batch 40/765, Loss: 8.7613, Time: 0.070s\n",
      "  Batch 50/765, Loss: 11.7260, Time: 0.069s\n",
      "  Batch 60/765, Loss: 10.2197, Time: 0.071s\n",
      "  Batch 70/765, Loss: 10.5285, Time: 0.070s\n",
      "  Batch 80/765, Loss: 9.2787, Time: 0.069s\n",
      "  Batch 90/765, Loss: 9.3702, Time: 0.073s\n",
      "  Batch 100/765, Loss: 7.2272, Time: 0.070s\n",
      "  Batch 110/765, Loss: 8.7319, Time: 0.070s\n",
      "  Batch 120/765, Loss: 5.4880, Time: 0.073s\n",
      "  Batch 130/765, Loss: 10.5120, Time: 0.070s\n",
      "  Batch 140/765, Loss: 9.7754, Time: 0.070s\n",
      "  Batch 150/765, Loss: 8.9621, Time: 0.072s\n",
      "  Batch 160/765, Loss: 10.1390, Time: 0.074s\n",
      "  Batch 170/765, Loss: 10.1435, Time: 0.073s\n",
      "  Batch 180/765, Loss: 10.1000, Time: 0.093s\n",
      "  Batch 190/765, Loss: 6.9927, Time: 0.074s\n",
      "  Batch 200/765, Loss: 8.9676, Time: 0.080s\n",
      "  Batch 210/765, Loss: 9.7428, Time: 0.092s\n",
      "  Batch 220/765, Loss: 9.3460, Time: 0.068s\n",
      "  Batch 230/765, Loss: 12.3899, Time: 0.077s\n",
      "  Batch 240/765, Loss: 9.3954, Time: 0.047s\n",
      "  Batch 250/765, Loss: 7.4800, Time: 0.071s\n",
      "  Batch 260/765, Loss: 11.2637, Time: 0.069s\n",
      "  Batch 270/765, Loss: 7.5959, Time: 0.071s\n",
      "  Batch 280/765, Loss: 7.6837, Time: 0.080s\n",
      "  Batch 290/765, Loss: 9.4190, Time: 0.070s\n",
      "  Batch 300/765, Loss: 7.9646, Time: 0.077s\n",
      "  Batch 310/765, Loss: 10.0129, Time: 0.071s\n",
      "  Batch 320/765, Loss: 11.2685, Time: 0.069s\n",
      "  Batch 330/765, Loss: 11.1956, Time: 0.047s\n",
      "  Batch 340/765, Loss: 7.2108, Time: 0.071s\n",
      "  Batch 350/765, Loss: 7.1645, Time: 0.120s\n",
      "  Batch 360/765, Loss: 8.4668, Time: 0.070s\n",
      "  Batch 370/765, Loss: 6.2412, Time: 0.080s\n",
      "  Batch 380/765, Loss: 9.6079, Time: 0.069s\n",
      "  Batch 390/765, Loss: 9.3723, Time: 0.072s\n",
      "  Batch 400/765, Loss: 11.8302, Time: 0.070s\n",
      "  Batch 410/765, Loss: 7.6980, Time: 0.071s\n",
      "  Batch 420/765, Loss: 9.5135, Time: 0.048s\n",
      "  Batch 430/765, Loss: 10.2681, Time: 0.070s\n",
      "  Batch 440/765, Loss: 7.0344, Time: 0.068s\n",
      "  Batch 450/765, Loss: 10.6929, Time: 0.070s\n",
      "  Batch 460/765, Loss: 8.8941, Time: 0.069s\n",
      "  Batch 470/765, Loss: 10.7118, Time: 0.045s\n",
      "  Batch 480/765, Loss: 7.0806, Time: 0.202s\n",
      "  Batch 490/765, Loss: 10.0003, Time: 0.071s\n",
      "  Batch 500/765, Loss: 5.9445, Time: 0.070s\n",
      "  Batch 510/765, Loss: 7.0888, Time: 0.071s\n",
      "  Batch 520/765, Loss: 8.9042, Time: 0.070s\n",
      "  Batch 530/765, Loss: 10.2103, Time: 0.070s\n",
      "  Batch 540/765, Loss: 12.8120, Time: 0.112s\n",
      "  Batch 550/765, Loss: 7.4817, Time: 0.069s\n",
      "  Batch 560/765, Loss: 9.3271, Time: 0.073s\n",
      "  Batch 570/765, Loss: 8.1466, Time: 0.070s\n",
      "  Batch 580/765, Loss: 7.4625, Time: 0.129s\n",
      "  Batch 590/765, Loss: 9.9482, Time: 0.070s\n",
      "  Batch 600/765, Loss: 9.3120, Time: 0.075s\n",
      "  Batch 610/765, Loss: 10.1008, Time: 0.083s\n",
      "  Batch 620/765, Loss: 9.8009, Time: 0.070s\n",
      "  Batch 630/765, Loss: 9.3656, Time: 0.081s\n",
      "  Batch 640/765, Loss: 8.6155, Time: 0.073s\n",
      "  Batch 650/765, Loss: 8.6663, Time: 0.070s\n",
      "  Batch 660/765, Loss: 7.7166, Time: 0.070s\n",
      "  Batch 670/765, Loss: 7.9642, Time: 0.071s\n",
      "  Batch 680/765, Loss: 8.4812, Time: 0.069s\n",
      "  Batch 690/765, Loss: 8.5380, Time: 0.071s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 4: HYPERPARAMETER OPTIMIZATION\n",
    "# ============================================================\n",
    "\n",
    "# Run ADE optimization to find best hyperparameters\n",
    "print(\"Running ADE optimization for hyperparameters...\")\n",
    "best_params = optimize_hyperparameters(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    feature_cols=expanded_feature_cols,\n",
    "    num_stations=len(all_stations)\n",
    ")\n",
    "\n",
    "print(\"Optimized hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"- {param}: {value}\")\n",
    "    \n",
    "print(\"BLOCK 4 COMPLETED: Hyperparameter optimization successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BLOCK 5: MODEL TRAINING WITH OPTIMIZED PARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "# Now recreate data loaders with optimized batch size\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=1,  # Usually keep test batch size at 1 for detailed evaluation\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Create model with optimized parameters\n",
    "model = TemporalFusionTransformer(\n",
    "    num_features=len(expanded_feature_cols),\n",
    "    num_stations=len(all_stations),\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_heads=1,  # Fixed as per requirements\n",
    "    dropout=0.1,\n",
    "    forecast_horizon=24,\n",
    "    hidden_layers=best_params['hidden_layers'] \n",
    ")\n",
    "\n",
    "# Train model with optimized parameters\n",
    "print(f\"Training optimized TFT model...\")\n",
    "model, train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    epochs=20,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "print(\"BLOCK 5 COMPLETED: Model training successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 6: LOSS VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss - LA Downtown')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('training_validation_loss.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"BLOCK 6 COMPLETED: Loss visualization successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 7: MODEL EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "# Evaluate model\n",
    "print(f\"Evaluating model on LA Downtown...\")\n",
    "rmse, mae, r2, station_metrics = evaluate_model(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    station_ids=all_stations,\n",
    "    regions=regions\n",
    ")\n",
    "\n",
    "print(f\"Overall metrics - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    station_ids=all_stations,\n",
    "    regions=regions,\n",
    "    season=\"All Seasons - LA Downtown\"\n",
    ")\n",
    "\n",
    "print(\"BLOCK 7 COMPLETED: Model evaluation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 8: SEASONAL ANALYSIS - DATASET CREATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nCreating seasonal datasets...\")\n",
    "seasonal_datasets = create_seasonal_datasets(df, target_days, all_stations, feature_cols)\n",
    "\n",
    "# Visualize predictions for each season\n",
    "for season, season_loader in seasonal_datasets.items():\n",
    "    print(f\"Visualizing predictions for {season}...\")\n",
    "    visualize_predictions(\n",
    "        model=model,\n",
    "        data_loader=season_loader,\n",
    "        station_ids=all_stations,\n",
    "        regions=regions,\n",
    "        season=season\n",
    "    )\n",
    "\n",
    "print(\"BLOCK 8 COMPLETED: Seasonal datasets creation successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 9: SEASONAL PERFORMANCE EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "# Evaluate for each season\n",
    "seasonal_results = {}\n",
    "for season, season_loader in seasonal_datasets.items():\n",
    "    if len(season_loader) > 0:\n",
    "        print(f\"Evaluating model on {season} data...\")\n",
    "        rmse, mae, r2, _ = evaluate_model(\n",
    "            model=model,\n",
    "            data_loader=season_loader,\n",
    "            station_ids=all_stations,\n",
    "            regions=regions\n",
    "        )\n",
    "        seasonal_results[season] = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        }\n",
    "        print(f\"{season} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "print(\"BLOCK 9 COMPLETED: Seasonal performance evaluation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 10: SEASONAL MIRROR PLOTS\n",
    "# ============================================================\n",
    "\n",
    "# Function to unnormalize temperature data\n",
    "def unnormalize_temperature(temp_normalized, temp_min, temp_max):\n",
    "    \"\"\"\n",
    "    Convert normalized temperature back to original scale.\n",
    "    \"\"\"\n",
    "    return temp_normalized * (temp_max - temp_min) + temp_min\n",
    "\n",
    "# Get the temperature normalization parameters\n",
    "temp_min = df['Temperature_C'].min()\n",
    "temp_max = df['Temperature_C'].max()\n",
    "print(f\"Temperature normalization range: min={temp_min:.2f}, max={temp_max:.2f}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for season, seasonal_data in seasonal_datasets.items():\n",
    "    print(f\"Processing {season} forecast...\")\n",
    "    \n",
    "    try:\n",
    "        # This is a list with a single data point\n",
    "        (X_temporal, X_static), y = seasonal_data[0]\n",
    "        \n",
    "        # Move to device\n",
    "        X_temporal = X_temporal.to(device)\n",
    "        X_static = X_static.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model((X_temporal, X_static))\n",
    "        \n",
    "        # Move to CPU for plotting\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        y_np = y.numpy()\n",
    "        \n",
    "        # Unnormalize the temperature data\n",
    "        outputs_unnorm = unnormalize_temperature(outputs, temp_min, temp_max)\n",
    "        y_np_unnorm = unnormalize_temperature(y_np, temp_min, temp_max)\n",
    "        \n",
    "        # Create the mirror plot with unnormalized data\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "        \n",
    "        hours = np.arange(24)\n",
    "        \n",
    "        # Left plot - Actual temperatures\n",
    "        ax1.plot(hours, y_np_unnorm[0, 0, :], 'b-o', linewidth=2)\n",
    "        ax1.set_title(f\"Actual Temperature - {season}\", fontsize=14)\n",
    "        ax1.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax1.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "        ax1.set_xlim(0, 23)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.invert_xaxis()  # Invert x-axis for mirror effect\n",
    "        \n",
    "        # Right plot - Predicted temperatures\n",
    "        ax2.plot(hours, outputs_unnorm[0, 0, :], 'r-o', linewidth=2)\n",
    "        ax2.set_title(f\"Predicted Temperature - {season}\", fontsize=14)\n",
    "        ax2.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax2.set_xlim(0, 23)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add overall title\n",
    "        plt.suptitle(f\"{all_stations[0]} ({regions.get(all_stations[0], 'Unknown')}) - {season} Day Comparison\", \n",
    "                    fontsize=16, y=1.05)\n",
    "        \n",
    "        # Tight layout with minimal space between plots\n",
    "        fig.tight_layout()\n",
    "        plt.subplots_adjust(wspace=0.05)  # Reduce space between subplots\n",
    "        \n",
    "        # Add a legend to explain colors\n",
    "        ax1.plot([], [], 'b-o', label='Actual Temperature')\n",
    "        ax2.plot([], [], 'r-o', label='Predicted Temperature')\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "        \n",
    "        # Save and display\n",
    "        plt.savefig(f\"{season}_mirror_comparison.png\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {season} forecast: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"BLOCK 10 COMPLETED: Seasonal mirror plots created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 11: FINAL ANALYSIS AND RESULTS SAVING\n",
    "# ============================================================\n",
    "\n",
    "# Analyze seasonal performance\n",
    "if seasonal_results:\n",
    "    analyze_seasonal_performance(seasonal_results)\n",
    "\n",
    "print(\"Skipping topographic analysis since we only have data for San Jose\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame([{\n",
    "    'Station': 'San Jose',\n",
    "    'Region': 'urban',\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2\n",
    "}])\n",
    "\n",
    "results_df.to_csv('la_downtown_results.csv', index=False)\n",
    "print(\"Results saved to la_downtown_results.csv\")\n",
    "\n",
    "print(\"BLOCK 11 COMPLETED: Final analysis and results saving successful.\")\n",
    "print(\"=\" * 70)\n",
    "print(\"ALL BLOCKS EXECUTED SUCCESSFULLY! Analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7096681,
     "sourceId": 11342745,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7114409,
     "sourceId": 11365940,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7114641,
     "sourceId": 11366236,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7114978,
     "sourceId": 11366666,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7124793,
     "sourceId": 11379404,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
