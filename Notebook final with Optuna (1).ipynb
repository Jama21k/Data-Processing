{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:58:27.778970Z",
     "iopub.status.busy": "2025-04-15T12:58:27.778601Z",
     "iopub.status.idle": "2025-04-15T12:58:27.784802Z",
     "shell.execute_reply": "2025-04-15T12:58:27.783818Z",
     "shell.execute_reply.started": "2025-04-15T12:58:27.778946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"GNN Model.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1pSO-VEyn5Cywjw9sXKn2fjXdVbI3hAsH\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Untitled2.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/193StgLnr4doKklAxwBiQsVX3njEfb1oa\n",
    "\"\"\"\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\jamak\\OneDrive\\Thesis\\Merged_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:58:32.355467Z",
     "iopub.status.busy": "2025-04-15T12:58:32.354707Z",
     "iopub.status.idle": "2025-04-15T12:58:36.162939Z",
     "shell.execute_reply": "2025-04-15T12:58:36.161897Z",
     "shell.execute_reply.started": "2025-04-15T12:58:32.355432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vmdpy in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from vmdpy) (1.26.4)\n",
      "Requirement already satisfied: optuna in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\jamak\\downloads\\ana\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install vmdpy\n",
    "!pip install optuna\n",
    "import optuna\n",
    "from vmdpy import VMD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:01:50.336244Z",
     "iopub.status.busy": "2025-04-15T13:01:50.335695Z",
     "iopub.status.idle": "2025-04-15T13:01:50.818103Z",
     "shell.execute_reply": "2025-04-15T13:01:50.817094Z",
     "shell.execute_reply.started": "2025-04-15T13:01:50.336204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamak\\Downloads\\Ana\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/vmdpy')  # Adjust the path if needed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Check for GPU and set device appropriately for Kaggle T4\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set optimized CUDA options for T4 GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False  # Better performance but less reproducible\n",
    "    # Set to float16 precision for faster computation on T4 GPU\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# ============================================================\n",
    "# MODIFIED DATA LOADING AND PROCESSING FOR TARGETED SEASONAL ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load weather data, clean missing values, and filter to LA Downtown only.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine file type and read\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    # Convert timestamp\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    elif 'DATE' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['DATE'])\n",
    "        df = df.rename(columns={'DATE': 'date_original'})\n",
    "\n",
    "    # Ensure time-based ordering before interpolation\n",
    "    df = df.sort_values(by='timestamp')\n",
    "\n",
    "    # Add hour of day feature - sine/cosine encoding for cyclical pattern\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "\n",
    "    # Add day of year feature - sine/cosine encoding for cyclical pattern\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n",
    "\n",
    "    # Define region mapping for LA Downtown only\n",
    "    region_mapping = {\n",
    "        'LA Downtown': 'urban'\n",
    "    }\n",
    "\n",
    "    # Filter to keep only LA Downtown\n",
    "    if 'station_id' in df.columns:\n",
    "        # Filter to LA Downtown only\n",
    "        df = df[df['station_id'] == 'LA Downtown']\n",
    "        \n",
    "        # Add region information\n",
    "        df['region'] = df['station_id'].map(region_mapping)\n",
    "\n",
    "        # Convert region to numerical encoding\n",
    "        region_to_num = {region: i for i, region in enumerate(df['region'].unique())}\n",
    "        df['region_code'] = df['region'].map(region_to_num)\n",
    "\n",
    "        # Add elevation for LA Downtown\n",
    "        elevation_mapping = {\n",
    "            'LA Downtown': 93         # meters\n",
    "        }\n",
    "        df['elevation'] = df['station_id'].map(elevation_mapping)\n",
    "        # Normalize elevation (will be constant for single station)\n",
    "        df['elevation_norm'] = 0.0  # Since we only have one station, just use 0 as normalized value\n",
    "\n",
    "        print(f\"Filtered data to LA Downtown only\")\n",
    "\n",
    "    # Interpolate missing values along the time dimension\n",
    "    df.interpolate(method='linear', limit_direction='both', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# VMD DECOMPOSITION\n",
    "# ============================================================\n",
    "\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def perform_vmd_decomposition(signal, alpha=2000, tau=0, K=10, DC=0, init=1, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Perform VMD decomposition on a signal.\n",
    "    \"\"\"\n",
    "    # Run VMD\n",
    "    u, u_hat, omega = VMD(signal, alpha, tau, K, DC, init, tol)\n",
    "    \n",
    "    # Calculate reconstruction residual\n",
    "    recon = np.sum(u, axis=0)\n",
    "    rres = np.sqrt(np.mean((signal - recon)**2)) / np.sqrt(np.mean(signal**2)) * 100\n",
    "    \n",
    "    return u, rres\n",
    "\n",
    "def find_optimal_k(signal, k_range=range(6, 15)):\n",
    "    \"\"\"\n",
    "    Find optimal K where rres < 3% with no sharp drop.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        print(f\"  Testing K={k}\")\n",
    "        _, rres = perform_vmd_decomposition(signal, K=k)\n",
    "        results.append((k, rres))\n",
    "        print(f\"  K={k}, rres={rres:.2f}%\")\n",
    "        if rres < 3.0:\n",
    "            return k, rres\n",
    "            \n",
    "    # If no K achieves rres < 3%, return the K with lowest rres\n",
    "    return min(results, key=lambda x: x[1])\n",
    "\n",
    "def decompose_station_data(data, feature_name, station_id, cache_dir='vmd_cache'):\n",
    "    \"\"\"\n",
    "    Decompose time series data for a single station.\n",
    "    \"\"\"\n",
    "    print(f\"Starting decomposition for {station_id} - {feature_name}\")\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a unique cache filename\n",
    "    cache_file = f\"{cache_dir}/vmd_{station_id}_{feature_name}.joblib\"\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading cached VMD decomposition for {station_id} - {feature_name}\")\n",
    "        return joblib.load(cache_file)\n",
    "    \n",
    "    # Get the time series to decompose\n",
    "    print(f\"  Extracting signal for {station_id}\")\n",
    "    signal = data[feature_name].values\n",
    "    \n",
    "    # Normalize signal\n",
    "    print(f\"  Normalizing signal\")\n",
    "    signal_norm = (signal - np.mean(signal)) / np.std(signal)\n",
    "    \n",
    "    # Find optimal K\n",
    "    print(f\"  Finding optimal K\")\n",
    "    k_opt, rres = find_optimal_k(signal_norm)\n",
    "    print(f\"Station {station_id}, feature {feature_name}: Optimal K={k_opt}, rres={rres:.2f}%\")\n",
    "    \n",
    "    # Perform VMD with optimal K\n",
    "    print(f\"  Performing VMD with K={k_opt}\")\n",
    "    u, _ = perform_vmd_decomposition(signal_norm, K=k_opt)\n",
    "    \n",
    "    # Denormalize modes\n",
    "    print(f\"  Denormalizing modes\")\n",
    "    u_denorm = u * np.std(signal) + np.mean(signal) / k_opt\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"  Saving to cache: {cache_file}\")\n",
    "    joblib.dump(u_denorm, cache_file)\n",
    "    \n",
    "    print(f\"Completed decomposition for {station_id} - {feature_name}\")\n",
    "    return u_denorm\n",
    "\n",
    "def parallel_vmd_decomposition(df, feature_cols_to_decompose=['Temperature_C']):\n",
    "    \"\"\"\n",
    "    Apply VMD decomposition to multiple stations serially to avoid multiprocessing issues.\n",
    "    \"\"\"\n",
    "    # Get the unique station IDs \n",
    "    station_ids = ['LA Downtown']  # We only have LA Downtown in this code\n",
    "    \n",
    "    decomposed_data = {}\n",
    "    \n",
    "    for feature in feature_cols_to_decompose:\n",
    "        print(f\"Performing VMD decomposition for feature: {feature}\")\n",
    "        \n",
    "        # Process each station serially instead of in parallel\n",
    "        results = []\n",
    "        for station_id in station_ids:\n",
    "            print(f\"  Processing station: {station_id}\")\n",
    "            result = decompose_station_data(df, feature, station_id)\n",
    "            results.append(result)\n",
    "            \n",
    "        # Store results\n",
    "        for i, station_id in enumerate(station_ids):\n",
    "            if station_id not in decomposed_data:\n",
    "                decomposed_data[station_id] = {}\n",
    "            decomposed_data[station_id][feature] = results[i]\n",
    "    \n",
    "    return decomposed_data\n",
    "\n",
    "def extract_target_days(df):\n",
    "    \"\"\"\n",
    "    Extract the 4 specific target days (one per season) for predictions\n",
    "    \"\"\"\n",
    "    # Define target days\n",
    "    target_days = [\n",
    "        {'season': 'Spring', 'month': 4, 'day': 15},  # April 15\n",
    "        {'season': 'Summer', 'month': 7, 'day': 20},  # July 20\n",
    "        {'season': 'Fall', 'month': 10, 'day': 10},   # October 10\n",
    "        {'season': 'Winter', 'month': 1, 'day': 15}   # January 15\n",
    "    ]\n",
    "\n",
    "    # Filter for each target day\n",
    "    target_data = {}\n",
    "    for target in target_days:\n",
    "        # Filter by month and day\n",
    "        day_data = df[(df['timestamp'].dt.month == target['month']) &\n",
    "                       (df['timestamp'].dt.day == target['day'])]\n",
    "\n",
    "        # Get the most recent year that has data for this day\n",
    "        if not day_data.empty:\n",
    "            latest_year = day_data['timestamp'].dt.year.max()\n",
    "            target_day_data = day_data[day_data['timestamp'].dt.year == latest_year]\n",
    "            target_data[target['season']] = target_day_data\n",
    "            print(f\"Found {len(target_day_data)} records for {target['season']} target day ({target['month']}/{target['day']}/{latest_year})\")\n",
    "        else:\n",
    "            print(f\"WARNING: No data found for {target['season']} target day\")\n",
    "\n",
    "    return target_data\n",
    "\n",
    "def prepare_seasonal_training_data(df, target_days):\n",
    "    \"\"\"\n",
    "    For each target day, prepare all historical data for training\n",
    "    \"\"\"\n",
    "    training_sets = {}\n",
    "\n",
    "    for season, target_day_data in target_days.items():\n",
    "        if target_day_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Get the date of this target\n",
    "        sample_date = target_day_data['timestamp'].iloc[0]\n",
    "        target_year = sample_date.year\n",
    "\n",
    "        # Use all historical data prior to the target year\n",
    "        historical_data = df[df['timestamp'].dt.year < target_year]\n",
    "\n",
    "        training_sets[season] = historical_data\n",
    "        print(f\"{season} training set: {len(historical_data)} samples from all historical data\")\n",
    "\n",
    "    return training_sets\n",
    "\n",
    "def normalize_features(train_df, val_df, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalize features using StandardScaler fitted on training data\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit on training data\n",
    "    scaler.fit(train_df[feature_cols])\n",
    "\n",
    "    # Transform datasets\n",
    "    train_scaled = scaler.transform(train_df[feature_cols])\n",
    "    val_scaled = scaler.transform(val_df[feature_cols])\n",
    "\n",
    "    # Convert back to DataFrames\n",
    "    train_norm = pd.DataFrame(train_scaled, columns=feature_cols, index=train_df.index)\n",
    "    val_norm = pd.DataFrame(val_scaled, columns=feature_cols, index=val_df.index)\n",
    "\n",
    "    return train_norm, val_norm, scaler\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for weather forecasting with sliding window approach.\n",
    "    Modified to work efficiently with single station (LA Downtown).\n",
    "    \"\"\"\n",
    "    def __init__(self, df, station_ids, feature_cols, seq_length=24, forecast_horizon=24):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with weather data (filtered to LA Downtown only)\n",
    "            station_ids: List containing only 'LA Downtown'\n",
    "            feature_cols: List of feature columns to use as input\n",
    "            seq_length: Length of input sequence (in hours)\n",
    "            forecast_horizon: How many hours ahead to predict\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.station_ids = station_ids\n",
    "        self.feature_cols = feature_cols\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.n_stations = len(station_ids)  # Should be 1\n",
    "\n",
    "        # Get unique timestamps (now all from one station)\n",
    "        self.timestamps = sorted(df['timestamp'].unique())\n",
    "\n",
    "        # Filter valid timestamps (those that have enough history and future data)\n",
    "        valid_idx = []\n",
    "        for i in range(len(self.timestamps) - (seq_length + forecast_horizon - 1)):\n",
    "            # Check if we have continuous data for this window\n",
    "            current_time = self.timestamps[i]\n",
    "            end_time = self.timestamps[i + seq_length + forecast_horizon - 1]\n",
    "            expected_duration = timedelta(hours=seq_length + forecast_horizon - 1)\n",
    "\n",
    "            if (end_time - current_time) == expected_duration:\n",
    "                valid_idx.append(i)\n",
    "\n",
    "        self.valid_indices = valid_idx\n",
    "\n",
    "        # Add a safety check to ensure we have at least one valid window\n",
    "        if len(self.valid_indices) == 0:\n",
    "            print(f\"WARNING: No valid windows found in dataset. Using reduced requirements.\")\n",
    "            # Fall back to allowing any windows where we have both input and output data\n",
    "            valid_idx = []\n",
    "            for i in range(len(self.timestamps) - seq_length):\n",
    "                if i + seq_length < len(self.timestamps):\n",
    "                    valid_idx.append(i)\n",
    "            self.valid_indices = valid_idx\n",
    "            self.fallback_mode = True\n",
    "            print(f\"Found {len(self.valid_indices)} windows with relaxed continuity requirements\")\n",
    "        else:\n",
    "            self.fallback_mode = False\n",
    "            print(f\"Created dataset with {len(self.valid_indices)} valid windows\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.valid_indices))  # Ensure length is at least 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.valid_indices) == 0:\n",
    "            # Return dummy data if no valid indices\n",
    "            X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))\n",
    "            y = np.zeros((self.n_stations, self.forecast_horizon))\n",
    "            static_features = np.zeros((self.n_stations, 2))  # region_code and elevation\n",
    "            return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n",
    "\n",
    "        # Get actual data when possible\n",
    "        start_idx = self.valid_indices[idx % len(self.valid_indices)]\n",
    "\n",
    "        # Get timestamps for input and output windows\n",
    "        input_timestamps = self.timestamps[start_idx:start_idx + self.seq_length]\n",
    "        output_timestamps = self.timestamps[start_idx + self.seq_length:\n",
    "                                            start_idx + self.seq_length + self.forecast_horizon]\n",
    "\n",
    "        # Handle potential shortfall in output window (fallback mode)\n",
    "        if self.fallback_mode and len(output_timestamps) < self.forecast_horizon:\n",
    "            # Pad with repetition of last timestamp if needed\n",
    "            last_time = output_timestamps[-1] if len(output_timestamps) > 0 else input_timestamps[-1]\n",
    "            padding = [last_time] * (self.forecast_horizon - len(output_timestamps))\n",
    "            output_timestamps = list(output_timestamps) + padding\n",
    "\n",
    "        # Initialize tensors - simpler now with just one station\n",
    "        X = np.zeros((len(self.feature_cols), 1, self.seq_length))  # Only one station\n",
    "        y = np.zeros((1, self.forecast_horizon))  # Only one station\n",
    "        static_features = np.zeros((1, 2))  # region_code and elevation\n",
    "\n",
    "        # LA Downtown is our only station\n",
    "        station_id = 'LA Downtown'\n",
    "        \n",
    "        # Input sequence\n",
    "        for t_idx, ts in enumerate(input_timestamps):\n",
    "            station_data = self.df[self.df['timestamp'] == ts]\n",
    "\n",
    "            if not station_data.empty:\n",
    "                for f_idx, feat in enumerate(self.feature_cols):\n",
    "                    X[f_idx, 0, t_idx] = station_data[feat].values[0]\n",
    "\n",
    "                # Store static features (same for all timestamps)\n",
    "                if t_idx == 0:\n",
    "                    static_features[0, 0] = station_data['region_code'].values[0]\n",
    "                    static_features[0, 1] = station_data['elevation_norm'].values[0]\n",
    "\n",
    "        # Target sequence (temperature only)\n",
    "        for t_idx, ts in enumerate(output_timestamps):\n",
    "            if t_idx < self.forecast_horizon:  # Safety check\n",
    "                station_data = self.df[self.df['timestamp'] == ts]\n",
    "\n",
    "                if not station_data.empty:\n",
    "                    y[0, t_idx] = station_data['Temperature_C'].values[0]\n",
    "\n",
    "        return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n",
    "    \n",
    "\n",
    "class TimeSeriesCVSplit:\n",
    "    \"\"\"\n",
    "    Time series cross-validation with multiple train-validation folds.\n",
    "    Supports both expanding window and sliding window approaches.\n",
    "    \"\"\"\n",
    "    def __init__(self, timestamps, n_splits=5, test_size=0.15, gap=24, min_train_size=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            timestamps: Array of all timestamps in the dataset\n",
    "            n_splits: Number of cross-validation folds\n",
    "            test_size: Size of validation set in each fold (as fraction)\n",
    "            gap: Minimum gap between train and validation sets (in hours)\n",
    "            min_train_size: Minimum size of training set (as fraction)\n",
    "        \"\"\"\n",
    "        self.timestamps = np.array(timestamps)\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.gap = gap\n",
    "        self.min_train_size = min_train_size\n",
    "        \n",
    "    def split(self):\n",
    "        \"\"\"Generate train/validation indices for each fold.\"\"\"\n",
    "        n_samples = len(self.timestamps)\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        # Size of each test fold\n",
    "        test_size = int(n_samples * self.test_size)\n",
    "        # Minimum training set size\n",
    "        min_train_size = int(n_samples * self.min_train_size)\n",
    "        \n",
    "        # Generate folds\n",
    "        for i in range(self.n_splits):\n",
    "            # For the last fold, use the last test_size samples as validation\n",
    "            if i == self.n_splits - 1:\n",
    "                test_start = n_samples - test_size\n",
    "            else:\n",
    "                # Distribute validation sets evenly across the timeline\n",
    "                test_start = min_train_size + i * ((n_samples - min_train_size - test_size) // max(1, self.n_splits - 1))\n",
    "            \n",
    "            test_end = test_start + test_size\n",
    "            \n",
    "            # Apply gap between train and test\n",
    "            train_end = max(0, test_start - self.gap)\n",
    "            \n",
    "            # Get the actual indices\n",
    "            train_indices = indices[:train_end]\n",
    "            test_indices = indices[test_start:test_end]\n",
    "            \n",
    "            yield train_indices, test_indices\n",
    "            \n",
    "    def get_fold_timestamps(self):\n",
    "        \"\"\"Return the timestamp ranges for each fold (for visualization).\"\"\"\n",
    "        fold_timestamps = []\n",
    "        \n",
    "        for train_idx, val_idx in self.split():\n",
    "            train_start = self.timestamps[train_idx[0]]\n",
    "            train_end = self.timestamps[train_idx[-1]]\n",
    "            val_start = self.timestamps[val_idx[0]]\n",
    "            val_end = self.timestamps[val_idx[-1]]\n",
    "            \n",
    "            fold_timestamps.append({\n",
    "                'fold': len(fold_timestamps) + 1,\n",
    "                'train_range': (train_start, train_end),\n",
    "                'val_range': (val_start, val_end)\n",
    "            })\n",
    "            \n",
    "        return fold_timestamps\n",
    "# ============================================================\n",
    "# TEMPORAL FUSION TRANSFORMER IMPLEMENTATION\n",
    "# ============================================================\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention layer for temporal data.\n",
    "    Simplified from the original TFT paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=2, dropout=0.1):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        # Linear projections\n",
    "        queries = self.query(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = self.key(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        values = self.value(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "        # Final linear layer\n",
    "        return self.out(out)\n",
    "\n",
    "class GatedResidualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Residual Network as described in the TFT paper.\n",
    "    Simplified version with fewer layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
    "        super(GatedResidualNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # If input and output sizes are different, apply a skip connection\n",
    "        self.skip_layer = None\n",
    "        if input_size != output_size:\n",
    "            self.skip_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "        # Main layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.gate = nn.Linear(input_size + output_size, output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch\n",
    "        hidden = F.elu(self.fc1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "\n",
    "        # Skip connection\n",
    "        if self.skip_layer is not None:\n",
    "            skip = self.skip_layer(x)\n",
    "        else:\n",
    "            skip = x\n",
    "\n",
    "        # Gate mechanism\n",
    "        gate_input = torch.cat([x, hidden], dim=-1)\n",
    "        gate = torch.sigmoid(self.gate(gate_input))\n",
    "\n",
    "        # Combine using gate\n",
    "        output = gate * hidden + (1 - gate) * skip\n",
    "\n",
    "        # Layer normalization\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class VariableSelectionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Variable Selection Network for TFT.\n",
    "    Simplified version with fewer layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size_per_var, num_vars, hidden_size, output_size, dropout=0.1):\n",
    "        super(VariableSelectionNetwork, self).__init__()\n",
    "        self.input_size_per_var = input_size_per_var\n",
    "        self.num_vars = num_vars\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # GRN for variable weights\n",
    "        self.weight_grn = GatedResidualNetwork(\n",
    "            input_size=input_size_per_var * num_vars,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=num_vars,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # GRN for each variable\n",
    "        self.var_grns = nn.ModuleList([\n",
    "            GatedResidualNetwork(\n",
    "                input_size=input_size_per_var,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_vars)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_vars, input_size_per_var]\n",
    "        batch_size = x.size(0)\n",
    "        flat_x = x.view(batch_size, -1)\n",
    "\n",
    "        # Calculate variable weights\n",
    "        var_weights = self.weight_grn(flat_x)\n",
    "        var_weights = F.softmax(var_weights, dim=-1).unsqueeze(-1)  # [batch_size, num_vars, 1]\n",
    "\n",
    "        # Transform each variable\n",
    "        var_outputs = []\n",
    "        for i in range(self.num_vars):\n",
    "            var_outputs.append(self.var_grns[i](x[:, i]))\n",
    "\n",
    "        var_outputs = torch.stack(var_outputs, dim=1)  # [batch_size, num_vars, output_size]\n",
    "\n",
    "        # Weighted combination\n",
    "        outputs = torch.sum(var_outputs * var_weights, dim=1)  # [batch_size, output_size]\n",
    "\n",
    "        return outputs, var_weights\n",
    "\n",
    "class TemporalFusionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Temporal Fusion Transformer specifically for single-station forecasting.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_size=64, num_heads=1, \n",
    "                 dropout=0.1, forecast_horizon=24, hidden_layers=2):\n",
    "        super(TemporalFusionTransformer, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Static variable processing\n",
    "        self.static_var_processor = GatedResidualNetwork(\n",
    "            input_size=2,  # region_code, elevation\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Variable selection for time-varying features\n",
    "        self.temporal_var_selection = VariableSelectionNetwork(\n",
    "            input_size_per_var=24,  # Sequence length per feature\n",
    "            num_vars=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # LSTM encoder layers\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size=hidden_size if i == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "                batch_first=True\n",
    "            ) for i in range(hidden_layers)\n",
    "        ])\n",
    "        \n",
    "        # Temporal self-attention\n",
    "        self.self_attention = TemporalSelfAttention(\n",
    "            d_model=hidden_size,\n",
    "            n_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Final output layer for forecasting (single prediction)\n",
    "        self.forecast_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, forecast_horizon)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Unpack inputs - simplified for single station\n",
    "        temporal_features, static_features = inputs\n",
    "        batch_size = temporal_features.size(0)\n",
    "        \n",
    "        # Process static features\n",
    "        static_embeddings = self.static_var_processor(static_features)\n",
    "        \n",
    "        # Process temporal features with variable selection\n",
    "        temporal_embeddings, temporal_weights = self.temporal_var_selection(temporal_features)\n",
    "        \n",
    "        # Reshape to [batch, seq_len, hidden]\n",
    "        temporal_embeddings = temporal_embeddings.unsqueeze(1).expand(-1, 24, -1)\n",
    "        \n",
    "        # Add static embeddings to each timestep\n",
    "        temporal_embeddings = temporal_embeddings + static_embeddings.unsqueeze(1)\n",
    "        \n",
    "        # Pass through LSTM layers\n",
    "        lstm_out = temporal_embeddings\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            lstm_out, _ = lstm_layer(lstm_out)\n",
    "        \n",
    "        # Self-attention\n",
    "        attention_out = self.self_attention(lstm_out)\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = self.forecast_projection(attention_out[:, -1, :])\n",
    "        \n",
    "        return forecast.unsqueeze(1)  # [batch, 1, horizon]\n",
    "        \n",
    "# ============================================================\n",
    "# ADE OPTIMIZATION\n",
    "# ============================================================\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "def objective(trial, train_loader, val_loader, feature_cols, num_stations):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna optimization.\n",
    "    \"\"\"\n",
    "    # Sample hyperparameters\n",
    "    time_steps = trial.suggest_int('time_steps', 12, 24)\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 64)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.05, log=True)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 32)\n",
    "    hidden_layers = trial.suggest_int('hidden_layers', 2, 3)\n",
    "    \n",
    "    # Create model\n",
    "    model = TemporalFusionTransformer(\n",
    "        num_features=len(feature_cols),\n",
    "        num_stations=num_stations,\n",
    "        hidden_size=hidden_size,\n",
    "        num_heads=1,  # Fixed as per requirements\n",
    "        dropout=0.1,\n",
    "        forecast_horizon=24,\n",
    "        hidden_layers=hidden_layers\n",
    "    )\n",
    "    \n",
    "    # Train model with limited epochs for faster optimization\n",
    "    model, train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=10,  # Reduced from 20 to speed up optimization\n",
    "        patience=3,  # Reduced from 5 to speed up optimization\n",
    "        use_checkpoint=False  # Don't save checkpoints during optimization\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate MAPE\n",
    "            epsilon = 1e-10  # Small constant to avoid division by zero\n",
    "            mape = torch.mean(torch.abs((targets - outputs) / (targets + epsilon))) * 100\n",
    "            \n",
    "            val_loss += mape.item()\n",
    "            val_batches += 1\n",
    "    \n",
    "    avg_val_mape = val_loss / max(1, val_batches)\n",
    "    print(f\"Trial {trial.number}: Params: time_steps={time_steps}, batch_size={batch_size}, lr={learning_rate:.5f}, \"\n",
    "          f\"hidden_size={hidden_size}, hidden_layers={hidden_layers}, Validation MAPE: {avg_val_mape:.4f}%\")\n",
    "    \n",
    "    # Report intermediate values to enable pruning\n",
    "    trial.report(avg_val_mape, step=1)\n",
    "    \n",
    "    # Handle pruning based on intermediate values\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return avg_val_mape\n",
    "\n",
    "def cross_validate_model(model_class, model_kwargs, train_kwargs, feature_cols, num_stations, ts_cv, full_dataset, fold_splits):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation with time series splits.\n",
    "    \n",
    "    Args:\n",
    "        model_class: Model class to instantiate\n",
    "        model_kwargs: Keyword arguments for model instantiation\n",
    "        train_kwargs: Keyword arguments for training function\n",
    "        feature_cols: Feature columns\n",
    "        num_stations: Number of stations\n",
    "        ts_cv: TimeSeriesCVSplit instance\n",
    "        full_dataset: Full dataset\n",
    "        fold_splits: Pre-computed fold splits\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with cross-validation results\n",
    "    \"\"\"\n",
    "    cv_results = []\n",
    "    \n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(fold_splits):\n",
    "        print(f\"\\n=== Fold {fold_idx + 1}/{len(fold_splits)} ===\")\n",
    "        \n",
    "        # Create samplers and loaders\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        val_sampler = SubsetRandomSampler(val_indices)\n",
    "        \n",
    "        # Create data loaders\n",
    "        batch_size = train_kwargs.get('batch_size', 32)\n",
    "        train_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = model_class(**model_kwargs)\n",
    "        \n",
    "        # Train model\n",
    "        model, train_losses, val_losses = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            **{k: v for k, v in train_kwargs.items() if k != 'batch_size'}\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_rmse, val_mae, val_r2, _ = evaluate_model(\n",
    "            model, val_loader, ['LA Downtown'], {'LA Downtown': 'urban'}\n",
    "        )\n",
    "        \n",
    "        cv_results.append({\n",
    "            'fold': fold_idx + 1,\n",
    "            'rmse': val_rmse,\n",
    "            'mae': val_mae,\n",
    "            'r2': val_r2,\n",
    "            'train_size': len(train_indices),\n",
    "            'val_size': len(val_indices)\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {fold_idx + 1} Results - RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, R²: {val_r2:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_rmse = np.mean([r['rmse'] for r in cv_results])\n",
    "    avg_mae = np.mean([r['mae'] for r in cv_results])\n",
    "    avg_r2 = np.mean([r['r2'] for r in cv_results])\n",
    "    std_rmse = np.std([r['rmse'] for r in cv_results])\n",
    "    \n",
    "    print(f\"\\n=== Cross-Validation Results ===\")\n",
    "    print(f\"Avg RMSE: {avg_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "    print(f\"Avg MAE: {avg_mae:.4f}\")\n",
    "    print(f\"Avg R²: {avg_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'folds': cv_results,\n",
    "        'avg_rmse': avg_rmse,\n",
    "        'avg_mae': avg_mae,\n",
    "        'avg_r2': avg_r2,\n",
    "        'std_rmse': std_rmse\n",
    "    }\n",
    "\n",
    "def optimize_hyperparameters(train_loader, val_loader, feature_cols, num_stations):\n",
    "    \"\"\"\n",
    "    Use Optuna to optimize hyperparameters.\n",
    "    \"\"\"\n",
    "    # Check for incompatible checkpoints\n",
    "    checkpoint_path = 'checkpoint.pth'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            # Create a test model with the bounds midpoint\n",
    "            test_model = TemporalFusionTransformer(\n",
    "                num_features=len(feature_cols),\n",
    "                num_stations=num_stations,\n",
    "                hidden_size=24,  # Midpoint of bounds\n",
    "                num_heads=1,\n",
    "                dropout=0.1,\n",
    "                forecast_horizon=24,\n",
    "                hidden_layers=2\n",
    "            )\n",
    "            \n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            try:\n",
    "                test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                print(\"Checkpoint is compatible with optimization parameters.\")\n",
    "            except RuntimeError as e:\n",
    "                if \"size mismatch\" in str(e):\n",
    "                    print(\"Removing incompatible checkpoint before optimization.\")\n",
    "                    os.remove(checkpoint_path)\n",
    "                else:\n",
    "                    raise e\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking checkpoint: {e}. Removing to be safe.\")\n",
    "            os.remove(checkpoint_path)\n",
    "    \n",
    "    # Create an Optuna study with TPE sampler (more efficient than default)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name=\"tft_optimization\"\n",
    "    )\n",
    "    \n",
    "    # Optimize with fewer trials than ADE's population × generations\n",
    "    start_time = time.time()\n",
    "    study.optimize(\n",
    "        func=partial(objective, train_loader=train_loader, val_loader=val_loader, \n",
    "                     feature_cols=feature_cols, num_stations=num_stations),\n",
    "        n_trials=20,  # This is less than ADE's pop_size(8) × max_iter(10) = 80 evaluations\n",
    "        timeout=1800,  # Add timeout of 30 minutes as safety\n",
    "        n_jobs=1  # Can increase if multiple CPUs/GPUs are available\n",
    "    )\n",
    "    \n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params\n",
    "    best_mape = study.best_value\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Optimization completed in {optimization_time/60:.2f} minutes\")\n",
    "    print(\"Optimal Parameters:\")\n",
    "    print(f\"Time Steps: {best_params['time_steps']}\")\n",
    "    print(f\"Batch Size: {best_params['batch_size']}\")\n",
    "    print(f\"Learning Rate: {best_params['learning_rate']:.5f}\")\n",
    "    print(f\"Hidden Size: {best_params['hidden_size']}\")\n",
    "    print(f\"Hidden Layers: {best_params['hidden_layers']}\")\n",
    "    print(f\"Validation MAPE: {best_mape:.4f}%\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Visualize the optimization process\n",
    "    try:\n",
    "        fig = optuna.visualization.plot_optimization_history(study)\n",
    "        fig.write_image(\"optuna_history.png\")\n",
    "        \n",
    "        fig = optuna.visualization.plot_param_importances(study)\n",
    "        fig.write_image(\"optuna_param_importance.png\")\n",
    "        \n",
    "        print(\"Optimization visualizations saved to files.\")\n",
    "    except:\n",
    "        print(\"Visualization could not be generated. Continue without visualizations.\")\n",
    "    \n",
    "    return {\n",
    "        'time_steps': best_params['time_steps'],\n",
    "        'batch_size': best_params['batch_size'],\n",
    "        'learning_rate': best_params['learning_rate'],\n",
    "        'hidden_size': best_params['hidden_size'],\n",
    "        'hidden_layers': best_params['hidden_layers']\n",
    "    }\n",
    "    \n",
    "def train_model(model, train_loader, val_loader, learning_rate=0.001, epochs=20, patience=5, use_checkpoint=True):\n",
    "    \"\"\"\n",
    "    Train the model with MSE loss and early stopping.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoint_path = 'checkpoint.pth'\n",
    "    # Add a model version identifier to prevent future mismatches\n",
    "    model_version = \"single_output_v1\"  # Change this when architecture changes\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Initialize optimizer and scheduler first\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "     # Try to load checkpoint, but handle architecture changes gracefully\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            # Check if the checkpoint has a version and if it matches current model\n",
    "            if 'model_version' not in checkpoint or checkpoint['model_version'] != model_version:\n",
    "                print(f\"Checkpoint is from a different model version. Current: {model_version}, Checkpoint: {checkpoint.get('model_version', 'unknown')}\")\n",
    "                print(\"Removing incompatible checkpoint and starting fresh training.\")\n",
    "                os.remove(checkpoint_path)\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                start_epoch = checkpoint['epoch'] + 1\n",
    "                patience_counter = checkpoint['patience_counter']\n",
    "                best_val_loss = checkpoint['best_val_loss']\n",
    "                print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        except RuntimeError as e:\n",
    "            if \"size mismatch\" in str(e):\n",
    "                print(\"Model architecture has changed - detected parameter size mismatch.\")\n",
    "                print(\"Removing incompatible checkpoint and starting fresh training.\")\n",
    "                # Delete the incompatible checkpoint\n",
    "                os.remove(checkpoint_path)\n",
    "            else:\n",
    "                # If it's some other kind of error, re-raise it\n",
    "                raise e\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "   \n",
    "    \n",
    "    # Add mixed precision training with autocast\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    \n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    epoch_times = []\n",
    "\n",
    "    print(f\"Starting training for {epochs} epochs with patience {patience}...\")\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "\n",
    "        batch_times = []\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            # Move to device\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                # CHANGED: Use MSE loss instead of quantile loss\n",
    "                loss = mse_loss(outputs, targets)\n",
    "\n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            batch_end = time.time()\n",
    "            batch_time = batch_end - batch_start\n",
    "            batch_times.append(batch_time)\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, Time: {batch_time:.3f}s\")\n",
    "\n",
    "        avg_train_loss = train_loss / max(1, train_batches)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n",
    "        \n",
    "        # Validation\n",
    "        val_start_time = time.time()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                # Move to device\n",
    "                if isinstance(inputs, tuple):\n",
    "                    inputs = tuple(x.to(device) for x in inputs)\n",
    "                elif isinstance(inputs, list):\n",
    "                    inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "                else:\n",
    "                    inputs = inputs.to(device)\n",
    "                    \n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                # CHANGED: Use MSE loss instead of quantile loss\n",
    "                loss = mse_loss(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / max(1, val_batches)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        val_time = time.time() - val_start_time\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Calculate estimated time remaining\n",
    "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_epochs = epochs - (epoch + 1)\n",
    "        est_time_remaining = avg_epoch_time * remaining_epochs\n",
    "        \n",
    "        # Print detailed progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} ({train_batches} batches, avg batch time: {avg_batch_time:.3f}s)\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f} (validation time: {val_time:.2f}s)\")\n",
    "        print(f\"  Epoch Time: {epoch_time:.2f}s, Est. Remaining: {est_time_remaining/60:.2f} minutes\")\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if use_checkpoint and avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model with version info\n",
    "            try:\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'patience_counter': patience_counter,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'model_version': model_version  # Save version with checkpoint\n",
    "                }, checkpoint_path)\n",
    "\n",
    "                print(f\"  Saved best model (version {model_version}) with val loss: {best_val_loss:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error saving model: {e}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement for {patience_counter}/{patience} epochs\")\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"Training completed in {total_time/60:.2f} minutes ({total_time:.1f} seconds)\")\n",
    "    \n",
    "        # At the end, only try to load the best model if use_checkpoint is True\n",
    "    if use_checkpoint:\n",
    "        try:\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                checkpoint = torch.load(checkpoint_path)\n",
    "                # Verify version before loading\n",
    "                if checkpoint.get('model_version', '') == model_version:\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    print(f\"Loaded best model (version {model_version})\")\n",
    "                else:\n",
    "                    print(\"Best model checkpoint has incompatible version. Using current model state.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading best model: {e}\")\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, data_loader, station_ids, regions):\n",
    "    \"\"\"\n",
    "    Evaluate the model and calculate metrics with robust dimension handling.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            # Move to device - handle ALL possible input types\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision for evaluation\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            # CHANGED: Store predictions directly instead of extracting median predictions\n",
    "            preds = outputs.detach().cpu().numpy()\n",
    "            targets_cpu = targets.numpy()\n",
    "            \n",
    "            # Append batch predictions and targets\n",
    "            all_predictions.append(median_preds)\n",
    "            all_actuals.append(targets_cpu)\n",
    "            \n",
    "            # Clear GPU cache periodically\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Process predictions in chunks to save memory\n",
    "    rmse_sum = 0\n",
    "    mae_sum = 0\n",
    "    r2_sum = 0\n",
    "    sample_count = 0\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    for preds, acts in zip(all_predictions, all_actuals):\n",
    "        # Flatten current batch\n",
    "        preds_flat = preds.flatten()  \n",
    "        acts_flat = acts.flatten()\n",
    "        \n",
    "        # Update metrics\n",
    "        rmse_sum += np.sum((preds_flat - acts_flat) ** 2)\n",
    "        mae_sum += np.sum(np.abs(preds_flat - acts_flat))\n",
    "        sample_count += len(preds_flat)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    rmse = np.sqrt(rmse_sum / sample_count)\n",
    "    mae = mae_sum / sample_count\n",
    "    \n",
    "    # For R², we need all data (this is an approximation)\n",
    "    all_preds_concat = np.concatenate([p.flatten() for p in all_predictions])\n",
    "    all_acts_concat = np.concatenate([a.flatten() for a in all_actuals])\n",
    "    r2 = r2_score(all_acts_concat, all_preds_concat)\n",
    "\n",
    "    print(f\"Overall Metrics - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # Since we only have one station (LA Downtown), simplify the station metrics\n",
    "    station_metrics = {\n",
    "        station_ids[0]: {\n",
    "            'region': regions.get(station_ids[0], 'Unknown'),\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Station {station_ids[0]} ({regions.get(station_ids[0], 'Unknown')}) - \"\n",
    "          f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    return rmse, mae, r2, station_metrics\n",
    "\n",
    "def visualize_predictions(model, data_loader, station_ids, regions, season):\n",
    "    \"\"\"\n",
    "    Visualize predictions for each station.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if len(data_loader) == 0:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions\n",
    "    try:\n",
    "        for inputs, targets in data_loader:\n",
    "            # Only process one batch for visualization\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Move to CPU for plotting\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.numpy()\n",
    "            break\n",
    "\n",
    "        # Check if we have data to plot\n",
    "        if 'outputs' not in locals():\n",
    "            print(\"No data was loaded from the dataloader\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing visualization data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots for each station\n",
    "    fig, axes = plt.subplots(len(station_ids), 1, figsize=(12, 3*len(station_ids)))\n",
    "    if len(station_ids) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    hours = np.arange(24)\n",
    "\n",
    "    for i, station in enumerate(station_ids):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Plot actual vs predicted\n",
    "        ax.plot(hours, targets[0, i, :], 'b-', label='Actual')\n",
    "        ax.plot(hours, outputs[0, i, :], 'r--', label='Predicted')\n",
    "\n",
    "        ax.set_title(f\"{station} ({regions.get(station, 'Unknown')}) - {season}\")\n",
    "        ax.set_xlabel('Hour of Day')\n",
    "        ax.set_ylabel('Temperature (°C)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{season}_predictions.png\")\n",
    "    plt.show()  # Add this line to display the plot\n",
    "    plt.close()\n",
    "\n",
    "def create_mirror_plot(model, data_loader, station_ids, regions, season):\n",
    "    \"\"\"\n",
    "    Create a mirror plot with actual temperature on left side and predicted on right side.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if len(data_loader) == 0:\n",
    "        print(f\"No data available for {season} mirror plot\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions\n",
    "    try:\n",
    "        for inputs, targets in data_loader:\n",
    "            # Move inputs to device - handle ALL possible input types\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Move to CPU for plotting\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.numpy()\n",
    "            break\n",
    "            \n",
    "        if 'outputs' not in locals():\n",
    "            print(f\"No data was loaded for {season} mirror plot\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing mirror plot data for {season}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print full error details\n",
    "        return\n",
    "    \n",
    "    # Create a figure for each station\n",
    "    for i, station in enumerate(station_ids):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "        \n",
    "        hours = np.arange(24)\n",
    "        \n",
    "        # Left plot - Actual temperatures\n",
    "        ax1.plot(hours, targets[0, i, :], 'b-o', linewidth=2)\n",
    "        ax1.set_title(f\"Actual Temperature - {season}\", fontsize=14)\n",
    "        ax1.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax1.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "        ax1.set_xlim(0, 23)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.invert_xaxis()  # Invert x-axis for mirror effect\n",
    "        \n",
    "        # Right plot - Predicted temperatures\n",
    "        ax2.plot(hours, outputs[0, i, :], 'r-o', linewidth=2)\n",
    "        ax2.set_title(f\"Predicted Temperature - {season}\", fontsize=14)\n",
    "        ax2.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax2.set_xlim(0, 23)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add overall title\n",
    "        plt.suptitle(f\"{station} ({regions.get(station, 'Unknown')}) - {season} Day Comparison\", \n",
    "                    fontsize=16, y=1.05)\n",
    "        \n",
    "        # Add a line in the middle\n",
    "        fig.tight_layout()\n",
    "        plt.subplots_adjust(wspace=0.05)  # Reduce space between subplots\n",
    "        \n",
    "        # Save and display\n",
    "        plt.savefig(f\"{season}_mirror_comparison.png\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def create_seasonal_datasets(df, target_days, all_stations, feature_cols):\n",
    "    \"\"\"\n",
    "    Create datasets for each seasonal day and visualize predictions.\n",
    "    \"\"\"\n",
    "    seasonal_datasets = {}\n",
    "    \n",
    "    for season, day_data in target_days.items():\n",
    "        if day_data.empty:\n",
    "            print(f\"No data available for {season}\")\n",
    "            continue\n",
    "            \n",
    "        # Get the date of this target day\n",
    "        sample_date = day_data['timestamp'].iloc[0]\n",
    "        print(f\"Creating dataset for {season}: {sample_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare the data in the format expected by the model\n",
    "            # Format should be: temporal_features [batch, features, stations, time], static_features [batch, stations, static_features]\n",
    "            \n",
    "            # Reshape features to match expected format\n",
    "            temporal_data = np.zeros((1, len(feature_cols), 1, 24))  # [batch=1, features, stations=1, time=24]\n",
    "            \n",
    "            # Fill in the temporal features\n",
    "            for f_idx, feat in enumerate(feature_cols):\n",
    "                if feat in day_data.columns:\n",
    "                    temporal_data[0, f_idx, 0, :] = day_data[feat].values[:24]  # Using first 24 records\n",
    "            \n",
    "            # Create static features (region_code and elevation)\n",
    "            static_data = np.zeros((1, 1, 2))  # [batch=1, stations=1, static_features=2]\n",
    "            static_data[0, 0, 0] = day_data['region_code'].iloc[0] \n",
    "            static_data[0, 0, 1] = day_data['elevation_norm'].iloc[0]\n",
    "            \n",
    "            # Create target (actual temperatures)\n",
    "            target_data = np.zeros((1, 1, 24))  # [batch=1, stations=1, time=24]\n",
    "            target_data[0, 0, :] = day_data['Temperature_C'].values[:24]  # Using first 24 records\n",
    "            \n",
    "            # Convert to tensors\n",
    "            X_temporal = torch.FloatTensor(temporal_data)\n",
    "            X_static = torch.FloatTensor(static_data)\n",
    "            y = torch.FloatTensor(target_data)\n",
    "            \n",
    "            # Store as a list containing a single data point\n",
    "            seasonal_datasets[season] = [((X_temporal, X_static), y)]\n",
    "            print(f\"Created {season} dataset with shapes: X_temporal={X_temporal.shape}, X_static={X_static.shape}, y={y.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating {season} dataset: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return seasonal_datasets\n",
    "\n",
    "\n",
    "def analyze_topographic_performance(station_metrics, regions):\n",
    "    \"\"\"\n",
    "    Analyze model performance across different topographic regions.\n",
    "    \"\"\"\n",
    "    # Group metrics by region\n",
    "    region_metrics = {}\n",
    "    for station, metrics in station_metrics.items():\n",
    "        region = regions.get(station, 'Unknown')\n",
    "        if region not in region_metrics:\n",
    "            region_metrics[region] = []\n",
    "        region_metrics[region].append(metrics)\n",
    "\n",
    "    # Calculate average metrics by region\n",
    "    region_avg_metrics = {}\n",
    "    for region, metrics_list in region_metrics.items():\n",
    "        avg_rmse = np.mean([m['rmse'] for m in metrics_list])\n",
    "        avg_mae = np.mean([m['mae'] for m in metrics_list])\n",
    "        avg_r2 = np.mean([m['r2'] for m in metrics_list])\n",
    "\n",
    "        region_avg_metrics[region] = {\n",
    "            'avg_rmse': avg_rmse,\n",
    "            'avg_mae': avg_mae,\n",
    "            'avg_r2': avg_r2\n",
    "        }\n",
    "\n",
    "        print(f\"Region {region} - Avg RMSE: {avg_rmse:.4f}, Avg MAE: {avg_mae:.4f}, Avg R²: {avg_r2:.4f}\")\n",
    "\n",
    "        if not station_metrics:\n",
    "            print(\"No station metrics available for analysis\")\n",
    "            return {}\n",
    "\n",
    "    # Create bar chart comparing regions\n",
    "    regions = list(region_avg_metrics.keys())\n",
    "    rmse_values = [region_avg_metrics[r]['avg_rmse'] for r in regions]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(regions, rmse_values)\n",
    "\n",
    "    # Add styling\n",
    "    plt.title('RMSE by Topographic Region', fontsize=16)\n",
    "\n",
    "    plt.title('RMSE by Topographic Region', fontsize=16)\n",
    "    plt.ylabel('RMSE (°C)', fontsize=14)\n",
    "    plt.xlabel('Region', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'{height:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig('region_performance.png')\n",
    "    plt.close()\n",
    "\n",
    "    return region_avg_metrics\n",
    "\n",
    "def analyze_seasonal_performance(seasonal_results):\n",
    "    \"\"\"\n",
    "    Compare model performance across different seasons.\n",
    "    \"\"\"\n",
    "    seasons = list(seasonal_results.keys())\n",
    "    rmse_values = [results['rmse'] for results in seasonal_results.values()]\n",
    "    mae_values = [results['mae'] for results in seasonal_results.values()]\n",
    "\n",
    "    if not seasonal_results:\n",
    "        print(\"No seasonal results available for analysis\")\n",
    "        return [], [], []\n",
    "\n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(seasons))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width/2, rmse_values, width, label='RMSE')\n",
    "    ax.bar(x + width/2, mae_values, width, label='MAE')\n",
    "\n",
    "    ax.set_title('Model Performance by Season', fontsize=16)\n",
    "    ax.set_ylabel('Error (°C)', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(seasons)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(rmse_values):\n",
    "        ax.text(i - width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "    for i, v in enumerate(mae_values):\n",
    "        ax.text(i + width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig('seasonal_performance.png')\n",
    "    plt.close()\n",
    "\n",
    "    return seasons, rmse_values, mae_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:19:23.787517Z",
     "iopub.status.busy": "2025-04-15T08:19:23.787198Z",
     "iopub.status.idle": "2025-04-15T08:19:24.850884Z",
     "shell.execute_reply": "2025-04-15T08:19:24.849897Z",
     "shell.execute_reply.started": "2025-04-15T08:19:23.787492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA Downtown Weather Forecasting with VMD + TFT\n",
      "======================================================================\n",
      "Filtered data to LA Downtown only\n",
      "Loaded data with 35040 records\n",
      "BLOCK 1 COMPLETED: Initialization and data loading successful.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 1: INITIALIZATION AND DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "print(\"LA Downtown Weather Forecasting with VMD + TFT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load and prepare data\n",
    "df = load_data(DATA_PATH)\n",
    "print(f\"Loaded data with {len(df)} records\")\n",
    "\n",
    "# All stations - only LA Downtown in this case\n",
    "all_stations = ['LA Downtown']\n",
    "regions = {'LA Downtown': 'urban'}\n",
    "\n",
    "# Define feature columns to use\n",
    "feature_cols = [\n",
    "    'Temperature_C',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyStationPressure',\n",
    "    'hour_sin', 'hour_cos',\n",
    "    'day_sin', 'day_cos'\n",
    "]\n",
    "\n",
    "print(\"BLOCK 1 COMPLETED: Initialization and data loading successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:02:08.988737Z",
     "iopub.status.busy": "2025-04-15T13:02:08.988407Z",
     "iopub.status.idle": "2025-04-15T13:02:09.020953Z",
     "shell.execute_reply": "2025-04-15T13:02:09.020152Z",
     "shell.execute_reply.started": "2025-04-15T13:02:08.988712Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing VMD decomposition...\n",
      "Performing VMD decomposition for feature: Temperature_C\n",
      "  Processing station: LA Downtown\n",
      "Starting decomposition for LA Downtown - Temperature_C\n",
      "Loading cached VMD decomposition for LA Downtown - Temperature_C\n",
      "Found 14 VMD modes for Temperature_C\n",
      "BLOCK 2 COMPLETED: VMD decomposition successful.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 2: VMD DECOMPOSITION\n",
    "# ============================================================\n",
    "\n",
    "# Apply VMD decomposition to the Temperature_C feature\n",
    "print(\"Performing VMD decomposition...\")\n",
    "decomposed_data = parallel_vmd_decomposition(df, feature_cols_to_decompose=['Temperature_C'])\n",
    "\n",
    "# Expand feature columns with decomposed modes\n",
    "expanded_feature_cols = feature_cols.copy()\n",
    "\n",
    "# For LA Downtown, add decomposed temperature modes to feature columns\n",
    "la_downtown_modes = decomposed_data['LA Downtown']['Temperature_C']\n",
    "n_modes = la_downtown_modes.shape[0]  # Number of VMD modes\n",
    "\n",
    "print(f\"Found {n_modes} VMD modes for Temperature_C\")\n",
    "\n",
    "# Create expanded dataset with VMD modes\n",
    "vmd_df = df.copy()\n",
    "\n",
    "# Add VMD modes as new features\n",
    "for i in range(n_modes):\n",
    "    mode_name = f'Temp_Mode_{i+1}'\n",
    "    vmd_df[mode_name] = la_downtown_modes[i, :]\n",
    "    expanded_feature_cols.append(mode_name)\n",
    "\n",
    "print(\"BLOCK 2 COMPLETED: VMD decomposition successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:02:17.348627Z",
     "iopub.status.busy": "2025-04-15T13:02:17.348266Z",
     "iopub.status.idle": "2025-04-15T13:02:17.562474Z",
     "shell.execute_reply": "2025-04-15T13:02:17.561212Z",
     "shell.execute_reply.started": "2025-04-15T13:02:17.348604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 34946 valid windows\n",
      "Fold 1:\n",
      "  Train: 2021-01-02 00:00:00 to 2023-05-26 23:00:00\n",
      "  Validation: 2023-05-28 00:00:00 to 2024-01-02 00:00:00\n",
      "Fold 2:\n",
      "  Train: 2021-01-02 00:00:00 to 2023-08-26 06:00:00\n",
      "  Validation: 2023-08-27 07:00:00 to 2024-04-02 06:00:00\n",
      "Fold 3:\n",
      "  Train: 2021-01-02 00:00:00 to 2023-11-25 12:00:00\n",
      "  Validation: 2023-11-26 13:00:00 to 2024-07-02 12:00:00\n",
      "Fold 4:\n",
      "  Train: 2021-01-02 00:00:00 to 2024-02-24 18:00:00\n",
      "  Validation: 2024-02-25 19:00:00 to 2024-10-01 18:00:00\n",
      "Fold 5:\n",
      "  Train: 2021-01-02 00:00:00 to 2024-05-26 00:00:00\n",
      "  Validation: 2024-05-27 01:00:00 to 2025-01-01 00:00:00\n",
      "Time-based CV split created: 21000 training samples, 5256 validation samples, 5256 test samples\n",
      "BLOCK 3 COMPLETED: Dataset creation and splitting successful.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 3: DATASET CREATION AND SPLITTING\n",
    "# ============================================================\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "# Create dataset with expanded features\n",
    "full_dataset = WeatherDataset(\n",
    "    df=vmd_df,\n",
    "    station_ids=all_stations,\n",
    "    feature_cols=expanded_feature_cols,\n",
    "    seq_length=24,\n",
    "    forecast_horizon=24\n",
    ")\n",
    "\n",
    "# First, create data loaders with default batch size\n",
    "initial_batch_size = 32  # Default value before optimization\n",
    "\n",
    "# Create time series CV splits\n",
    "ts_cv = TimeSeriesCVSplit(\n",
    "    timestamps=full_dataset.timestamps, \n",
    "    n_splits=5,\n",
    "    test_size=0.15, \n",
    "    gap=24,  # 24-hour gap between train and validation\n",
    "    min_train_size=0.6\n",
    ")\n",
    "\n",
    "# Visualize the CV folds\n",
    "fold_info = ts_cv.get_fold_timestamps()\n",
    "for fold in fold_info:\n",
    "    print(f\"Fold {fold['fold']}:\")\n",
    "    print(f\"  Train: {fold['train_range'][0]} to {fold['train_range'][1]}\")\n",
    "    print(f\"  Validation: {fold['val_range'][0]} to {fold['val_range'][1]}\")\n",
    "\n",
    "# Create samplers and loaders for the first fold (for initial training)\n",
    "fold_splits = list(ts_cv.split())\n",
    "train_indices, val_indices = fold_splits[0]\n",
    "\n",
    "# Reserve the last fold for final testing\n",
    "test_indices = fold_splits[-1][1]\n",
    "\n",
    "# Create data loaders with the samplers\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# Create data loaders with the samplers\n",
    "train_dataset = DataLoader(full_dataset, batch_size=initial_batch_size, sampler=train_sampler)\n",
    "val_dataset = DataLoader(full_dataset, batch_size=initial_batch_size, sampler=val_sampler)\n",
    "test_dataset = DataLoader(full_dataset, batch_size=initial_batch_size, sampler=test_sampler)\n",
    "\n",
    "print(f\"Time-based CV split created: {len(train_indices)} training samples, \"\n",
    "      f\"{len(val_indices)} validation samples, {len(test_indices)} test samples\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=initial_batch_size, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=initial_batch_size, \n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(\"BLOCK 3 COMPLETED: Dataset creation and splitting successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:25:15.653642Z",
     "iopub.status.busy": "2025-04-15T08:25:15.653319Z",
     "iopub.status.idle": "2025-04-15T08:47:41.341130Z",
     "shell.execute_reply": "2025-04-15T08:47:41.339745Z",
     "shell.execute_reply.started": "2025-04-15T08:25:15.653616Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK 3.5: Running quick test before optimization...\n",
      "Running quick test to verify checkpoint compatibility...\n",
      "Starting training from scratch.\n",
      "Starting training for 2 epochs with patience 5...\n"
     ]
    }
   ],
   "source": [
    "# Define the quick test function\n",
    "def quick_test_run(train_loader, val_loader, feature_cols, num_stations):\n",
    "    \"\"\"Run a quick test to ensure training works.\"\"\"\n",
    "    print(\"Running quick test to verify checkpoint compatibility...\")\n",
    "    \n",
    "    # Create a small model with fixed parameters\n",
    "    test_model = TemporalFusionTransformer(\n",
    "        num_features=len(feature_cols),\n",
    "        hidden_size=24,\n",
    "        num_heads=1,\n",
    "        dropout=0.1,\n",
    "        forecast_horizon=24,\n",
    "        hidden_layers=2\n",
    "    )\n",
    "    \n",
    "    # Train for only 2 epochs with early stopping disabled\n",
    "    model, train_losses, val_losses = train_model(\n",
    "        model=test_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        learning_rate=0.01,\n",
    "        epochs=2,  # Just 2 epochs for quick testing\n",
    "        patience=5,  # High patience to avoid early stopping\n",
    "        use_checkpoint=False  # Set to False to avoid checkpoint issues\n",
    "    )\n",
    "    \n",
    "    print(\"Test run completed successfully!\")\n",
    "    return True\n",
    "\n",
    "# First run the quick test to verify everything works\n",
    "print(\"BLOCK 3.5: Running quick test before optimization...\")\n",
    "quick_test_run(train_loader, val_loader, expanded_feature_cols, len(all_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:02:33.189875Z",
     "iopub.status.busy": "2025-04-15T13:02:33.189504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ADE optimization for hyperparameters...\n",
      "Removing incompatible checkpoint before optimization.\n",
      "Starting training from scratch.\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 60.4488, Time: 0.069s\n",
      "  Batch 20/765, Loss: 29.6770, Time: 0.084s\n",
      "  Batch 30/765, Loss: 18.9761, Time: 0.071s\n",
      "  Batch 40/765, Loss: 18.9913, Time: 0.086s\n",
      "  Batch 50/765, Loss: 19.8823, Time: 0.072s\n",
      "  Batch 60/765, Loss: 21.7851, Time: 0.078s\n",
      "  Batch 70/765, Loss: 22.8416, Time: 0.071s\n",
      "  Batch 80/765, Loss: 19.0067, Time: 0.072s\n",
      "  Batch 90/765, Loss: 30.1333, Time: 0.054s\n",
      "  Batch 100/765, Loss: 25.2879, Time: 0.068s\n",
      "  Batch 110/765, Loss: 31.4014, Time: 0.068s\n",
      "  Batch 120/765, Loss: 28.7064, Time: 0.072s\n",
      "  Batch 130/765, Loss: 28.7608, Time: 0.069s\n",
      "  Batch 140/765, Loss: 30.9138, Time: 0.044s\n",
      "  Batch 150/765, Loss: 25.6082, Time: 0.068s\n",
      "  Batch 160/765, Loss: 25.1586, Time: 0.071s\n",
      "  Batch 170/765, Loss: 25.9122, Time: 0.071s\n",
      "  Batch 180/765, Loss: 24.8534, Time: 0.071s\n",
      "  Batch 190/765, Loss: 26.1963, Time: 0.070s\n",
      "  Batch 200/765, Loss: 20.2368, Time: 0.070s\n",
      "  Batch 210/765, Loss: 30.5441, Time: 0.067s\n",
      "  Batch 220/765, Loss: 23.5357, Time: 0.071s\n",
      "  Batch 230/765, Loss: 30.4766, Time: 0.069s\n",
      "  Batch 240/765, Loss: 23.4015, Time: 0.089s\n",
      "  Batch 250/765, Loss: 28.4134, Time: 0.070s\n",
      "  Batch 260/765, Loss: 29.8147, Time: 0.067s\n",
      "  Batch 270/765, Loss: 27.0964, Time: 0.068s\n",
      "  Batch 280/765, Loss: 28.3097, Time: 0.069s\n",
      "  Batch 290/765, Loss: 21.0304, Time: 0.069s\n",
      "  Batch 300/765, Loss: 21.9782, Time: 0.071s\n",
      "  Batch 310/765, Loss: 19.0724, Time: 0.070s\n",
      "  Batch 320/765, Loss: 29.7988, Time: 0.068s\n",
      "  Batch 330/765, Loss: 21.6966, Time: 0.086s\n",
      "  Batch 340/765, Loss: 19.2451, Time: 0.067s\n",
      "  Batch 350/765, Loss: 30.1951, Time: 0.068s\n",
      "  Batch 360/765, Loss: 24.4531, Time: 0.067s\n",
      "  Batch 370/765, Loss: 22.3554, Time: 0.071s\n",
      "  Batch 380/765, Loss: 25.5658, Time: 0.068s\n",
      "  Batch 390/765, Loss: 27.6206, Time: 0.068s\n",
      "  Batch 400/765, Loss: 25.6305, Time: 0.068s\n",
      "  Batch 410/765, Loss: 20.8522, Time: 0.048s\n",
      "  Batch 420/765, Loss: 13.9580, Time: 0.069s\n",
      "  Batch 430/765, Loss: 26.4452, Time: 0.071s\n",
      "  Batch 440/765, Loss: 18.1891, Time: 0.069s\n",
      "  Batch 450/765, Loss: 12.7521, Time: 0.068s\n",
      "  Batch 460/765, Loss: 19.3154, Time: 0.070s\n",
      "  Batch 470/765, Loss: 12.6784, Time: 0.070s\n",
      "  Batch 480/765, Loss: 14.3989, Time: 0.071s\n",
      "  Batch 490/765, Loss: 18.7804, Time: 0.071s\n",
      "  Batch 500/765, Loss: 13.1153, Time: 0.070s\n",
      "  Batch 510/765, Loss: 9.9207, Time: 0.068s\n",
      "  Batch 520/765, Loss: 18.4215, Time: 0.070s\n",
      "  Batch 530/765, Loss: 17.5239, Time: 0.070s\n",
      "  Batch 540/765, Loss: 21.6746, Time: 0.092s\n",
      "  Batch 550/765, Loss: 13.1417, Time: 0.071s\n",
      "  Batch 560/765, Loss: 12.4777, Time: 0.084s\n",
      "  Batch 570/765, Loss: 14.0618, Time: 0.069s\n",
      "  Batch 580/765, Loss: 14.6083, Time: 0.071s\n",
      "  Batch 590/765, Loss: 12.3563, Time: 0.070s\n",
      "  Batch 600/765, Loss: 12.7236, Time: 0.071s\n",
      "  Batch 610/765, Loss: 9.9828, Time: 0.073s\n",
      "  Batch 620/765, Loss: 11.5078, Time: 0.074s\n",
      "  Batch 630/765, Loss: 14.9977, Time: 0.047s\n",
      "  Batch 640/765, Loss: 12.0098, Time: 0.069s\n",
      "  Batch 650/765, Loss: 11.5294, Time: 0.071s\n",
      "  Batch 660/765, Loss: 11.7418, Time: 0.073s\n",
      "  Batch 670/765, Loss: 11.3438, Time: 0.072s\n",
      "  Batch 680/765, Loss: 9.6422, Time: 0.080s\n",
      "  Batch 690/765, Loss: 13.0550, Time: 0.075s\n",
      "  Batch 700/765, Loss: 11.8902, Time: 0.143s\n",
      "  Batch 710/765, Loss: 15.6963, Time: 0.090s\n",
      "  Batch 720/765, Loss: 11.9993, Time: 0.072s\n",
      "  Batch 730/765, Loss: 10.3015, Time: 0.072s\n",
      "  Batch 740/765, Loss: 12.6119, Time: 0.071s\n",
      "  Batch 750/765, Loss: 8.8654, Time: 0.070s\n",
      "  Batch 760/765, Loss: 12.4726, Time: 0.071s\n",
      "Epoch 1/5\n",
      "  Train Loss: 21.2453 (765 batches, avg batch time: 0.073s)\n",
      "  Val Loss: 10.7155 (validation time: 75.89s)\n",
      "  Epoch Time: 657.45s, Est. Remaining: 43.83 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 10.7155\n",
      "  Batch 10/765, Loss: 12.1798, Time: 0.069s\n",
      "  Batch 20/765, Loss: 8.8881, Time: 0.044s\n",
      "  Batch 30/765, Loss: 10.8776, Time: 0.044s\n",
      "  Batch 40/765, Loss: 12.0932, Time: 0.071s\n",
      "  Batch 50/765, Loss: 11.0040, Time: 0.070s\n",
      "  Batch 60/765, Loss: 11.1475, Time: 0.071s\n",
      "  Batch 70/765, Loss: 8.5769, Time: 0.071s\n",
      "  Batch 80/765, Loss: 9.7896, Time: 0.069s\n",
      "  Batch 90/765, Loss: 12.4687, Time: 0.069s\n",
      "  Batch 100/765, Loss: 9.4855, Time: 0.159s\n",
      "  Batch 110/765, Loss: 15.2918, Time: 0.070s\n",
      "  Batch 120/765, Loss: 12.3475, Time: 0.070s\n",
      "  Batch 130/765, Loss: 10.6862, Time: 0.070s\n",
      "  Batch 140/765, Loss: 10.2746, Time: 0.072s\n",
      "  Batch 150/765, Loss: 12.8270, Time: 0.073s\n",
      "  Batch 160/765, Loss: 14.3445, Time: 0.068s\n",
      "  Batch 170/765, Loss: 13.3170, Time: 0.089s\n",
      "  Batch 180/765, Loss: 12.3551, Time: 0.070s\n",
      "  Batch 190/765, Loss: 9.8238, Time: 0.071s\n",
      "  Batch 200/765, Loss: 13.3285, Time: 0.071s\n",
      "  Batch 210/765, Loss: 12.1592, Time: 0.092s\n",
      "  Batch 220/765, Loss: 11.7179, Time: 0.077s\n",
      "  Batch 230/765, Loss: 12.4563, Time: 0.200s\n",
      "  Batch 240/765, Loss: 10.4855, Time: 0.071s\n",
      "  Batch 250/765, Loss: 12.5606, Time: 0.046s\n",
      "  Batch 260/765, Loss: 14.4668, Time: 0.070s\n",
      "  Batch 270/765, Loss: 11.0803, Time: 0.070s\n",
      "  Batch 280/765, Loss: 10.5269, Time: 0.069s\n",
      "  Batch 290/765, Loss: 12.6557, Time: 0.069s\n",
      "  Batch 300/765, Loss: 12.7374, Time: 0.070s\n",
      "  Batch 310/765, Loss: 10.4440, Time: 0.069s\n",
      "  Batch 320/765, Loss: 11.1202, Time: 0.070s\n",
      "  Batch 330/765, Loss: 11.0648, Time: 0.070s\n",
      "  Batch 340/765, Loss: 9.8892, Time: 0.049s\n",
      "  Batch 350/765, Loss: 12.8045, Time: 0.069s\n",
      "  Batch 360/765, Loss: 7.8830, Time: 0.071s\n",
      "  Batch 370/765, Loss: 10.9666, Time: 0.068s\n",
      "  Batch 380/765, Loss: 11.8933, Time: 0.071s\n",
      "  Batch 390/765, Loss: 9.4051, Time: 0.069s\n",
      "  Batch 400/765, Loss: 8.3322, Time: 0.070s\n",
      "  Batch 410/765, Loss: 14.7169, Time: 0.072s\n",
      "  Batch 420/765, Loss: 11.5249, Time: 0.069s\n",
      "  Batch 430/765, Loss: 15.2621, Time: 0.071s\n",
      "  Batch 440/765, Loss: 10.9798, Time: 0.070s\n",
      "  Batch 450/765, Loss: 10.5810, Time: 0.112s\n",
      "  Batch 460/765, Loss: 11.5740, Time: 0.069s\n",
      "  Batch 470/765, Loss: 12.3591, Time: 0.063s\n",
      "  Batch 480/765, Loss: 10.1535, Time: 0.073s\n",
      "  Batch 490/765, Loss: 11.7040, Time: 0.045s\n",
      "  Batch 500/765, Loss: 10.9326, Time: 0.045s\n",
      "  Batch 510/765, Loss: 10.3549, Time: 0.073s\n",
      "  Batch 520/765, Loss: 13.7479, Time: 0.069s\n",
      "  Batch 530/765, Loss: 10.1048, Time: 0.051s\n",
      "  Batch 540/765, Loss: 9.8892, Time: 0.068s\n",
      "  Batch 550/765, Loss: 11.4753, Time: 0.067s\n",
      "  Batch 560/765, Loss: 11.7503, Time: 0.069s\n",
      "  Batch 570/765, Loss: 10.3682, Time: 0.068s\n",
      "  Batch 580/765, Loss: 10.2084, Time: 0.070s\n",
      "  Batch 590/765, Loss: 9.0836, Time: 0.071s\n",
      "  Batch 600/765, Loss: 9.1695, Time: 0.070s\n",
      "  Batch 610/765, Loss: 10.9371, Time: 0.073s\n",
      "  Batch 620/765, Loss: 9.0700, Time: 0.046s\n",
      "  Batch 630/765, Loss: 10.7069, Time: 0.069s\n",
      "  Batch 640/765, Loss: 9.1222, Time: 0.069s\n",
      "  Batch 650/765, Loss: 11.6702, Time: 0.069s\n",
      "  Batch 660/765, Loss: 8.4352, Time: 0.049s\n",
      "  Batch 670/765, Loss: 10.4700, Time: 0.075s\n",
      "  Batch 680/765, Loss: 11.5852, Time: 0.080s\n",
      "  Batch 690/765, Loss: 7.9429, Time: 0.072s\n",
      "  Batch 700/765, Loss: 9.7683, Time: 0.225s\n",
      "  Batch 710/765, Loss: 8.3687, Time: 0.072s\n",
      "  Batch 720/765, Loss: 12.2186, Time: 0.046s\n",
      "  Batch 730/765, Loss: 11.7270, Time: 0.078s\n",
      "  Batch 740/765, Loss: 7.5706, Time: 0.073s\n",
      "  Batch 750/765, Loss: 10.4572, Time: 0.073s\n",
      "  Batch 760/765, Loss: 11.1078, Time: 0.073s\n",
      "Epoch 2/5\n",
      "  Train Loss: 10.7902 (765 batches, avg batch time: 0.073s)\n",
      "  Val Loss: 10.4972 (validation time: 75.22s)\n",
      "  Epoch Time: 651.37s, Est. Remaining: 32.72 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 10.4972\n",
      "  Batch 10/765, Loss: 11.6743, Time: 0.050s\n",
      "  Batch 20/765, Loss: 11.5366, Time: 0.081s\n",
      "  Batch 30/765, Loss: 9.3940, Time: 0.118s\n",
      "  Batch 40/765, Loss: 9.1982, Time: 0.074s\n",
      "  Batch 50/765, Loss: 9.3758, Time: 0.105s\n",
      "  Batch 60/765, Loss: 10.1516, Time: 0.074s\n",
      "  Batch 70/765, Loss: 10.1627, Time: 0.075s\n",
      "  Batch 80/765, Loss: 12.2645, Time: 0.075s\n",
      "  Batch 90/765, Loss: 10.8575, Time: 0.052s\n",
      "  Batch 100/765, Loss: 10.5034, Time: 0.074s\n",
      "  Batch 110/765, Loss: 8.1230, Time: 0.075s\n",
      "  Batch 120/765, Loss: 8.9230, Time: 0.052s\n",
      "  Batch 130/765, Loss: 11.3410, Time: 0.078s\n",
      "  Batch 140/765, Loss: 7.1985, Time: 0.076s\n",
      "  Batch 150/765, Loss: 9.7610, Time: 0.051s\n",
      "  Batch 160/765, Loss: 8.9487, Time: 0.077s\n",
      "  Batch 170/765, Loss: 8.5953, Time: 0.074s\n",
      "  Batch 180/765, Loss: 10.8941, Time: 0.050s\n",
      "  Batch 190/765, Loss: 9.7961, Time: 0.075s\n",
      "  Batch 200/765, Loss: 10.3948, Time: 0.072s\n",
      "  Batch 210/765, Loss: 8.8071, Time: 0.073s\n",
      "  Batch 220/765, Loss: 11.4575, Time: 0.125s\n",
      "  Batch 230/765, Loss: 10.0329, Time: 0.075s\n",
      "  Batch 240/765, Loss: 7.9754, Time: 0.076s\n",
      "  Batch 250/765, Loss: 9.6880, Time: 0.078s\n",
      "  Batch 260/765, Loss: 8.4553, Time: 0.074s\n",
      "  Batch 270/765, Loss: 7.6669, Time: 0.072s\n",
      "  Batch 280/765, Loss: 10.0569, Time: 0.076s\n",
      "  Batch 290/765, Loss: 7.4454, Time: 0.074s\n",
      "  Batch 300/765, Loss: 9.3566, Time: 0.074s\n",
      "  Batch 310/765, Loss: 10.8888, Time: 0.084s\n",
      "  Batch 320/765, Loss: 10.9202, Time: 0.049s\n",
      "  Batch 330/765, Loss: 9.1495, Time: 0.076s\n",
      "  Batch 340/765, Loss: 9.2258, Time: 0.080s\n",
      "  Batch 350/765, Loss: 11.5922, Time: 0.070s\n",
      "  Batch 360/765, Loss: 9.5944, Time: 0.047s\n",
      "  Batch 370/765, Loss: 7.8832, Time: 0.072s\n",
      "  Batch 380/765, Loss: 10.7281, Time: 0.071s\n",
      "  Batch 390/765, Loss: 10.1006, Time: 0.071s\n",
      "  Batch 400/765, Loss: 11.0236, Time: 0.072s\n",
      "  Batch 410/765, Loss: 11.1802, Time: 0.073s\n",
      "  Batch 420/765, Loss: 8.9834, Time: 0.071s\n",
      "  Batch 430/765, Loss: 8.6553, Time: 0.072s\n",
      "  Batch 440/765, Loss: 9.6766, Time: 0.071s\n",
      "  Batch 450/765, Loss: 11.1716, Time: 0.046s\n",
      "  Batch 460/765, Loss: 8.8064, Time: 0.076s\n",
      "  Batch 470/765, Loss: 10.6599, Time: 0.073s\n",
      "  Batch 480/765, Loss: 11.1431, Time: 0.072s\n",
      "  Batch 490/765, Loss: 11.3338, Time: 0.071s\n",
      "  Batch 500/765, Loss: 8.2404, Time: 0.071s\n",
      "  Batch 510/765, Loss: 11.6332, Time: 0.075s\n",
      "  Batch 520/765, Loss: 10.7276, Time: 0.074s\n",
      "  Batch 530/765, Loss: 9.6555, Time: 0.071s\n",
      "  Batch 540/765, Loss: 7.9665, Time: 0.077s\n",
      "  Batch 550/765, Loss: 9.6371, Time: 0.075s\n",
      "  Batch 560/765, Loss: 11.0955, Time: 0.086s\n",
      "  Batch 570/765, Loss: 7.1803, Time: 0.071s\n",
      "  Batch 580/765, Loss: 11.7022, Time: 0.087s\n",
      "  Batch 590/765, Loss: 11.3998, Time: 0.071s\n",
      "  Batch 600/765, Loss: 8.6768, Time: 0.222s\n",
      "  Batch 610/765, Loss: 9.6417, Time: 0.047s\n",
      "  Batch 620/765, Loss: 12.0752, Time: 0.092s\n",
      "  Batch 630/765, Loss: 9.0346, Time: 0.071s\n",
      "  Batch 640/765, Loss: 9.7008, Time: 0.071s\n",
      "  Batch 650/765, Loss: 9.8643, Time: 0.070s\n",
      "  Batch 660/765, Loss: 8.9796, Time: 0.072s\n",
      "  Batch 670/765, Loss: 10.0112, Time: 0.075s\n",
      "  Batch 680/765, Loss: 6.7119, Time: 0.047s\n",
      "  Batch 690/765, Loss: 8.7752, Time: 0.071s\n",
      "  Batch 700/765, Loss: 10.3938, Time: 0.071s\n",
      "  Batch 710/765, Loss: 11.1193, Time: 0.071s\n",
      "  Batch 720/765, Loss: 10.5008, Time: 0.072s\n",
      "  Batch 730/765, Loss: 10.7540, Time: 0.075s\n",
      "  Batch 740/765, Loss: 11.0747, Time: 0.073s\n",
      "  Batch 750/765, Loss: 9.9880, Time: 0.072s\n",
      "  Batch 760/765, Loss: 8.2510, Time: 0.070s\n",
      "Epoch 3/5\n",
      "  Train Loss: 10.0695 (765 batches, avg batch time: 0.077s)\n",
      "  Val Loss: 9.3564 (validation time: 76.30s)\n",
      "  Epoch Time: 661.25s, Est. Remaining: 21.89 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.3564\n",
      "  Batch 10/765, Loss: 6.9873, Time: 0.071s\n",
      "  Batch 20/765, Loss: 7.6070, Time: 0.070s\n",
      "  Batch 30/765, Loss: 13.3414, Time: 0.084s\n",
      "  Batch 40/765, Loss: 10.6351, Time: 0.072s\n",
      "  Batch 50/765, Loss: 8.4407, Time: 0.071s\n",
      "  Batch 60/765, Loss: 9.5212, Time: 0.070s\n",
      "  Batch 70/765, Loss: 9.2060, Time: 0.070s\n",
      "  Batch 80/765, Loss: 8.7516, Time: 0.070s\n",
      "  Batch 90/765, Loss: 9.2270, Time: 0.069s\n",
      "  Batch 100/765, Loss: 9.9854, Time: 0.070s\n",
      "  Batch 110/765, Loss: 9.2756, Time: 0.075s\n",
      "  Batch 120/765, Loss: 8.7726, Time: 0.096s\n",
      "  Batch 130/765, Loss: 10.6510, Time: 0.072s\n",
      "  Batch 140/765, Loss: 8.4536, Time: 0.072s\n",
      "  Batch 150/765, Loss: 10.1789, Time: 0.073s\n",
      "  Batch 160/765, Loss: 11.5696, Time: 0.130s\n",
      "  Batch 170/765, Loss: 7.6082, Time: 0.050s\n",
      "  Batch 180/765, Loss: 10.0456, Time: 0.070s\n",
      "  Batch 190/765, Loss: 8.2441, Time: 0.075s\n",
      "  Batch 200/765, Loss: 12.8203, Time: 0.104s\n",
      "  Batch 210/765, Loss: 10.3190, Time: 0.071s\n",
      "  Batch 220/765, Loss: 10.5711, Time: 0.073s\n",
      "  Batch 230/765, Loss: 11.6651, Time: 0.071s\n",
      "  Batch 240/765, Loss: 9.4822, Time: 0.163s\n",
      "  Batch 250/765, Loss: 6.8615, Time: 0.079s\n",
      "  Batch 260/765, Loss: 8.2160, Time: 0.070s\n",
      "  Batch 270/765, Loss: 7.4764, Time: 0.071s\n",
      "  Batch 280/765, Loss: 11.7747, Time: 0.071s\n",
      "  Batch 290/765, Loss: 9.0915, Time: 0.070s\n",
      "  Batch 300/765, Loss: 8.4153, Time: 0.070s\n",
      "  Batch 310/765, Loss: 10.8601, Time: 0.070s\n",
      "  Batch 320/765, Loss: 9.7982, Time: 0.069s\n",
      "  Batch 330/765, Loss: 8.0465, Time: 0.070s\n",
      "  Batch 340/765, Loss: 6.5432, Time: 0.070s\n",
      "  Batch 350/765, Loss: 8.8261, Time: 0.070s\n",
      "  Batch 360/765, Loss: 11.5908, Time: 0.072s\n",
      "  Batch 370/765, Loss: 13.2076, Time: 0.087s\n",
      "  Batch 380/765, Loss: 11.3408, Time: 0.070s\n",
      "  Batch 390/765, Loss: 12.1073, Time: 0.069s\n",
      "  Batch 400/765, Loss: 8.6036, Time: 0.071s\n",
      "  Batch 410/765, Loss: 9.1459, Time: 0.070s\n",
      "  Batch 420/765, Loss: 7.8514, Time: 0.087s\n",
      "  Batch 430/765, Loss: 8.9273, Time: 0.071s\n",
      "  Batch 440/765, Loss: 10.5147, Time: 0.093s\n",
      "  Batch 450/765, Loss: 9.8191, Time: 0.069s\n",
      "  Batch 460/765, Loss: 9.8031, Time: 0.057s\n",
      "  Batch 470/765, Loss: 9.7866, Time: 0.070s\n",
      "  Batch 480/765, Loss: 9.6730, Time: 0.069s\n",
      "  Batch 490/765, Loss: 12.9493, Time: 0.073s\n",
      "  Batch 500/765, Loss: 10.7546, Time: 0.077s\n",
      "  Batch 510/765, Loss: 11.5135, Time: 0.069s\n",
      "  Batch 520/765, Loss: 6.8875, Time: 0.081s\n",
      "  Batch 530/765, Loss: 8.7796, Time: 0.072s\n",
      "  Batch 540/765, Loss: 9.6346, Time: 0.124s\n",
      "  Batch 550/765, Loss: 9.7809, Time: 0.069s\n",
      "  Batch 560/765, Loss: 8.1661, Time: 0.070s\n",
      "  Batch 570/765, Loss: 8.7547, Time: 0.080s\n",
      "  Batch 580/765, Loss: 10.1201, Time: 0.072s\n",
      "  Batch 590/765, Loss: 8.2472, Time: 0.069s\n",
      "  Batch 600/765, Loss: 8.3053, Time: 0.070s\n",
      "  Batch 610/765, Loss: 12.2918, Time: 0.072s\n",
      "  Batch 620/765, Loss: 10.9859, Time: 0.072s\n",
      "  Batch 630/765, Loss: 10.7304, Time: 0.070s\n",
      "  Batch 640/765, Loss: 8.1775, Time: 0.079s\n",
      "  Batch 650/765, Loss: 8.5558, Time: 0.069s\n",
      "  Batch 660/765, Loss: 9.3174, Time: 0.070s\n",
      "  Batch 670/765, Loss: 11.2211, Time: 0.070s\n",
      "  Batch 680/765, Loss: 11.0414, Time: 0.072s\n",
      "  Batch 690/765, Loss: 10.1308, Time: 0.071s\n",
      "  Batch 700/765, Loss: 9.2808, Time: 0.048s\n",
      "  Batch 710/765, Loss: 9.0557, Time: 0.106s\n",
      "  Batch 720/765, Loss: 10.2089, Time: 0.072s\n",
      "  Batch 730/765, Loss: 9.3465, Time: 0.068s\n",
      "  Batch 740/765, Loss: 9.2294, Time: 0.070s\n",
      "  Batch 750/765, Loss: 8.1258, Time: 0.071s\n",
      "  Batch 760/765, Loss: 10.5444, Time: 0.072s\n",
      "Epoch 4/5\n",
      "  Train Loss: 9.5844 (765 batches, avg batch time: 0.075s)\n",
      "  Val Loss: 9.1731 (validation time: 76.81s)\n",
      "  Epoch Time: 658.24s, Est. Remaining: 10.95 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.1731\n",
      "  Batch 10/765, Loss: 7.7009, Time: 0.072s\n",
      "  Batch 20/765, Loss: 9.7043, Time: 0.073s\n",
      "  Batch 30/765, Loss: 9.2675, Time: 0.073s\n",
      "  Batch 40/765, Loss: 12.2135, Time: 0.070s\n",
      "  Batch 50/765, Loss: 11.3325, Time: 0.069s\n",
      "  Batch 60/765, Loss: 9.2753, Time: 0.069s\n",
      "  Batch 70/765, Loss: 8.7382, Time: 0.070s\n",
      "  Batch 80/765, Loss: 9.7211, Time: 0.070s\n",
      "  Batch 90/765, Loss: 10.5278, Time: 0.073s\n",
      "  Batch 100/765, Loss: 9.2177, Time: 0.070s\n",
      "  Batch 110/765, Loss: 9.9712, Time: 0.080s\n",
      "  Batch 120/765, Loss: 10.7498, Time: 0.074s\n",
      "  Batch 130/765, Loss: 11.1582, Time: 0.074s\n",
      "  Batch 140/765, Loss: 9.9067, Time: 0.091s\n",
      "  Batch 150/765, Loss: 9.8082, Time: 0.071s\n",
      "  Batch 160/765, Loss: 7.7951, Time: 0.078s\n",
      "  Batch 170/765, Loss: 10.6078, Time: 0.070s\n",
      "  Batch 180/765, Loss: 9.3386, Time: 0.068s\n",
      "  Batch 190/765, Loss: 12.2657, Time: 0.069s\n",
      "  Batch 200/765, Loss: 11.4274, Time: 0.070s\n",
      "  Batch 210/765, Loss: 7.2031, Time: 0.069s\n",
      "  Batch 220/765, Loss: 9.3813, Time: 0.070s\n",
      "  Batch 230/765, Loss: 8.9290, Time: 0.069s\n",
      "  Batch 240/765, Loss: 9.5546, Time: 0.069s\n",
      "  Batch 250/765, Loss: 10.4301, Time: 0.070s\n",
      "  Batch 260/765, Loss: 9.9420, Time: 0.070s\n",
      "  Batch 270/765, Loss: 6.7892, Time: 0.082s\n",
      "  Batch 280/765, Loss: 9.2017, Time: 0.069s\n",
      "  Batch 290/765, Loss: 8.6691, Time: 0.078s\n",
      "  Batch 300/765, Loss: 10.2559, Time: 0.071s\n",
      "  Batch 310/765, Loss: 8.1168, Time: 0.071s\n",
      "  Batch 320/765, Loss: 9.3018, Time: 0.072s\n",
      "  Batch 330/765, Loss: 9.9589, Time: 0.070s\n",
      "  Batch 340/765, Loss: 7.8931, Time: 0.071s\n",
      "  Batch 350/765, Loss: 10.0640, Time: 0.073s\n",
      "  Batch 360/765, Loss: 10.4461, Time: 0.069s\n",
      "  Batch 370/765, Loss: 9.7706, Time: 0.069s\n",
      "  Batch 380/765, Loss: 6.3515, Time: 0.071s\n",
      "  Batch 390/765, Loss: 8.7222, Time: 0.045s\n",
      "  Batch 400/765, Loss: 10.7580, Time: 0.069s\n",
      "  Batch 410/765, Loss: 10.1771, Time: 0.070s\n",
      "  Batch 420/765, Loss: 10.4611, Time: 0.069s\n",
      "  Batch 430/765, Loss: 9.9875, Time: 0.077s\n",
      "  Batch 440/765, Loss: 6.9981, Time: 0.203s\n",
      "  Batch 450/765, Loss: 9.9834, Time: 0.068s\n",
      "  Batch 460/765, Loss: 9.7457, Time: 0.071s\n",
      "  Batch 470/765, Loss: 8.1205, Time: 0.071s\n",
      "  Batch 480/765, Loss: 8.9498, Time: 0.072s\n",
      "  Batch 490/765, Loss: 10.0042, Time: 0.073s\n",
      "  Batch 500/765, Loss: 9.2050, Time: 0.070s\n",
      "  Batch 510/765, Loss: 10.5037, Time: 0.069s\n",
      "  Batch 520/765, Loss: 11.2742, Time: 0.069s\n",
      "  Batch 530/765, Loss: 11.9246, Time: 0.070s\n",
      "  Batch 540/765, Loss: 9.1862, Time: 0.069s\n",
      "  Batch 550/765, Loss: 10.2104, Time: 0.072s\n",
      "  Batch 560/765, Loss: 9.1224, Time: 0.071s\n",
      "  Batch 570/765, Loss: 12.8888, Time: 0.081s\n",
      "  Batch 580/765, Loss: 12.4687, Time: 0.072s\n",
      "  Batch 590/765, Loss: 10.6584, Time: 0.071s\n",
      "  Batch 600/765, Loss: 7.7310, Time: 0.068s\n",
      "  Batch 610/765, Loss: 9.6319, Time: 0.069s\n",
      "  Batch 620/765, Loss: 8.0956, Time: 0.069s\n",
      "  Batch 630/765, Loss: 9.2739, Time: 0.046s\n",
      "  Batch 640/765, Loss: 6.9123, Time: 0.069s\n",
      "  Batch 650/765, Loss: 11.2991, Time: 0.087s\n",
      "  Batch 660/765, Loss: 6.7525, Time: 0.074s\n",
      "  Batch 670/765, Loss: 10.5507, Time: 0.072s\n",
      "  Batch 680/765, Loss: 10.3142, Time: 0.069s\n",
      "  Batch 690/765, Loss: 10.7291, Time: 0.068s\n",
      "  Batch 700/765, Loss: 7.7689, Time: 0.094s\n",
      "  Batch 710/765, Loss: 10.7281, Time: 0.049s\n",
      "  Batch 720/765, Loss: 8.8179, Time: 0.069s\n",
      "  Batch 730/765, Loss: 10.2516, Time: 0.104s\n",
      "  Batch 740/765, Loss: 8.4660, Time: 0.072s\n",
      "  Batch 750/765, Loss: 10.3492, Time: 0.072s\n",
      "  Batch 760/765, Loss: 9.1804, Time: 0.071s\n",
      "Epoch 5/5\n",
      "  Train Loss: 9.3915 (765 batches, avg batch time: 0.074s)\n",
      "  Val Loss: 9.4533 (validation time: 76.95s)\n",
      "  Epoch Time: 656.34s, Est. Remaining: 0.00 minutes\n",
      "  No improvement for 1/2 epochs\n",
      "Training completed in 54.75 minutes (3284.8 seconds)\n",
      "Loaded best model (version single_output_v1)\n",
      "Params: [2.09894729e+01 4.42327400e+01 3.24787652e-02 1.73104562e+01\n",
      " 2.23418722e+00], Validation MAPE: 14.3235%\n",
      "Resuming training from epoch 4\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 11.2939, Time: 0.072s\n",
      "  Batch 20/765, Loss: 11.6169, Time: 0.072s\n",
      "  Batch 30/765, Loss: 11.2774, Time: 0.071s\n",
      "  Batch 40/765, Loss: 9.9644, Time: 0.075s\n",
      "  Batch 50/765, Loss: 8.5508, Time: 0.083s\n",
      "  Batch 60/765, Loss: 11.8884, Time: 0.078s\n",
      "  Batch 70/765, Loss: 7.2328, Time: 0.073s\n",
      "  Batch 80/765, Loss: 9.9160, Time: 0.074s\n",
      "  Batch 90/765, Loss: 7.3848, Time: 0.047s\n",
      "  Batch 100/765, Loss: 8.2954, Time: 0.071s\n",
      "  Batch 110/765, Loss: 6.1651, Time: 0.104s\n",
      "  Batch 120/765, Loss: 12.8563, Time: 0.085s\n",
      "  Batch 130/765, Loss: 9.9097, Time: 0.076s\n",
      "  Batch 140/765, Loss: 6.7204, Time: 0.071s\n",
      "  Batch 150/765, Loss: 10.7533, Time: 0.048s\n",
      "  Batch 160/765, Loss: 8.0757, Time: 0.074s\n",
      "  Batch 170/765, Loss: 6.9149, Time: 0.072s\n",
      "  Batch 180/765, Loss: 12.0520, Time: 0.072s\n",
      "  Batch 190/765, Loss: 7.7016, Time: 0.073s\n",
      "  Batch 200/765, Loss: 7.7780, Time: 0.072s\n",
      "  Batch 210/765, Loss: 9.0754, Time: 0.078s\n",
      "  Batch 220/765, Loss: 9.5153, Time: 0.074s\n",
      "  Batch 230/765, Loss: 9.0610, Time: 0.179s\n",
      "  Batch 240/765, Loss: 11.5099, Time: 0.077s\n",
      "  Batch 250/765, Loss: 10.0610, Time: 0.070s\n",
      "  Batch 260/765, Loss: 10.1740, Time: 0.074s\n",
      "  Batch 270/765, Loss: 9.9317, Time: 0.070s\n",
      "  Batch 280/765, Loss: 9.6133, Time: 0.072s\n",
      "  Batch 290/765, Loss: 9.1140, Time: 0.073s\n",
      "  Batch 300/765, Loss: 8.5319, Time: 0.072s\n",
      "  Batch 310/765, Loss: 7.4152, Time: 0.075s\n",
      "  Batch 320/765, Loss: 8.0760, Time: 0.069s\n",
      "  Batch 330/765, Loss: 8.9879, Time: 0.047s\n",
      "  Batch 340/765, Loss: 11.6836, Time: 0.072s\n",
      "  Batch 350/765, Loss: 8.6498, Time: 0.079s\n",
      "  Batch 360/765, Loss: 7.1785, Time: 0.071s\n",
      "  Batch 370/765, Loss: 6.7745, Time: 0.071s\n",
      "  Batch 380/765, Loss: 8.7625, Time: 0.076s\n",
      "  Batch 390/765, Loss: 8.4021, Time: 0.070s\n",
      "  Batch 400/765, Loss: 9.2404, Time: 0.073s\n",
      "  Batch 410/765, Loss: 12.0833, Time: 0.076s\n",
      "  Batch 420/765, Loss: 8.1791, Time: 0.070s\n",
      "  Batch 430/765, Loss: 7.2325, Time: 0.071s\n",
      "  Batch 440/765, Loss: 9.8601, Time: 0.131s\n",
      "  Batch 450/765, Loss: 7.0640, Time: 0.075s\n",
      "  Batch 460/765, Loss: 11.7111, Time: 0.078s\n",
      "  Batch 470/765, Loss: 9.0741, Time: 0.071s\n",
      "  Batch 480/765, Loss: 11.4202, Time: 0.072s\n",
      "  Batch 490/765, Loss: 7.2144, Time: 0.072s\n",
      "  Batch 500/765, Loss: 7.0669, Time: 0.071s\n",
      "  Batch 510/765, Loss: 10.1913, Time: 0.071s\n",
      "  Batch 520/765, Loss: 10.7821, Time: 0.073s\n",
      "  Batch 530/765, Loss: 9.1376, Time: 0.069s\n",
      "  Batch 540/765, Loss: 9.2182, Time: 0.072s\n",
      "  Batch 550/765, Loss: 9.0404, Time: 0.071s\n",
      "  Batch 560/765, Loss: 7.6404, Time: 0.071s\n",
      "  Batch 570/765, Loss: 8.2506, Time: 0.071s\n",
      "  Batch 580/765, Loss: 10.4395, Time: 0.071s\n",
      "  Batch 590/765, Loss: 9.2275, Time: 0.071s\n",
      "  Batch 600/765, Loss: 9.5616, Time: 0.070s\n",
      "  Batch 610/765, Loss: 6.7583, Time: 0.049s\n",
      "  Batch 620/765, Loss: 7.4016, Time: 0.070s\n",
      "  Batch 630/765, Loss: 9.4364, Time: 0.073s\n",
      "  Batch 640/765, Loss: 10.7356, Time: 0.071s\n",
      "  Batch 650/765, Loss: 12.2716, Time: 0.081s\n",
      "  Batch 660/765, Loss: 9.5894, Time: 0.070s\n",
      "  Batch 670/765, Loss: 8.9833, Time: 0.074s\n",
      "  Batch 680/765, Loss: 9.3328, Time: 0.069s\n",
      "  Batch 690/765, Loss: 8.7482, Time: 0.072s\n",
      "  Batch 700/765, Loss: 7.9867, Time: 0.072s\n",
      "  Batch 710/765, Loss: 10.3816, Time: 0.046s\n",
      "  Batch 720/765, Loss: 6.0373, Time: 0.069s\n",
      "  Batch 730/765, Loss: 10.3913, Time: 0.073s\n",
      "  Batch 740/765, Loss: 7.8628, Time: 0.069s\n",
      "  Batch 750/765, Loss: 10.8852, Time: 0.069s\n",
      "  Batch 760/765, Loss: 14.6462, Time: 0.070s\n",
      "Epoch 5/5\n",
      "  Train Loss: 9.4228 (765 batches, avg batch time: 0.075s)\n",
      "  Val Loss: 9.2235 (validation time: 75.74s)\n",
      "  Epoch Time: 669.88s, Est. Remaining: 0.00 minutes\n",
      "  No improvement for 1/2 epochs\n",
      "Training completed in 11.16 minutes (669.9 seconds)\n",
      "Loaded best model (version single_output_v1)\n",
      "Params: [18.29659782 33.76749702  0.04846934 17.58042272  2.91651477], Validation MAPE: 14.3235%\n",
      "Model architecture has changed - detected parameter size mismatch.\n",
      "Removing incompatible checkpoint and starting fresh training.\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 51.9950, Time: 0.074s\n",
      "  Batch 20/765, Loss: 32.1764, Time: 0.085s\n",
      "  Batch 30/765, Loss: 23.6355, Time: 0.073s\n",
      "  Batch 40/765, Loss: 36.1736, Time: 0.072s\n",
      "  Batch 50/765, Loss: 27.0274, Time: 0.072s\n",
      "  Batch 60/765, Loss: 14.0929, Time: 0.072s\n",
      "  Batch 70/765, Loss: 17.2827, Time: 0.094s\n",
      "  Batch 80/765, Loss: 16.4952, Time: 0.048s\n",
      "  Batch 90/765, Loss: 25.8291, Time: 0.079s\n",
      "  Batch 100/765, Loss: 14.8279, Time: 0.083s\n",
      "  Batch 110/765, Loss: 12.2600, Time: 0.080s\n",
      "  Batch 120/765, Loss: 10.6005, Time: 0.076s\n",
      "  Batch 130/765, Loss: 9.5815, Time: 0.076s\n",
      "  Batch 140/765, Loss: 9.2576, Time: 0.072s\n",
      "  Batch 150/765, Loss: 13.3987, Time: 0.075s\n",
      "  Batch 160/765, Loss: 13.3520, Time: 0.072s\n",
      "  Batch 170/765, Loss: 13.4368, Time: 0.075s\n",
      "  Batch 180/765, Loss: 9.8105, Time: 0.052s\n",
      "  Batch 190/765, Loss: 9.1486, Time: 0.079s\n",
      "  Batch 200/765, Loss: 12.4060, Time: 0.073s\n",
      "  Batch 210/765, Loss: 11.3558, Time: 0.081s\n",
      "  Batch 220/765, Loss: 11.9628, Time: 0.074s\n",
      "  Batch 230/765, Loss: 8.9270, Time: 0.074s\n",
      "  Batch 240/765, Loss: 10.6850, Time: 0.078s\n",
      "  Batch 250/765, Loss: 11.2604, Time: 0.072s\n",
      "  Batch 260/765, Loss: 9.6825, Time: 0.074s\n",
      "  Batch 270/765, Loss: 7.5744, Time: 0.219s\n",
      "  Batch 280/765, Loss: 9.7583, Time: 0.075s\n",
      "  Batch 290/765, Loss: 9.8541, Time: 0.081s\n",
      "  Batch 300/765, Loss: 9.1008, Time: 0.078s\n",
      "  Batch 310/765, Loss: 9.0505, Time: 0.110s\n",
      "  Batch 320/765, Loss: 9.9630, Time: 0.084s\n",
      "  Batch 330/765, Loss: 11.7268, Time: 0.077s\n",
      "  Batch 340/765, Loss: 10.4232, Time: 0.051s\n",
      "  Batch 350/765, Loss: 11.2916, Time: 0.095s\n",
      "  Batch 360/765, Loss: 6.9037, Time: 0.077s\n",
      "  Batch 370/765, Loss: 7.4140, Time: 0.077s\n",
      "  Batch 380/765, Loss: 6.8462, Time: 0.076s\n",
      "  Batch 390/765, Loss: 11.7847, Time: 0.079s\n",
      "  Batch 400/765, Loss: 8.8679, Time: 0.084s\n",
      "  Batch 410/765, Loss: 7.8188, Time: 0.083s\n",
      "  Batch 420/765, Loss: 8.6925, Time: 0.074s\n",
      "  Batch 430/765, Loss: 9.2609, Time: 0.075s\n",
      "  Batch 440/765, Loss: 11.0708, Time: 0.080s\n",
      "  Batch 450/765, Loss: 8.3983, Time: 0.077s\n",
      "  Batch 460/765, Loss: 9.9600, Time: 0.074s\n",
      "  Batch 470/765, Loss: 11.8156, Time: 0.076s\n",
      "  Batch 480/765, Loss: 11.7137, Time: 0.074s\n",
      "  Batch 490/765, Loss: 8.6961, Time: 0.074s\n",
      "  Batch 500/765, Loss: 9.8419, Time: 0.076s\n",
      "  Batch 510/765, Loss: 10.4092, Time: 0.075s\n",
      "  Batch 520/765, Loss: 9.8327, Time: 0.051s\n",
      "  Batch 530/765, Loss: 9.4566, Time: 0.075s\n",
      "  Batch 540/765, Loss: 8.5941, Time: 0.084s\n",
      "  Batch 550/765, Loss: 9.9697, Time: 0.072s\n",
      "  Batch 560/765, Loss: 7.3827, Time: 0.071s\n",
      "  Batch 570/765, Loss: 10.0858, Time: 0.073s\n",
      "  Batch 580/765, Loss: 12.1875, Time: 0.074s\n",
      "  Batch 590/765, Loss: 9.0092, Time: 0.073s\n",
      "  Batch 600/765, Loss: 9.2420, Time: 0.074s\n",
      "  Batch 610/765, Loss: 11.9305, Time: 0.072s\n",
      "  Batch 620/765, Loss: 11.1867, Time: 0.073s\n",
      "  Batch 630/765, Loss: 11.6748, Time: 0.074s\n",
      "  Batch 640/765, Loss: 7.9771, Time: 0.114s\n",
      "  Batch 650/765, Loss: 6.6666, Time: 0.075s\n",
      "  Batch 660/765, Loss: 10.3145, Time: 0.072s\n",
      "  Batch 670/765, Loss: 11.4737, Time: 0.096s\n",
      "  Batch 680/765, Loss: 8.3364, Time: 0.071s\n",
      "  Batch 690/765, Loss: 9.0988, Time: 0.044s\n",
      "  Batch 700/765, Loss: 7.3088, Time: 0.043s\n",
      "  Batch 710/765, Loss: 10.3627, Time: 0.078s\n",
      "  Batch 720/765, Loss: 8.5324, Time: 0.079s\n",
      "  Batch 730/765, Loss: 9.1427, Time: 0.048s\n",
      "  Batch 740/765, Loss: 7.9506, Time: 0.047s\n",
      "  Batch 750/765, Loss: 10.3139, Time: 0.044s\n",
      "  Batch 760/765, Loss: 9.1113, Time: 0.070s\n",
      "Epoch 1/5\n",
      "  Train Loss: 13.3992 (765 batches, avg batch time: 0.076s)\n",
      "  Val Loss: 9.2609 (validation time: 75.81s)\n",
      "  Epoch Time: 674.58s, Est. Remaining: 44.97 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.2609\n",
      "  Batch 10/765, Loss: 8.5612, Time: 0.072s\n",
      "  Batch 20/765, Loss: 10.1628, Time: 0.076s\n",
      "  Batch 30/765, Loss: 7.9897, Time: 0.075s\n",
      "  Batch 40/765, Loss: 9.1956, Time: 0.074s\n",
      "  Batch 50/765, Loss: 8.2045, Time: 0.072s\n",
      "  Batch 60/765, Loss: 8.3778, Time: 0.073s\n",
      "  Batch 70/765, Loss: 8.5697, Time: 0.082s\n",
      "  Batch 80/765, Loss: 8.9167, Time: 0.073s\n",
      "  Batch 90/765, Loss: 9.4361, Time: 0.072s\n",
      "  Batch 100/765, Loss: 9.9310, Time: 0.071s\n",
      "  Batch 110/765, Loss: 10.3076, Time: 0.073s\n",
      "  Batch 120/765, Loss: 9.9941, Time: 0.077s\n",
      "  Batch 130/765, Loss: 7.6678, Time: 0.074s\n",
      "  Batch 140/765, Loss: 9.3975, Time: 0.073s\n",
      "  Batch 150/765, Loss: 12.1197, Time: 0.073s\n",
      "  Batch 160/765, Loss: 10.6587, Time: 0.084s\n",
      "  Batch 170/765, Loss: 11.2806, Time: 0.076s\n",
      "  Batch 180/765, Loss: 8.8893, Time: 0.073s\n",
      "  Batch 190/765, Loss: 9.8648, Time: 0.073s\n",
      "  Batch 200/765, Loss: 9.2266, Time: 0.049s\n",
      "  Batch 210/765, Loss: 9.4682, Time: 0.073s\n",
      "  Batch 220/765, Loss: 8.5095, Time: 0.074s\n",
      "  Batch 230/765, Loss: 8.9734, Time: 0.053s\n",
      "  Batch 240/765, Loss: 6.2574, Time: 0.073s\n",
      "  Batch 250/765, Loss: 7.7899, Time: 0.078s\n",
      "  Batch 260/765, Loss: 13.1216, Time: 0.076s\n",
      "  Batch 270/765, Loss: 9.3180, Time: 0.076s\n",
      "  Batch 280/765, Loss: 10.2479, Time: 0.073s\n",
      "  Batch 290/765, Loss: 10.6092, Time: 0.075s\n",
      "  Batch 300/765, Loss: 10.4375, Time: 0.075s\n",
      "  Batch 310/765, Loss: 10.1732, Time: 0.074s\n",
      "  Batch 320/765, Loss: 12.9071, Time: 0.074s\n",
      "  Batch 330/765, Loss: 9.3105, Time: 0.077s\n",
      "  Batch 340/765, Loss: 8.1994, Time: 0.049s\n",
      "  Batch 350/765, Loss: 8.4261, Time: 0.076s\n",
      "  Batch 360/765, Loss: 8.4746, Time: 0.073s\n",
      "  Batch 370/765, Loss: 8.5207, Time: 0.072s\n",
      "  Batch 380/765, Loss: 8.9069, Time: 0.050s\n",
      "  Batch 390/765, Loss: 7.1919, Time: 0.072s\n",
      "  Batch 400/765, Loss: 11.4627, Time: 0.073s\n",
      "  Batch 410/765, Loss: 10.4551, Time: 0.201s\n",
      "  Batch 420/765, Loss: 9.9176, Time: 0.077s\n",
      "  Batch 430/765, Loss: 7.9487, Time: 0.074s\n",
      "  Batch 440/765, Loss: 10.5776, Time: 0.074s\n",
      "  Batch 450/765, Loss: 5.5387, Time: 0.076s\n",
      "  Batch 460/765, Loss: 8.9371, Time: 0.076s\n",
      "  Batch 470/765, Loss: 11.1669, Time: 0.051s\n",
      "  Batch 480/765, Loss: 7.3868, Time: 0.074s\n",
      "  Batch 490/765, Loss: 8.7283, Time: 0.073s\n",
      "  Batch 500/765, Loss: 7.6708, Time: 0.085s\n",
      "  Batch 510/765, Loss: 9.7946, Time: 0.073s\n",
      "  Batch 520/765, Loss: 8.8324, Time: 0.073s\n",
      "  Batch 530/765, Loss: 12.1200, Time: 0.072s\n",
      "  Batch 540/765, Loss: 10.3484, Time: 0.075s\n",
      "  Batch 550/765, Loss: 8.6921, Time: 0.073s\n",
      "  Batch 560/765, Loss: 9.0136, Time: 0.075s\n",
      "  Batch 570/765, Loss: 10.2969, Time: 0.073s\n",
      "  Batch 580/765, Loss: 8.9840, Time: 0.073s\n",
      "  Batch 590/765, Loss: 10.4184, Time: 0.079s\n",
      "  Batch 600/765, Loss: 8.6659, Time: 0.075s\n",
      "  Batch 610/765, Loss: 10.3810, Time: 0.074s\n",
      "  Batch 620/765, Loss: 11.4317, Time: 0.072s\n",
      "  Batch 630/765, Loss: 9.2806, Time: 0.073s\n",
      "  Batch 640/765, Loss: 10.2972, Time: 0.117s\n",
      "  Batch 650/765, Loss: 6.8664, Time: 0.072s\n",
      "  Batch 660/765, Loss: 8.8072, Time: 0.075s\n",
      "  Batch 670/765, Loss: 9.3188, Time: 0.049s\n",
      "  Batch 680/765, Loss: 8.8760, Time: 0.072s\n",
      "  Batch 690/765, Loss: 8.2905, Time: 0.074s\n",
      "  Batch 700/765, Loss: 9.3715, Time: 0.073s\n",
      "  Batch 710/765, Loss: 10.0086, Time: 0.075s\n",
      "  Batch 720/765, Loss: 11.7781, Time: 0.080s\n",
      "  Batch 730/765, Loss: 12.0713, Time: 0.074s\n",
      "  Batch 740/765, Loss: 8.7043, Time: 0.076s\n",
      "  Batch 750/765, Loss: 12.6081, Time: 0.098s\n",
      "  Batch 760/765, Loss: 9.6994, Time: 0.075s\n",
      "Epoch 2/5\n",
      "  Train Loss: 9.2853 (765 batches, avg batch time: 0.077s)\n",
      "  Val Loss: 8.9425 (validation time: 78.43s)\n",
      "  Epoch Time: 661.78s, Est. Remaining: 33.41 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.9425\n",
      "  Batch 10/765, Loss: 10.5924, Time: 0.081s\n",
      "  Batch 20/765, Loss: 9.3521, Time: 0.076s\n",
      "  Batch 30/765, Loss: 10.3318, Time: 0.077s\n",
      "  Batch 40/765, Loss: 12.0081, Time: 0.076s\n",
      "  Batch 50/765, Loss: 9.5082, Time: 0.077s\n",
      "  Batch 60/765, Loss: 5.2435, Time: 0.049s\n",
      "  Batch 70/765, Loss: 9.7338, Time: 0.072s\n",
      "  Batch 80/765, Loss: 10.2487, Time: 0.075s\n",
      "  Batch 90/765, Loss: 8.9459, Time: 0.075s\n",
      "  Batch 100/765, Loss: 12.7282, Time: 0.072s\n",
      "  Batch 110/765, Loss: 9.4644, Time: 0.075s\n",
      "  Batch 120/765, Loss: 7.8722, Time: 0.071s\n",
      "  Batch 130/765, Loss: 11.7595, Time: 0.101s\n",
      "  Batch 140/765, Loss: 6.4934, Time: 0.072s\n",
      "  Batch 150/765, Loss: 8.8900, Time: 0.073s\n",
      "  Batch 160/765, Loss: 11.8171, Time: 0.101s\n",
      "  Batch 170/765, Loss: 9.2072, Time: 0.075s\n",
      "  Batch 180/765, Loss: 7.5883, Time: 0.074s\n",
      "  Batch 190/765, Loss: 8.3189, Time: 0.073s\n",
      "  Batch 200/765, Loss: 9.5570, Time: 0.075s\n",
      "  Batch 210/765, Loss: 8.9878, Time: 0.075s\n",
      "  Batch 220/765, Loss: 9.0940, Time: 0.075s\n",
      "  Batch 230/765, Loss: 10.0710, Time: 0.076s\n",
      "  Batch 240/765, Loss: 9.5448, Time: 0.130s\n",
      "  Batch 250/765, Loss: 11.1133, Time: 0.093s\n",
      "  Batch 260/765, Loss: 8.8499, Time: 0.074s\n",
      "  Batch 270/765, Loss: 5.7266, Time: 0.073s\n",
      "  Batch 280/765, Loss: 8.5806, Time: 0.074s\n",
      "  Batch 290/765, Loss: 8.9040, Time: 0.072s\n",
      "  Batch 300/765, Loss: 9.5914, Time: 0.075s\n",
      "  Batch 310/765, Loss: 7.8741, Time: 0.082s\n",
      "  Batch 320/765, Loss: 9.2125, Time: 0.076s\n",
      "  Batch 330/765, Loss: 9.7248, Time: 0.074s\n",
      "  Batch 340/765, Loss: 7.3056, Time: 0.076s\n",
      "  Batch 350/765, Loss: 6.4707, Time: 0.051s\n",
      "  Batch 360/765, Loss: 7.6953, Time: 0.075s\n",
      "  Batch 370/765, Loss: 9.5436, Time: 0.083s\n",
      "  Batch 380/765, Loss: 10.7589, Time: 0.053s\n",
      "  Batch 390/765, Loss: 6.2262, Time: 0.072s\n",
      "  Batch 400/765, Loss: 8.8738, Time: 0.074s\n",
      "  Batch 410/765, Loss: 9.5569, Time: 0.072s\n",
      "  Batch 420/765, Loss: 9.2052, Time: 0.092s\n",
      "  Batch 430/765, Loss: 9.6642, Time: 0.095s\n",
      "  Batch 440/765, Loss: 10.0158, Time: 0.073s\n",
      "  Batch 450/765, Loss: 9.2268, Time: 0.073s\n",
      "  Batch 460/765, Loss: 8.9075, Time: 0.076s\n",
      "  Batch 470/765, Loss: 9.7451, Time: 0.073s\n",
      "  Batch 480/765, Loss: 8.4838, Time: 0.073s\n",
      "  Batch 490/765, Loss: 9.8200, Time: 0.080s\n",
      "  Batch 500/765, Loss: 9.1228, Time: 0.074s\n",
      "  Batch 510/765, Loss: 9.9556, Time: 0.074s\n",
      "  Batch 520/765, Loss: 8.1249, Time: 0.073s\n",
      "  Batch 530/765, Loss: 8.2084, Time: 0.075s\n",
      "  Batch 540/765, Loss: 10.7529, Time: 0.075s\n",
      "  Batch 550/765, Loss: 10.2931, Time: 0.073s\n",
      "  Batch 560/765, Loss: 10.5997, Time: 0.112s\n",
      "  Batch 570/765, Loss: 11.2981, Time: 0.071s\n",
      "  Batch 580/765, Loss: 9.4685, Time: 0.050s\n",
      "  Batch 590/765, Loss: 8.5683, Time: 0.074s\n",
      "  Batch 600/765, Loss: 7.8261, Time: 0.073s\n",
      "  Batch 610/765, Loss: 10.1475, Time: 0.072s\n",
      "  Batch 620/765, Loss: 7.8412, Time: 0.072s\n",
      "  Batch 630/765, Loss: 8.0169, Time: 0.073s\n",
      "  Batch 640/765, Loss: 10.1846, Time: 0.073s\n",
      "  Batch 650/765, Loss: 8.4277, Time: 0.073s\n",
      "  Batch 660/765, Loss: 7.8411, Time: 0.085s\n",
      "  Batch 670/765, Loss: 9.5602, Time: 0.077s\n",
      "  Batch 680/765, Loss: 8.1969, Time: 0.074s\n",
      "  Batch 690/765, Loss: 7.9829, Time: 0.130s\n",
      "  Batch 700/765, Loss: 8.3915, Time: 0.076s\n",
      "  Batch 710/765, Loss: 9.6685, Time: 0.073s\n",
      "  Batch 720/765, Loss: 6.7874, Time: 0.075s\n",
      "  Batch 730/765, Loss: 8.6915, Time: 0.076s\n",
      "  Batch 740/765, Loss: 10.2353, Time: 0.073s\n",
      "  Batch 750/765, Loss: 8.5661, Time: 0.080s\n",
      "  Batch 760/765, Loss: 7.8140, Time: 0.074s\n",
      "Epoch 3/5\n",
      "  Train Loss: 9.1375 (765 batches, avg batch time: 0.078s)\n",
      "  Val Loss: 8.8333 (validation time: 75.59s)\n",
      "  Epoch Time: 657.50s, Est. Remaining: 22.15 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.8333\n",
      "  Batch 10/765, Loss: 10.2514, Time: 0.072s\n",
      "  Batch 20/765, Loss: 12.2487, Time: 0.075s\n",
      "  Batch 30/765, Loss: 9.5180, Time: 0.073s\n",
      "  Batch 40/765, Loss: 9.2198, Time: 0.084s\n",
      "  Batch 50/765, Loss: 11.4903, Time: 0.076s\n",
      "  Batch 60/765, Loss: 8.5176, Time: 0.048s\n",
      "  Batch 70/765, Loss: 7.8036, Time: 0.048s\n",
      "  Batch 80/765, Loss: 9.4127, Time: 0.054s\n",
      "  Batch 90/765, Loss: 10.5124, Time: 0.097s\n",
      "  Batch 100/765, Loss: 8.1029, Time: 0.073s\n",
      "  Batch 110/765, Loss: 11.2737, Time: 0.072s\n",
      "  Batch 120/765, Loss: 11.4596, Time: 0.073s\n",
      "  Batch 130/765, Loss: 8.3172, Time: 0.108s\n",
      "  Batch 140/765, Loss: 6.8657, Time: 0.072s\n",
      "  Batch 150/765, Loss: 8.5589, Time: 0.073s\n",
      "  Batch 160/765, Loss: 9.0329, Time: 0.072s\n",
      "  Batch 170/765, Loss: 7.7219, Time: 0.075s\n",
      "  Batch 180/765, Loss: 7.2564, Time: 0.072s\n",
      "  Batch 190/765, Loss: 6.2059, Time: 0.075s\n",
      "  Batch 200/765, Loss: 10.3924, Time: 0.078s\n",
      "  Batch 210/765, Loss: 10.6242, Time: 0.048s\n",
      "  Batch 220/765, Loss: 7.6417, Time: 0.073s\n",
      "  Batch 230/765, Loss: 6.8538, Time: 0.047s\n",
      "  Batch 240/765, Loss: 9.7399, Time: 0.072s\n",
      "  Batch 250/765, Loss: 10.5553, Time: 0.072s\n",
      "  Batch 260/765, Loss: 11.5439, Time: 0.073s\n",
      "  Batch 270/765, Loss: 10.8382, Time: 0.047s\n",
      "  Batch 280/765, Loss: 10.6404, Time: 0.073s\n",
      "  Batch 290/765, Loss: 8.6650, Time: 0.072s\n",
      "  Batch 300/765, Loss: 11.3079, Time: 0.072s\n",
      "  Batch 310/765, Loss: 8.6366, Time: 0.072s\n",
      "  Batch 320/765, Loss: 10.6752, Time: 0.073s\n",
      "  Batch 330/765, Loss: 9.4796, Time: 0.072s\n",
      "  Batch 340/765, Loss: 11.1331, Time: 0.072s\n",
      "  Batch 350/765, Loss: 8.5672, Time: 0.073s\n",
      "  Batch 360/765, Loss: 9.6634, Time: 0.074s\n",
      "  Batch 370/765, Loss: 9.6080, Time: 0.071s\n",
      "  Batch 380/765, Loss: 9.6183, Time: 0.074s\n",
      "  Batch 390/765, Loss: 10.5783, Time: 0.075s\n",
      "  Batch 400/765, Loss: 7.5699, Time: 0.073s\n",
      "  Batch 410/765, Loss: 8.9368, Time: 0.073s\n",
      "  Batch 420/765, Loss: 8.6383, Time: 0.315s\n",
      "  Batch 430/765, Loss: 7.4898, Time: 0.074s\n",
      "  Batch 440/765, Loss: 8.3191, Time: 0.075s\n",
      "  Batch 450/765, Loss: 10.0162, Time: 0.075s\n",
      "  Batch 460/765, Loss: 8.6802, Time: 0.085s\n",
      "  Batch 470/765, Loss: 8.9506, Time: 0.077s\n",
      "  Batch 480/765, Loss: 6.4749, Time: 0.074s\n",
      "  Batch 490/765, Loss: 10.3621, Time: 0.072s\n",
      "  Batch 500/765, Loss: 8.4518, Time: 0.048s\n",
      "  Batch 510/765, Loss: 9.9976, Time: 0.073s\n",
      "  Batch 520/765, Loss: 8.3906, Time: 0.072s\n",
      "  Batch 530/765, Loss: 7.7340, Time: 0.072s\n",
      "  Batch 540/765, Loss: 8.2736, Time: 0.071s\n",
      "  Batch 550/765, Loss: 10.3392, Time: 0.073s\n",
      "  Batch 560/765, Loss: 9.8778, Time: 0.076s\n",
      "  Batch 570/765, Loss: 10.3813, Time: 0.055s\n",
      "  Batch 580/765, Loss: 8.1303, Time: 0.072s\n",
      "  Batch 590/765, Loss: 7.2984, Time: 0.127s\n",
      "  Batch 600/765, Loss: 6.6171, Time: 0.074s\n",
      "  Batch 610/765, Loss: 8.0289, Time: 0.073s\n",
      "  Batch 620/765, Loss: 12.6064, Time: 0.073s\n",
      "  Batch 630/765, Loss: 7.1981, Time: 0.073s\n",
      "  Batch 640/765, Loss: 11.2505, Time: 0.082s\n",
      "  Batch 650/765, Loss: 8.6306, Time: 0.049s\n",
      "  Batch 660/765, Loss: 9.8893, Time: 0.074s\n",
      "  Batch 670/765, Loss: 8.8027, Time: 0.074s\n",
      "  Batch 680/765, Loss: 11.1011, Time: 0.074s\n",
      "  Batch 690/765, Loss: 8.0578, Time: 0.075s\n",
      "  Batch 700/765, Loss: 9.1842, Time: 0.074s\n",
      "  Batch 710/765, Loss: 9.1543, Time: 0.078s\n",
      "  Batch 720/765, Loss: 9.4188, Time: 0.077s\n",
      "  Batch 730/765, Loss: 8.7997, Time: 0.077s\n",
      "  Batch 740/765, Loss: 8.7043, Time: 0.077s\n",
      "  Batch 750/765, Loss: 11.5531, Time: 0.058s\n",
      "  Batch 760/765, Loss: 8.9795, Time: 0.073s\n",
      "Epoch 4/5\n",
      "  Train Loss: 9.0266 (765 batches, avg batch time: 0.077s)\n",
      "  Val Loss: 8.8284 (validation time: 78.57s)\n",
      "  Epoch Time: 659.54s, Est. Remaining: 11.06 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.8284\n",
      "  Batch 10/765, Loss: 11.8664, Time: 0.083s\n",
      "  Batch 20/765, Loss: 9.1643, Time: 0.078s\n",
      "  Batch 30/765, Loss: 9.5089, Time: 0.074s\n",
      "  Batch 40/765, Loss: 14.6653, Time: 0.075s\n",
      "  Batch 50/765, Loss: 12.6343, Time: 0.075s\n",
      "  Batch 60/765, Loss: 9.7003, Time: 0.074s\n",
      "  Batch 70/765, Loss: 8.0188, Time: 0.074s\n",
      "  Batch 80/765, Loss: 5.7658, Time: 0.047s\n",
      "  Batch 90/765, Loss: 9.8492, Time: 0.072s\n",
      "  Batch 100/765, Loss: 9.4603, Time: 0.082s\n",
      "  Batch 110/765, Loss: 7.5338, Time: 0.053s\n",
      "  Batch 120/765, Loss: 9.9710, Time: 0.073s\n",
      "  Batch 130/765, Loss: 9.8320, Time: 0.073s\n",
      "  Batch 140/765, Loss: 8.9081, Time: 0.074s\n",
      "  Batch 150/765, Loss: 10.7256, Time: 0.072s\n",
      "  Batch 160/765, Loss: 7.4379, Time: 0.072s\n",
      "  Batch 170/765, Loss: 6.2861, Time: 0.073s\n",
      "  Batch 180/765, Loss: 7.8892, Time: 0.074s\n",
      "  Batch 190/765, Loss: 10.1473, Time: 0.094s\n",
      "  Batch 200/765, Loss: 7.7224, Time: 0.085s\n",
      "  Batch 210/765, Loss: 7.1095, Time: 0.072s\n",
      "  Batch 220/765, Loss: 7.5452, Time: 0.100s\n",
      "  Batch 230/765, Loss: 8.1476, Time: 0.076s\n",
      "  Batch 240/765, Loss: 9.0107, Time: 0.073s\n",
      "  Batch 250/765, Loss: 7.8301, Time: 0.072s\n",
      "  Batch 260/765, Loss: 8.4849, Time: 0.072s\n",
      "  Batch 270/765, Loss: 7.3223, Time: 0.074s\n",
      "  Batch 280/765, Loss: 8.4672, Time: 0.073s\n",
      "  Batch 290/765, Loss: 8.4928, Time: 0.078s\n",
      "  Batch 300/765, Loss: 8.7367, Time: 0.079s\n",
      "  Batch 310/765, Loss: 9.3010, Time: 0.077s\n",
      "  Batch 320/765, Loss: 11.1194, Time: 0.072s\n",
      "  Batch 330/765, Loss: 9.2285, Time: 0.071s\n",
      "  Batch 340/765, Loss: 7.8477, Time: 0.071s\n",
      "  Batch 350/765, Loss: 9.0822, Time: 0.076s\n",
      "  Batch 360/765, Loss: 7.4729, Time: 0.074s\n",
      "  Batch 370/765, Loss: 7.0000, Time: 0.091s\n",
      "  Batch 380/765, Loss: 7.6797, Time: 0.090s\n",
      "  Batch 390/765, Loss: 5.1258, Time: 0.089s\n",
      "  Batch 400/765, Loss: 11.0170, Time: 0.094s\n",
      "  Batch 410/765, Loss: 12.9317, Time: 0.079s\n",
      "  Batch 420/765, Loss: 7.9493, Time: 0.072s\n",
      "  Batch 430/765, Loss: 11.5835, Time: 0.053s\n",
      "  Batch 440/765, Loss: 7.7962, Time: 0.101s\n",
      "  Batch 450/765, Loss: 9.8067, Time: 0.073s\n",
      "  Batch 460/765, Loss: 9.9821, Time: 0.074s\n",
      "  Batch 470/765, Loss: 8.8279, Time: 0.052s\n",
      "  Batch 480/765, Loss: 9.5017, Time: 0.073s\n",
      "  Batch 490/765, Loss: 8.6705, Time: 0.073s\n",
      "  Batch 500/765, Loss: 11.3515, Time: 0.074s\n",
      "  Batch 510/765, Loss: 8.6893, Time: 0.075s\n",
      "  Batch 520/765, Loss: 9.4822, Time: 0.071s\n",
      "  Batch 530/765, Loss: 7.9626, Time: 0.050s\n",
      "  Batch 540/765, Loss: 10.6816, Time: 0.076s\n",
      "  Batch 550/765, Loss: 7.1992, Time: 0.084s\n",
      "  Batch 560/765, Loss: 10.0645, Time: 0.073s\n",
      "  Batch 570/765, Loss: 7.8592, Time: 0.086s\n",
      "  Batch 580/765, Loss: 6.7736, Time: 0.073s\n",
      "  Batch 590/765, Loss: 10.9450, Time: 0.075s\n",
      "  Batch 600/765, Loss: 7.8120, Time: 0.074s\n",
      "  Batch 610/765, Loss: 7.5009, Time: 0.092s\n",
      "  Batch 620/765, Loss: 10.2667, Time: 0.075s\n",
      "  Batch 630/765, Loss: 8.1258, Time: 0.074s\n",
      "  Batch 640/765, Loss: 8.3304, Time: 0.074s\n",
      "  Batch 650/765, Loss: 9.3935, Time: 0.074s\n",
      "  Batch 660/765, Loss: 7.2003, Time: 0.048s\n",
      "  Batch 670/765, Loss: 11.1198, Time: 0.078s\n",
      "  Batch 680/765, Loss: 7.6845, Time: 0.074s\n",
      "  Batch 690/765, Loss: 9.2523, Time: 0.075s\n",
      "  Batch 700/765, Loss: 9.8346, Time: 0.090s\n",
      "  Batch 710/765, Loss: 7.9728, Time: 0.072s\n",
      "  Batch 720/765, Loss: 8.0615, Time: 0.073s\n",
      "  Batch 730/765, Loss: 9.5352, Time: 0.074s\n",
      "  Batch 740/765, Loss: 10.5881, Time: 0.076s\n",
      "  Batch 750/765, Loss: 6.9417, Time: 0.072s\n",
      "  Batch 760/765, Loss: 7.6231, Time: 0.073s\n",
      "Epoch 5/5\n",
      "  Train Loss: 9.0710 (765 batches, avg batch time: 0.077s)\n",
      "  Val Loss: 8.8005 (validation time: 75.73s)\n",
      "  Epoch Time: 655.87s, Est. Remaining: 0.00 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.8005\n",
      "Training completed in 55.16 minutes (3309.5 seconds)\n",
      "Loaded best model (version single_output_v1)\n",
      "Params: [1.83896252e+01 4.97870778e+01 2.45246325e-02 2.21847065e+01\n",
      " 2.57622793e+00], Validation MAPE: 13.5046%\n",
      "Model architecture has changed - detected parameter size mismatch.\n",
      "Removing incompatible checkpoint and starting fresh training.\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 21.7754, Time: 0.074s\n",
      "  Batch 20/765, Loss: 17.4635, Time: 0.072s\n",
      "  Batch 30/765, Loss: 26.9054, Time: 0.074s\n",
      "  Batch 40/765, Loss: 17.1485, Time: 0.079s\n",
      "  Batch 50/765, Loss: 27.2690, Time: 0.067s\n",
      "  Batch 60/765, Loss: 18.0303, Time: 0.074s\n",
      "  Batch 70/765, Loss: 18.9542, Time: 0.083s\n",
      "  Batch 80/765, Loss: 13.3502, Time: 0.065s\n",
      "  Batch 90/765, Loss: 11.2790, Time: 0.074s\n",
      "  Batch 100/765, Loss: 14.7190, Time: 0.134s\n",
      "  Batch 110/765, Loss: 15.3994, Time: 0.074s\n",
      "  Batch 120/765, Loss: 12.8650, Time: 0.078s\n",
      "  Batch 130/765, Loss: 10.9229, Time: 0.073s\n",
      "  Batch 140/765, Loss: 10.1432, Time: 0.075s\n",
      "  Batch 150/765, Loss: 14.5896, Time: 0.076s\n",
      "  Batch 160/765, Loss: 14.3326, Time: 0.073s\n",
      "  Batch 170/765, Loss: 15.4690, Time: 0.072s\n",
      "  Batch 180/765, Loss: 7.1032, Time: 0.075s\n",
      "  Batch 190/765, Loss: 9.7195, Time: 0.075s\n",
      "  Batch 200/765, Loss: 7.9978, Time: 0.076s\n",
      "  Batch 210/765, Loss: 8.8636, Time: 0.089s\n",
      "  Batch 220/765, Loss: 9.3107, Time: 0.073s\n",
      "  Batch 230/765, Loss: 9.0607, Time: 0.075s\n",
      "  Batch 240/765, Loss: 8.2426, Time: 0.151s\n",
      "  Batch 250/765, Loss: 10.9247, Time: 0.078s\n",
      "  Batch 260/765, Loss: 9.9727, Time: 0.075s\n",
      "  Batch 270/765, Loss: 10.5756, Time: 0.074s\n",
      "  Batch 280/765, Loss: 11.0320, Time: 0.075s\n",
      "  Batch 290/765, Loss: 9.6672, Time: 0.076s\n",
      "  Batch 300/765, Loss: 13.3550, Time: 0.096s\n",
      "  Batch 310/765, Loss: 10.6548, Time: 0.073s\n",
      "  Batch 320/765, Loss: 10.6375, Time: 0.073s\n",
      "  Batch 330/765, Loss: 9.0018, Time: 0.072s\n",
      "  Batch 340/765, Loss: 8.5024, Time: 0.075s\n",
      "  Batch 350/765, Loss: 9.1530, Time: 0.071s\n",
      "  Batch 360/765, Loss: 9.9786, Time: 0.072s\n",
      "  Batch 370/765, Loss: 8.4081, Time: 0.074s\n",
      "  Batch 380/765, Loss: 10.2097, Time: 0.074s\n",
      "  Batch 390/765, Loss: 7.7136, Time: 0.074s\n",
      "  Batch 400/765, Loss: 9.7705, Time: 0.048s\n",
      "  Batch 410/765, Loss: 11.5404, Time: 0.135s\n",
      "  Batch 420/765, Loss: 11.3845, Time: 0.077s\n",
      "  Batch 430/765, Loss: 10.3839, Time: 0.081s\n",
      "  Batch 440/765, Loss: 8.4182, Time: 0.075s\n",
      "  Batch 450/765, Loss: 9.8153, Time: 0.074s\n",
      "  Batch 460/765, Loss: 9.9028, Time: 0.074s\n",
      "  Batch 470/765, Loss: 10.8534, Time: 0.096s\n",
      "  Batch 480/765, Loss: 13.6452, Time: 0.083s\n",
      "  Batch 490/765, Loss: 11.8967, Time: 0.093s\n",
      "  Batch 500/765, Loss: 8.9357, Time: 0.072s\n",
      "  Batch 510/765, Loss: 8.3255, Time: 0.075s\n",
      "  Batch 520/765, Loss: 9.7001, Time: 0.073s\n",
      "  Batch 530/765, Loss: 14.0027, Time: 0.074s\n",
      "  Batch 540/765, Loss: 9.5527, Time: 0.074s\n",
      "  Batch 550/765, Loss: 8.5141, Time: 0.075s\n",
      "  Batch 560/765, Loss: 9.9080, Time: 0.076s\n",
      "  Batch 570/765, Loss: 9.0600, Time: 0.078s\n",
      "  Batch 580/765, Loss: 9.3653, Time: 0.079s\n",
      "  Batch 590/765, Loss: 8.9034, Time: 0.076s\n",
      "  Batch 600/765, Loss: 8.6747, Time: 0.079s\n",
      "  Batch 610/765, Loss: 9.3476, Time: 0.088s\n",
      "  Batch 620/765, Loss: 5.8631, Time: 0.075s\n",
      "  Batch 630/765, Loss: 10.5298, Time: 0.073s\n",
      "  Batch 640/765, Loss: 11.6162, Time: 0.077s\n",
      "  Batch 650/765, Loss: 12.4762, Time: 0.075s\n",
      "  Batch 660/765, Loss: 10.0337, Time: 0.108s\n",
      "  Batch 670/765, Loss: 9.0445, Time: 0.077s\n",
      "  Batch 680/765, Loss: 9.0151, Time: 0.076s\n",
      "  Batch 690/765, Loss: 7.3001, Time: 0.074s\n",
      "  Batch 700/765, Loss: 9.2195, Time: 0.079s\n",
      "  Batch 710/765, Loss: 9.5859, Time: 0.079s\n",
      "  Batch 720/765, Loss: 9.9649, Time: 0.074s\n",
      "  Batch 730/765, Loss: 7.6412, Time: 0.077s\n",
      "  Batch 740/765, Loss: 10.3227, Time: 0.075s\n",
      "  Batch 750/765, Loss: 9.4088, Time: 0.076s\n",
      "  Batch 760/765, Loss: 8.9871, Time: 0.074s\n",
      "Epoch 1/5\n",
      "  Train Loss: 12.8178 (765 batches, avg batch time: 0.081s)\n",
      "  Val Loss: 9.0389 (validation time: 76.15s)\n",
      "  Epoch Time: 663.61s, Est. Remaining: 44.24 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.0389\n",
      "  Batch 10/765, Loss: 12.3524, Time: 0.074s\n",
      "  Batch 20/765, Loss: 10.3879, Time: 0.078s\n",
      "  Batch 30/765, Loss: 10.6670, Time: 0.074s\n",
      "  Batch 40/765, Loss: 8.5937, Time: 0.074s\n",
      "  Batch 50/765, Loss: 9.8633, Time: 0.099s\n",
      "  Batch 60/765, Loss: 8.6899, Time: 0.082s\n",
      "  Batch 70/765, Loss: 10.9560, Time: 0.102s\n",
      "  Batch 80/765, Loss: 11.4043, Time: 0.080s\n",
      "  Batch 90/765, Loss: 9.1736, Time: 0.077s\n",
      "  Batch 100/765, Loss: 11.0960, Time: 0.078s\n",
      "  Batch 110/765, Loss: 9.9295, Time: 0.096s\n",
      "  Batch 120/765, Loss: 9.4786, Time: 0.076s\n",
      "  Batch 130/765, Loss: 10.7124, Time: 0.076s\n",
      "  Batch 140/765, Loss: 7.4468, Time: 0.079s\n",
      "  Batch 150/765, Loss: 7.3832, Time: 0.074s\n",
      "  Batch 160/765, Loss: 10.3316, Time: 0.075s\n",
      "  Batch 170/765, Loss: 7.6732, Time: 0.088s\n",
      "  Batch 180/765, Loss: 8.7814, Time: 0.158s\n",
      "  Batch 190/765, Loss: 10.0753, Time: 0.074s\n",
      "  Batch 200/765, Loss: 9.7625, Time: 0.076s\n",
      "  Batch 210/765, Loss: 8.5284, Time: 0.099s\n",
      "  Batch 220/765, Loss: 10.1638, Time: 0.076s\n",
      "  Batch 230/765, Loss: 7.7936, Time: 0.074s\n",
      "  Batch 240/765, Loss: 7.9595, Time: 0.075s\n",
      "  Batch 250/765, Loss: 9.6101, Time: 0.074s\n",
      "  Batch 260/765, Loss: 7.0220, Time: 0.075s\n",
      "  Batch 270/765, Loss: 6.2685, Time: 0.075s\n",
      "  Batch 280/765, Loss: 12.3563, Time: 0.073s\n",
      "  Batch 290/765, Loss: 10.2167, Time: 0.074s\n",
      "  Batch 300/765, Loss: 10.4483, Time: 0.073s\n",
      "  Batch 310/765, Loss: 9.2702, Time: 0.227s\n",
      "  Batch 320/765, Loss: 7.3335, Time: 0.074s\n",
      "  Batch 330/765, Loss: 8.3452, Time: 0.077s\n",
      "  Batch 340/765, Loss: 8.7052, Time: 0.076s\n",
      "  Batch 350/765, Loss: 8.9085, Time: 0.048s\n",
      "  Batch 360/765, Loss: 8.0024, Time: 0.073s\n",
      "  Batch 370/765, Loss: 10.3824, Time: 0.072s\n",
      "  Batch 380/765, Loss: 9.1313, Time: 0.074s\n",
      "  Batch 390/765, Loss: 9.3137, Time: 0.072s\n",
      "  Batch 400/765, Loss: 7.5380, Time: 0.075s\n",
      "  Batch 410/765, Loss: 10.0881, Time: 0.074s\n",
      "  Batch 420/765, Loss: 9.1418, Time: 0.074s\n",
      "  Batch 430/765, Loss: 8.7617, Time: 0.100s\n",
      "  Batch 440/765, Loss: 11.3858, Time: 0.076s\n",
      "  Batch 450/765, Loss: 9.6202, Time: 0.076s\n",
      "  Batch 460/765, Loss: 7.2796, Time: 0.097s\n",
      "  Batch 470/765, Loss: 6.7426, Time: 0.073s\n",
      "  Batch 480/765, Loss: 9.8486, Time: 0.119s\n",
      "  Batch 490/765, Loss: 8.8925, Time: 0.088s\n",
      "  Batch 500/765, Loss: 10.0091, Time: 0.074s\n",
      "  Batch 510/765, Loss: 8.2231, Time: 0.075s\n",
      "  Batch 520/765, Loss: 9.4296, Time: 0.051s\n",
      "  Batch 530/765, Loss: 10.6052, Time: 0.054s\n",
      "  Batch 540/765, Loss: 7.6678, Time: 0.050s\n",
      "  Batch 550/765, Loss: 9.3723, Time: 0.076s\n",
      "  Batch 560/765, Loss: 9.6455, Time: 0.074s\n",
      "  Batch 570/765, Loss: 9.3540, Time: 0.052s\n",
      "  Batch 580/765, Loss: 8.9265, Time: 0.074s\n",
      "  Batch 590/765, Loss: 7.8084, Time: 0.075s\n",
      "  Batch 600/765, Loss: 13.7026, Time: 0.075s\n",
      "  Batch 610/765, Loss: 10.0274, Time: 0.126s\n",
      "  Batch 620/765, Loss: 8.4171, Time: 0.076s\n",
      "  Batch 630/765, Loss: 10.5696, Time: 0.073s\n",
      "  Batch 640/765, Loss: 10.5127, Time: 0.074s\n",
      "  Batch 650/765, Loss: 8.6368, Time: 0.075s\n",
      "  Batch 660/765, Loss: 9.8473, Time: 0.077s\n",
      "  Batch 670/765, Loss: 10.5781, Time: 0.076s\n",
      "  Batch 680/765, Loss: 6.6102, Time: 0.074s\n",
      "  Batch 690/765, Loss: 8.3893, Time: 0.076s\n",
      "  Batch 700/765, Loss: 13.3574, Time: 0.077s\n",
      "  Batch 710/765, Loss: 10.1652, Time: 0.049s\n",
      "  Batch 720/765, Loss: 9.7574, Time: 0.076s\n",
      "  Batch 730/765, Loss: 6.9937, Time: 0.073s\n",
      "  Batch 740/765, Loss: 12.4578, Time: 0.104s\n",
      "  Batch 750/765, Loss: 6.3066, Time: 0.074s\n",
      "  Batch 760/765, Loss: 10.5608, Time: 0.076s\n",
      "Epoch 2/5\n",
      "  Train Loss: 9.2666 (765 batches, avg batch time: 0.079s)\n",
      "  Val Loss: 9.0053 (validation time: 75.97s)\n",
      "  Epoch Time: 651.77s, Est. Remaining: 32.88 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.0053\n",
      "  Batch 10/765, Loss: 9.0920, Time: 0.075s\n",
      "  Batch 20/765, Loss: 10.2140, Time: 0.074s\n",
      "  Batch 30/765, Loss: 8.1233, Time: 0.076s\n",
      "  Batch 40/765, Loss: 9.2757, Time: 0.078s\n",
      "  Batch 50/765, Loss: 9.0282, Time: 0.075s\n",
      "  Batch 60/765, Loss: 11.3313, Time: 0.072s\n",
      "  Batch 70/765, Loss: 5.9685, Time: 0.049s\n",
      "  Batch 80/765, Loss: 11.0436, Time: 0.054s\n",
      "  Batch 90/765, Loss: 9.6116, Time: 0.076s\n",
      "  Batch 100/765, Loss: 8.6772, Time: 0.073s\n",
      "  Batch 110/765, Loss: 9.7554, Time: 0.074s\n",
      "  Batch 120/765, Loss: 10.1437, Time: 0.075s\n",
      "  Batch 130/765, Loss: 11.4951, Time: 0.211s\n",
      "  Batch 140/765, Loss: 9.3810, Time: 0.076s\n",
      "  Batch 150/765, Loss: 9.4553, Time: 0.077s\n",
      "  Batch 160/765, Loss: 8.6359, Time: 0.049s\n",
      "  Batch 170/765, Loss: 11.4463, Time: 0.078s\n",
      "  Batch 180/765, Loss: 9.1161, Time: 0.075s\n",
      "  Batch 190/765, Loss: 10.2753, Time: 0.080s\n",
      "  Batch 200/765, Loss: 8.8772, Time: 0.075s\n",
      "  Batch 210/765, Loss: 9.6692, Time: 0.076s\n",
      "  Batch 220/765, Loss: 9.4622, Time: 0.075s\n",
      "  Batch 230/765, Loss: 9.4284, Time: 0.079s\n",
      "  Batch 240/765, Loss: 10.3889, Time: 0.114s\n",
      "  Batch 250/765, Loss: 10.0956, Time: 0.076s\n",
      "  Batch 260/765, Loss: 8.6142, Time: 0.096s\n",
      "  Batch 270/765, Loss: 8.3608, Time: 0.074s\n",
      "  Batch 280/765, Loss: 7.1612, Time: 0.112s\n",
      "  Batch 290/765, Loss: 11.3877, Time: 0.076s\n",
      "  Batch 300/765, Loss: 9.2210, Time: 0.074s\n",
      "  Batch 310/765, Loss: 7.6094, Time: 0.073s\n",
      "  Batch 320/765, Loss: 9.0894, Time: 0.086s\n",
      "  Batch 330/765, Loss: 8.8486, Time: 0.073s\n",
      "  Batch 340/765, Loss: 9.4630, Time: 0.076s\n",
      "  Batch 350/765, Loss: 9.3944, Time: 0.074s\n",
      "  Batch 360/765, Loss: 9.0181, Time: 0.073s\n",
      "  Batch 370/765, Loss: 9.2265, Time: 0.076s\n",
      "  Batch 380/765, Loss: 7.2792, Time: 0.074s\n",
      "  Batch 390/765, Loss: 8.6615, Time: 0.075s\n",
      "  Batch 400/765, Loss: 8.4337, Time: 0.080s\n",
      "  Batch 410/765, Loss: 10.9024, Time: 0.076s\n",
      "  Batch 420/765, Loss: 10.0179, Time: 0.075s\n",
      "  Batch 430/765, Loss: 11.0487, Time: 0.143s\n",
      "  Batch 440/765, Loss: 7.5878, Time: 0.049s\n",
      "  Batch 450/765, Loss: 8.3752, Time: 0.075s\n",
      "  Batch 460/765, Loss: 8.1096, Time: 0.074s\n",
      "  Batch 470/765, Loss: 6.4059, Time: 0.078s\n",
      "  Batch 480/765, Loss: 11.0115, Time: 0.076s\n",
      "  Batch 490/765, Loss: 10.1953, Time: 0.049s\n",
      "  Batch 500/765, Loss: 8.0141, Time: 0.073s\n",
      "  Batch 510/765, Loss: 6.8099, Time: 0.074s\n",
      "  Batch 520/765, Loss: 7.7654, Time: 0.078s\n",
      "  Batch 530/765, Loss: 10.0271, Time: 0.076s\n",
      "  Batch 540/765, Loss: 10.2001, Time: 0.077s\n",
      "  Batch 550/765, Loss: 9.4380, Time: 0.075s\n",
      "  Batch 560/765, Loss: 7.9577, Time: 0.148s\n",
      "  Batch 570/765, Loss: 8.2034, Time: 0.074s\n",
      "  Batch 580/765, Loss: 10.8928, Time: 0.078s\n",
      "  Batch 590/765, Loss: 7.9875, Time: 0.077s\n",
      "  Batch 600/765, Loss: 11.6144, Time: 0.076s\n",
      "  Batch 610/765, Loss: 9.8450, Time: 0.083s\n",
      "  Batch 620/765, Loss: 8.6670, Time: 0.078s\n",
      "  Batch 630/765, Loss: 6.4478, Time: 0.076s\n",
      "  Batch 640/765, Loss: 9.4335, Time: 0.077s\n",
      "  Batch 650/765, Loss: 11.1480, Time: 0.052s\n",
      "  Batch 660/765, Loss: 11.1528, Time: 0.075s\n",
      "  Batch 670/765, Loss: 9.1298, Time: 0.083s\n",
      "  Batch 680/765, Loss: 7.5504, Time: 0.077s\n",
      "  Batch 690/765, Loss: 11.3972, Time: 0.075s\n",
      "  Batch 700/765, Loss: 7.5403, Time: 0.074s\n",
      "  Batch 710/765, Loss: 12.1493, Time: 0.084s\n",
      "  Batch 720/765, Loss: 9.6980, Time: 0.075s\n",
      "  Batch 730/765, Loss: 5.5391, Time: 0.178s\n",
      "  Batch 740/765, Loss: 8.9225, Time: 0.075s\n",
      "  Batch 750/765, Loss: 8.8669, Time: 0.075s\n",
      "  Batch 760/765, Loss: 8.4506, Time: 0.051s\n",
      "Epoch 3/5\n",
      "  Train Loss: 9.1328 (765 batches, avg batch time: 0.080s)\n",
      "  Val Loss: 8.8502 (validation time: 76.06s)\n",
      "  Epoch Time: 654.46s, Est. Remaining: 21.89 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 8.8502\n",
      "  Batch 10/765, Loss: 9.9718, Time: 0.074s\n",
      "  Batch 20/765, Loss: 9.2262, Time: 0.074s\n",
      "  Batch 30/765, Loss: 10.3007, Time: 0.078s\n",
      "  Batch 40/765, Loss: 7.3678, Time: 0.076s\n",
      "  Batch 50/765, Loss: 8.4314, Time: 0.079s\n",
      "  Batch 60/765, Loss: 8.8301, Time: 0.049s\n",
      "  Batch 70/765, Loss: 11.6185, Time: 0.074s\n",
      "  Batch 80/765, Loss: 8.9708, Time: 0.075s\n",
      "  Batch 90/765, Loss: 11.0978, Time: 0.074s\n",
      "  Batch 100/765, Loss: 9.4143, Time: 0.083s\n",
      "  Batch 110/765, Loss: 9.3115, Time: 0.078s\n",
      "  Batch 120/765, Loss: 8.2292, Time: 0.207s\n",
      "  Batch 130/765, Loss: 10.5146, Time: 0.075s\n",
      "  Batch 140/765, Loss: 9.9436, Time: 0.077s\n",
      "  Batch 150/765, Loss: 7.3644, Time: 0.076s\n",
      "  Batch 160/765, Loss: 7.2540, Time: 0.076s\n",
      "  Batch 170/765, Loss: 10.4140, Time: 0.079s\n",
      "  Batch 180/765, Loss: 6.1258, Time: 0.116s\n",
      "  Batch 190/765, Loss: 8.8864, Time: 0.073s\n",
      "  Batch 200/765, Loss: 9.8066, Time: 0.078s\n",
      "  Batch 210/765, Loss: 8.0989, Time: 0.076s\n",
      "  Batch 220/765, Loss: 7.9249, Time: 0.073s\n",
      "  Batch 230/765, Loss: 8.6467, Time: 0.073s\n",
      "  Batch 240/765, Loss: 9.3600, Time: 0.077s\n",
      "  Batch 250/765, Loss: 9.1577, Time: 0.075s\n",
      "  Batch 260/765, Loss: 8.1913, Time: 0.078s\n",
      "  Batch 270/765, Loss: 10.4520, Time: 0.050s\n",
      "  Batch 280/765, Loss: 8.1243, Time: 0.048s\n",
      "  Batch 290/765, Loss: 7.7551, Time: 0.076s\n",
      "  Batch 300/765, Loss: 9.0440, Time: 0.106s\n",
      "  Batch 310/765, Loss: 8.0239, Time: 0.076s\n",
      "  Batch 320/765, Loss: 9.6571, Time: 0.076s\n",
      "  Batch 330/765, Loss: 9.7928, Time: 0.075s\n",
      "  Batch 340/765, Loss: 11.0603, Time: 0.074s\n",
      "  Batch 350/765, Loss: 8.8220, Time: 0.075s\n",
      "  Batch 360/765, Loss: 9.6078, Time: 0.074s\n",
      "  Batch 370/765, Loss: 6.3589, Time: 0.076s\n",
      "  Batch 380/765, Loss: 9.7610, Time: 0.051s\n",
      "  Batch 390/765, Loss: 13.1264, Time: 0.079s\n",
      "  Batch 400/765, Loss: 10.2573, Time: 0.076s\n",
      "  Batch 410/765, Loss: 13.3103, Time: 0.074s\n",
      "  Batch 420/765, Loss: 11.4708, Time: 0.091s\n",
      "  Batch 430/765, Loss: 10.6098, Time: 0.077s\n",
      "  Batch 440/765, Loss: 7.3198, Time: 0.073s\n",
      "  Batch 450/765, Loss: 7.7991, Time: 0.077s\n",
      "  Batch 460/765, Loss: 7.8794, Time: 0.049s\n",
      "  Batch 470/765, Loss: 8.6162, Time: 0.076s\n",
      "  Batch 480/765, Loss: 9.9938, Time: 0.080s\n",
      "  Batch 490/765, Loss: 8.9689, Time: 0.075s\n",
      "  Batch 500/765, Loss: 9.8021, Time: 0.077s\n",
      "  Batch 510/765, Loss: 9.6808, Time: 0.079s\n",
      "  Batch 520/765, Loss: 10.4804, Time: 0.078s\n",
      "  Batch 530/765, Loss: 8.8929, Time: 0.076s\n",
      "  Batch 540/765, Loss: 8.9971, Time: 0.077s\n",
      "  Batch 550/765, Loss: 8.7990, Time: 0.080s\n",
      "  Batch 560/765, Loss: 8.1391, Time: 0.075s\n",
      "  Batch 570/765, Loss: 9.1378, Time: 0.075s\n",
      "  Batch 580/765, Loss: 7.6254, Time: 0.051s\n",
      "  Batch 590/765, Loss: 9.8801, Time: 0.092s\n",
      "  Batch 600/765, Loss: 10.2524, Time: 0.079s\n",
      "  Batch 610/765, Loss: 8.3931, Time: 0.074s\n",
      "  Batch 620/765, Loss: 6.8872, Time: 0.074s\n",
      "  Batch 630/765, Loss: 7.5930, Time: 0.119s\n",
      "  Batch 640/765, Loss: 8.0490, Time: 0.076s\n",
      "  Batch 650/765, Loss: 7.4894, Time: 0.074s\n",
      "  Batch 660/765, Loss: 8.3134, Time: 0.076s\n",
      "  Batch 670/765, Loss: 7.6330, Time: 0.074s\n",
      "  Batch 680/765, Loss: 8.2034, Time: 0.075s\n",
      "  Batch 690/765, Loss: 9.2573, Time: 0.074s\n",
      "  Batch 700/765, Loss: 6.4354, Time: 0.074s\n",
      "  Batch 710/765, Loss: 10.9258, Time: 0.086s\n",
      "  Batch 720/765, Loss: 8.5213, Time: 0.076s\n",
      "  Batch 730/765, Loss: 8.7701, Time: 0.099s\n",
      "  Batch 740/765, Loss: 10.2239, Time: 0.077s\n",
      "  Batch 750/765, Loss: 7.1785, Time: 0.080s\n",
      "  Batch 760/765, Loss: 6.7540, Time: 0.083s\n",
      "Epoch 4/5\n",
      "  Train Loss: 9.1648 (765 batches, avg batch time: 0.080s)\n",
      "  Val Loss: 8.8777 (validation time: 77.98s)\n",
      "  Epoch Time: 662.55s, Est. Remaining: 10.97 minutes\n",
      "  No improvement for 1/2 epochs\n",
      "  Batch 10/765, Loss: 8.5521, Time: 0.085s\n",
      "  Batch 20/765, Loss: 9.0291, Time: 0.089s\n",
      "  Batch 30/765, Loss: 8.2833, Time: 0.066s\n",
      "  Batch 40/765, Loss: 9.8088, Time: 0.075s\n",
      "  Batch 50/765, Loss: 7.9477, Time: 0.076s\n",
      "  Batch 60/765, Loss: 11.2969, Time: 0.075s\n",
      "  Batch 70/765, Loss: 10.3669, Time: 0.074s\n",
      "  Batch 80/765, Loss: 7.0863, Time: 0.075s\n",
      "  Batch 90/765, Loss: 9.2913, Time: 0.074s\n",
      "  Batch 100/765, Loss: 7.9549, Time: 0.075s\n",
      "  Batch 110/765, Loss: 9.3773, Time: 0.075s\n",
      "  Batch 120/765, Loss: 7.7609, Time: 0.075s\n",
      "  Batch 130/765, Loss: 10.6744, Time: 0.078s\n",
      "  Batch 140/765, Loss: 10.2164, Time: 0.090s\n",
      "  Batch 150/765, Loss: 8.3169, Time: 0.075s\n",
      "  Batch 160/765, Loss: 9.0309, Time: 0.074s\n",
      "  Batch 170/765, Loss: 9.0635, Time: 0.075s\n",
      "  Batch 180/765, Loss: 7.5668, Time: 0.075s\n",
      "  Batch 190/765, Loss: 9.3139, Time: 0.052s\n",
      "  Batch 200/765, Loss: 6.9490, Time: 0.077s\n",
      "  Batch 210/765, Loss: 8.7856, Time: 0.074s\n",
      "  Batch 220/765, Loss: 10.4496, Time: 0.083s\n",
      "  Batch 230/765, Loss: 9.3887, Time: 0.076s\n",
      "  Batch 240/765, Loss: 8.8122, Time: 0.076s\n",
      "  Batch 250/765, Loss: 8.2806, Time: 0.074s\n",
      "  Batch 260/765, Loss: 8.6923, Time: 0.075s\n",
      "  Batch 270/765, Loss: 8.0286, Time: 0.074s\n",
      "  Batch 280/765, Loss: 8.3214, Time: 0.079s\n",
      "  Batch 290/765, Loss: 8.9448, Time: 0.074s\n",
      "  Batch 300/765, Loss: 8.4798, Time: 0.049s\n",
      "  Batch 310/765, Loss: 12.3334, Time: 0.074s\n",
      "  Batch 320/765, Loss: 11.7276, Time: 0.074s\n",
      "  Batch 330/765, Loss: 8.9182, Time: 0.075s\n",
      "  Batch 340/765, Loss: 10.2920, Time: 0.075s\n",
      "  Batch 350/765, Loss: 12.5119, Time: 0.075s\n",
      "  Batch 360/765, Loss: 9.4120, Time: 0.074s\n",
      "  Batch 370/765, Loss: 10.2653, Time: 0.085s\n",
      "  Batch 380/765, Loss: 9.1302, Time: 0.074s\n",
      "  Batch 390/765, Loss: 10.3924, Time: 0.075s\n",
      "  Batch 400/765, Loss: 7.4458, Time: 0.074s\n",
      "  Batch 410/765, Loss: 8.4245, Time: 0.073s\n",
      "  Batch 420/765, Loss: 9.7507, Time: 0.075s\n",
      "  Batch 430/765, Loss: 11.2696, Time: 0.076s\n",
      "  Batch 440/765, Loss: 7.0593, Time: 0.201s\n",
      "  Batch 450/765, Loss: 9.2542, Time: 0.075s\n",
      "  Batch 460/765, Loss: 9.3179, Time: 0.077s\n",
      "  Batch 470/765, Loss: 9.1306, Time: 0.074s\n",
      "  Batch 480/765, Loss: 8.4834, Time: 0.074s\n",
      "  Batch 490/765, Loss: 7.5505, Time: 0.083s\n",
      "  Batch 500/765, Loss: 9.5703, Time: 0.076s\n",
      "  Batch 510/765, Loss: 8.1920, Time: 0.074s\n",
      "  Batch 520/765, Loss: 10.3081, Time: 0.050s\n",
      "  Batch 530/765, Loss: 7.7799, Time: 0.074s\n",
      "  Batch 540/765, Loss: 8.0439, Time: 0.076s\n",
      "  Batch 550/765, Loss: 9.2680, Time: 0.075s\n",
      "  Batch 560/765, Loss: 10.1416, Time: 0.075s\n",
      "  Batch 570/765, Loss: 10.3473, Time: 0.074s\n",
      "  Batch 580/765, Loss: 8.3638, Time: 0.076s\n",
      "  Batch 590/765, Loss: 10.9950, Time: 0.075s\n",
      "  Batch 600/765, Loss: 9.4354, Time: 0.073s\n",
      "  Batch 610/765, Loss: 10.0765, Time: 0.075s\n",
      "  Batch 620/765, Loss: 8.7745, Time: 0.077s\n",
      "  Batch 630/765, Loss: 8.4498, Time: 0.075s\n",
      "  Batch 640/765, Loss: 8.8330, Time: 0.074s\n",
      "  Batch 650/765, Loss: 8.8176, Time: 0.076s\n",
      "  Batch 660/765, Loss: 9.9015, Time: 0.076s\n",
      "  Batch 670/765, Loss: 7.2821, Time: 0.076s\n",
      "  Batch 680/765, Loss: 11.4710, Time: 0.074s\n",
      "  Batch 690/765, Loss: 9.1444, Time: 0.075s\n",
      "  Batch 700/765, Loss: 7.8851, Time: 0.080s\n",
      "  Batch 710/765, Loss: 6.1676, Time: 0.075s\n",
      "  Batch 720/765, Loss: 9.4555, Time: 0.077s\n",
      "  Batch 730/765, Loss: 8.8088, Time: 0.074s\n",
      "  Batch 740/765, Loss: 8.9874, Time: 0.074s\n",
      "  Batch 750/765, Loss: 7.1527, Time: 0.050s\n",
      "  Batch 760/765, Loss: 7.4814, Time: 0.075s\n",
      "Epoch 5/5\n",
      "  Train Loss: 9.0506 (765 batches, avg batch time: 0.078s)\n",
      "  Val Loss: 8.9056 (validation time: 75.59s)\n",
      "  Epoch Time: 654.16s, Est. Remaining: 0.00 minutes\n",
      "  No improvement for 2/2 epochs\n",
      "Early stopping at epoch 5\n",
      "Training completed in 54.78 minutes (3286.7 seconds)\n",
      "Loaded best model (version single_output_v1)\n",
      "Params: [2.29677418e+01 6.35056601e+01 2.04272411e-02 2.79373497e+01\n",
      " 2.05715977e+00], Validation MAPE: 13.5584%\n",
      "Model architecture has changed - detected parameter size mismatch.\n",
      "Removing incompatible checkpoint and starting fresh training.\n",
      "Starting training for 5 epochs with patience 2...\n",
      "  Batch 10/765, Loss: 100.5721, Time: 0.094s\n",
      "  Batch 20/765, Loss: 34.6541, Time: 0.079s\n",
      "  Batch 30/765, Loss: 32.6211, Time: 0.068s\n",
      "  Batch 40/765, Loss: 21.7688, Time: 0.072s\n",
      "  Batch 50/765, Loss: 15.0459, Time: 0.080s\n",
      "  Batch 60/765, Loss: 26.8934, Time: 0.072s\n",
      "  Batch 70/765, Loss: 29.9735, Time: 0.078s\n",
      "  Batch 80/765, Loss: 24.5138, Time: 0.071s\n",
      "  Batch 90/765, Loss: 26.4800, Time: 0.075s\n",
      "  Batch 100/765, Loss: 21.5311, Time: 0.089s\n",
      "  Batch 110/765, Loss: 26.1330, Time: 0.074s\n",
      "  Batch 120/765, Loss: 22.1820, Time: 0.070s\n",
      "  Batch 130/765, Loss: 20.0499, Time: 0.045s\n",
      "  Batch 140/765, Loss: 17.5519, Time: 0.071s\n",
      "  Batch 150/765, Loss: 10.6652, Time: 0.072s\n",
      "  Batch 160/765, Loss: 12.4776, Time: 0.071s\n",
      "  Batch 170/765, Loss: 10.4776, Time: 0.070s\n",
      "  Batch 180/765, Loss: 10.9551, Time: 0.082s\n",
      "  Batch 190/765, Loss: 11.1330, Time: 0.074s\n",
      "  Batch 200/765, Loss: 15.2021, Time: 0.102s\n",
      "  Batch 210/765, Loss: 10.2178, Time: 0.078s\n",
      "  Batch 220/765, Loss: 13.6555, Time: 0.072s\n",
      "  Batch 230/765, Loss: 12.6000, Time: 0.049s\n",
      "  Batch 240/765, Loss: 11.5569, Time: 0.071s\n",
      "  Batch 250/765, Loss: 10.8484, Time: 0.148s\n",
      "  Batch 260/765, Loss: 11.9629, Time: 0.072s\n",
      "  Batch 270/765, Loss: 8.1666, Time: 0.073s\n",
      "  Batch 280/765, Loss: 8.8328, Time: 0.071s\n",
      "  Batch 290/765, Loss: 10.6021, Time: 0.071s\n",
      "  Batch 300/765, Loss: 9.4220, Time: 0.072s\n",
      "  Batch 310/765, Loss: 9.2713, Time: 0.047s\n",
      "  Batch 320/765, Loss: 7.6086, Time: 0.072s\n",
      "  Batch 330/765, Loss: 9.7735, Time: 0.071s\n",
      "  Batch 340/765, Loss: 11.0354, Time: 0.069s\n",
      "  Batch 350/765, Loss: 10.2986, Time: 0.065s\n",
      "  Batch 360/765, Loss: 8.5028, Time: 0.071s\n",
      "  Batch 370/765, Loss: 12.7941, Time: 0.075s\n",
      "  Batch 380/765, Loss: 11.5982, Time: 0.072s\n",
      "  Batch 390/765, Loss: 11.3686, Time: 0.070s\n",
      "  Batch 400/765, Loss: 7.9941, Time: 0.073s\n",
      "  Batch 410/765, Loss: 9.2013, Time: 0.072s\n",
      "  Batch 420/765, Loss: 11.9144, Time: 0.073s\n",
      "  Batch 430/765, Loss: 10.2637, Time: 0.071s\n",
      "  Batch 440/765, Loss: 10.2546, Time: 0.072s\n",
      "  Batch 450/765, Loss: 9.3648, Time: 0.071s\n",
      "  Batch 460/765, Loss: 12.4738, Time: 0.072s\n",
      "  Batch 470/765, Loss: 12.4525, Time: 0.097s\n",
      "  Batch 480/765, Loss: 12.0429, Time: 0.070s\n",
      "  Batch 490/765, Loss: 8.5578, Time: 0.071s\n",
      "  Batch 500/765, Loss: 8.9602, Time: 0.074s\n",
      "  Batch 510/765, Loss: 10.1592, Time: 0.076s\n",
      "  Batch 520/765, Loss: 9.3718, Time: 0.235s\n",
      "  Batch 530/765, Loss: 12.6044, Time: 0.047s\n",
      "  Batch 540/765, Loss: 11.0792, Time: 0.071s\n",
      "  Batch 550/765, Loss: 8.1622, Time: 0.071s\n",
      "  Batch 560/765, Loss: 9.6057, Time: 0.069s\n",
      "  Batch 570/765, Loss: 7.7961, Time: 0.069s\n",
      "  Batch 580/765, Loss: 10.2883, Time: 0.045s\n",
      "  Batch 590/765, Loss: 9.2413, Time: 0.071s\n",
      "  Batch 600/765, Loss: 9.7798, Time: 0.070s\n",
      "  Batch 610/765, Loss: 7.4616, Time: 0.072s\n",
      "  Batch 620/765, Loss: 7.2525, Time: 0.069s\n",
      "  Batch 630/765, Loss: 6.4222, Time: 0.071s\n",
      "  Batch 640/765, Loss: 8.1128, Time: 0.069s\n",
      "  Batch 650/765, Loss: 9.2899, Time: 0.103s\n",
      "  Batch 660/765, Loss: 8.7319, Time: 0.071s\n",
      "  Batch 670/765, Loss: 10.5969, Time: 0.071s\n",
      "  Batch 680/765, Loss: 9.2922, Time: 0.070s\n",
      "  Batch 690/765, Loss: 9.7540, Time: 0.048s\n",
      "  Batch 700/765, Loss: 8.8823, Time: 0.072s\n",
      "  Batch 710/765, Loss: 12.1866, Time: 0.069s\n",
      "  Batch 720/765, Loss: 8.2750, Time: 0.069s\n",
      "  Batch 730/765, Loss: 8.4429, Time: 0.069s\n",
      "  Batch 740/765, Loss: 11.0929, Time: 0.071s\n",
      "  Batch 750/765, Loss: 10.7221, Time: 0.050s\n",
      "  Batch 760/765, Loss: 9.8206, Time: 0.071s\n",
      "Epoch 1/5\n",
      "  Train Loss: 15.0046 (765 batches, avg batch time: 0.074s)\n",
      "  Val Loss: 9.0210 (validation time: 75.37s)\n",
      "  Epoch Time: 665.85s, Est. Remaining: 44.39 minutes\n",
      "  Saved best model (version single_output_v1) with val loss: 9.0210\n",
      "  Batch 10/765, Loss: 9.4016, Time: 0.072s\n",
      "  Batch 20/765, Loss: 12.4118, Time: 0.045s\n",
      "  Batch 30/765, Loss: 8.5826, Time: 0.069s\n",
      "  Batch 40/765, Loss: 7.3645, Time: 0.069s\n",
      "  Batch 50/765, Loss: 8.9906, Time: 0.070s\n",
      "  Batch 60/765, Loss: 8.9096, Time: 0.070s\n",
      "  Batch 70/765, Loss: 9.8397, Time: 0.069s\n",
      "  Batch 80/765, Loss: 9.4423, Time: 0.071s\n",
      "  Batch 90/765, Loss: 10.6277, Time: 0.045s\n",
      "  Batch 100/765, Loss: 9.5722, Time: 0.072s\n",
      "  Batch 110/765, Loss: 10.4376, Time: 0.069s\n",
      "  Batch 120/765, Loss: 8.2886, Time: 0.071s\n",
      "  Batch 130/765, Loss: 8.4165, Time: 0.085s\n",
      "  Batch 140/765, Loss: 8.7464, Time: 0.069s\n",
      "  Batch 150/765, Loss: 9.4870, Time: 0.070s\n",
      "  Batch 160/765, Loss: 9.8824, Time: 0.071s\n",
      "  Batch 170/765, Loss: 7.6594, Time: 0.071s\n",
      "  Batch 180/765, Loss: 8.1729, Time: 0.080s\n",
      "  Batch 190/765, Loss: 8.6313, Time: 0.077s\n",
      "  Batch 200/765, Loss: 8.8754, Time: 0.072s\n",
      "  Batch 210/765, Loss: 9.4001, Time: 0.074s\n",
      "  Batch 220/765, Loss: 9.6313, Time: 0.091s\n",
      "  Batch 230/765, Loss: 8.6791, Time: 0.070s\n",
      "  Batch 240/765, Loss: 9.4108, Time: 0.070s\n",
      "  Batch 250/765, Loss: 9.1519, Time: 0.071s\n",
      "  Batch 260/765, Loss: 7.5312, Time: 0.080s\n",
      "  Batch 270/765, Loss: 8.4183, Time: 0.070s\n",
      "  Batch 280/765, Loss: 8.0670, Time: 0.071s\n",
      "  Batch 290/765, Loss: 8.8283, Time: 0.070s\n",
      "  Batch 300/765, Loss: 9.6744, Time: 0.074s\n",
      "  Batch 310/765, Loss: 11.5785, Time: 0.086s\n",
      "  Batch 320/765, Loss: 8.8153, Time: 0.070s\n",
      "  Batch 330/765, Loss: 8.7979, Time: 0.072s\n",
      "  Batch 340/765, Loss: 9.7363, Time: 0.075s\n",
      "  Batch 350/765, Loss: 12.9320, Time: 0.070s\n",
      "  Batch 360/765, Loss: 11.6953, Time: 0.070s\n",
      "  Batch 370/765, Loss: 8.4078, Time: 0.069s\n",
      "  Batch 380/765, Loss: 8.8808, Time: 0.070s\n",
      "  Batch 390/765, Loss: 10.7803, Time: 0.071s\n",
      "  Batch 400/765, Loss: 10.7721, Time: 0.071s\n",
      "  Batch 410/765, Loss: 10.3127, Time: 0.070s\n",
      "  Batch 420/765, Loss: 7.2313, Time: 0.070s\n",
      "  Batch 430/765, Loss: 11.8235, Time: 0.072s\n",
      "  Batch 440/765, Loss: 10.5455, Time: 0.095s\n",
      "  Batch 450/765, Loss: 9.1834, Time: 0.070s\n",
      "  Batch 460/765, Loss: 6.8297, Time: 0.071s\n",
      "  Batch 470/765, Loss: 10.4161, Time: 0.078s\n",
      "  Batch 480/765, Loss: 7.6050, Time: 0.074s\n",
      "  Batch 490/765, Loss: 9.7806, Time: 0.071s\n",
      "  Batch 500/765, Loss: 6.5401, Time: 0.072s\n",
      "  Batch 510/765, Loss: 8.3742, Time: 0.070s\n",
      "  Batch 520/765, Loss: 9.6879, Time: 0.076s\n",
      "  Batch 530/765, Loss: 9.5587, Time: 0.073s\n",
      "  Batch 540/765, Loss: 9.5603, Time: 0.072s\n",
      "  Batch 550/765, Loss: 9.1580, Time: 0.070s\n",
      "  Batch 560/765, Loss: 9.4925, Time: 0.069s\n",
      "  Batch 570/765, Loss: 9.8503, Time: 0.069s\n",
      "  Batch 580/765, Loss: 9.9259, Time: 0.077s\n",
      "  Batch 590/765, Loss: 8.2517, Time: 0.070s\n",
      "  Batch 600/765, Loss: 9.7610, Time: 0.071s\n",
      "  Batch 610/765, Loss: 8.5891, Time: 0.075s\n",
      "  Batch 620/765, Loss: 10.7263, Time: 0.071s\n",
      "  Batch 630/765, Loss: 9.1325, Time: 0.070s\n",
      "  Batch 640/765, Loss: 7.4625, Time: 0.072s\n",
      "  Batch 650/765, Loss: 9.5241, Time: 0.070s\n",
      "  Batch 660/765, Loss: 8.5624, Time: 0.070s\n",
      "  Batch 670/765, Loss: 9.0321, Time: 0.071s\n",
      "  Batch 680/765, Loss: 6.9542, Time: 0.082s\n",
      "  Batch 690/765, Loss: 8.8222, Time: 0.069s\n",
      "  Batch 700/765, Loss: 8.9713, Time: 0.070s\n",
      "  Batch 710/765, Loss: 10.4947, Time: 0.096s\n",
      "  Batch 720/765, Loss: 10.2704, Time: 0.076s\n",
      "  Batch 730/765, Loss: 8.6052, Time: 0.092s\n",
      "  Batch 740/765, Loss: 9.6996, Time: 0.049s\n",
      "  Batch 750/765, Loss: 9.1463, Time: 0.069s\n",
      "  Batch 760/765, Loss: 6.4840, Time: 0.075s\n",
      "Epoch 2/5\n",
      "  Train Loss: 9.1614 (765 batches, avg batch time: 0.073s)\n",
      "  Val Loss: 9.0974 (validation time: 75.43s)\n",
      "  Epoch Time: 653.11s, Est. Remaining: 32.97 minutes\n",
      "  No improvement for 1/2 epochs\n",
      "  Batch 10/765, Loss: 6.9560, Time: 0.084s\n",
      "  Batch 20/765, Loss: 8.1346, Time: 0.087s\n",
      "  Batch 30/765, Loss: 8.4381, Time: 0.091s\n",
      "  Batch 40/765, Loss: 8.7613, Time: 0.070s\n",
      "  Batch 50/765, Loss: 11.7260, Time: 0.069s\n",
      "  Batch 60/765, Loss: 10.2197, Time: 0.071s\n",
      "  Batch 70/765, Loss: 10.5285, Time: 0.070s\n",
      "  Batch 80/765, Loss: 9.2787, Time: 0.069s\n",
      "  Batch 90/765, Loss: 9.3702, Time: 0.073s\n",
      "  Batch 100/765, Loss: 7.2272, Time: 0.070s\n",
      "  Batch 110/765, Loss: 8.7319, Time: 0.070s\n",
      "  Batch 120/765, Loss: 5.4880, Time: 0.073s\n",
      "  Batch 130/765, Loss: 10.5120, Time: 0.070s\n",
      "  Batch 140/765, Loss: 9.7754, Time: 0.070s\n",
      "  Batch 150/765, Loss: 8.9621, Time: 0.072s\n",
      "  Batch 160/765, Loss: 10.1390, Time: 0.074s\n",
      "  Batch 170/765, Loss: 10.1435, Time: 0.073s\n",
      "  Batch 180/765, Loss: 10.1000, Time: 0.093s\n",
      "  Batch 190/765, Loss: 6.9927, Time: 0.074s\n",
      "  Batch 200/765, Loss: 8.9676, Time: 0.080s\n",
      "  Batch 210/765, Loss: 9.7428, Time: 0.092s\n",
      "  Batch 220/765, Loss: 9.3460, Time: 0.068s\n",
      "  Batch 230/765, Loss: 12.3899, Time: 0.077s\n",
      "  Batch 240/765, Loss: 9.3954, Time: 0.047s\n",
      "  Batch 250/765, Loss: 7.4800, Time: 0.071s\n",
      "  Batch 260/765, Loss: 11.2637, Time: 0.069s\n",
      "  Batch 270/765, Loss: 7.5959, Time: 0.071s\n",
      "  Batch 280/765, Loss: 7.6837, Time: 0.080s\n",
      "  Batch 290/765, Loss: 9.4190, Time: 0.070s\n",
      "  Batch 300/765, Loss: 7.9646, Time: 0.077s\n",
      "  Batch 310/765, Loss: 10.0129, Time: 0.071s\n",
      "  Batch 320/765, Loss: 11.2685, Time: 0.069s\n",
      "  Batch 330/765, Loss: 11.1956, Time: 0.047s\n",
      "  Batch 340/765, Loss: 7.2108, Time: 0.071s\n",
      "  Batch 350/765, Loss: 7.1645, Time: 0.120s\n",
      "  Batch 360/765, Loss: 8.4668, Time: 0.070s\n",
      "  Batch 370/765, Loss: 6.2412, Time: 0.080s\n",
      "  Batch 380/765, Loss: 9.6079, Time: 0.069s\n",
      "  Batch 390/765, Loss: 9.3723, Time: 0.072s\n",
      "  Batch 400/765, Loss: 11.8302, Time: 0.070s\n",
      "  Batch 410/765, Loss: 7.6980, Time: 0.071s\n",
      "  Batch 420/765, Loss: 9.5135, Time: 0.048s\n",
      "  Batch 430/765, Loss: 10.2681, Time: 0.070s\n",
      "  Batch 440/765, Loss: 7.0344, Time: 0.068s\n",
      "  Batch 450/765, Loss: 10.6929, Time: 0.070s\n",
      "  Batch 460/765, Loss: 8.8941, Time: 0.069s\n",
      "  Batch 470/765, Loss: 10.7118, Time: 0.045s\n",
      "  Batch 480/765, Loss: 7.0806, Time: 0.202s\n",
      "  Batch 490/765, Loss: 10.0003, Time: 0.071s\n",
      "  Batch 500/765, Loss: 5.9445, Time: 0.070s\n",
      "  Batch 510/765, Loss: 7.0888, Time: 0.071s\n",
      "  Batch 520/765, Loss: 8.9042, Time: 0.070s\n",
      "  Batch 530/765, Loss: 10.2103, Time: 0.070s\n",
      "  Batch 540/765, Loss: 12.8120, Time: 0.112s\n",
      "  Batch 550/765, Loss: 7.4817, Time: 0.069s\n",
      "  Batch 560/765, Loss: 9.3271, Time: 0.073s\n",
      "  Batch 570/765, Loss: 8.1466, Time: 0.070s\n",
      "  Batch 580/765, Loss: 7.4625, Time: 0.129s\n",
      "  Batch 590/765, Loss: 9.9482, Time: 0.070s\n",
      "  Batch 600/765, Loss: 9.3120, Time: 0.075s\n",
      "  Batch 610/765, Loss: 10.1008, Time: 0.083s\n",
      "  Batch 620/765, Loss: 9.8009, Time: 0.070s\n",
      "  Batch 630/765, Loss: 9.3656, Time: 0.081s\n",
      "  Batch 640/765, Loss: 8.6155, Time: 0.073s\n",
      "  Batch 650/765, Loss: 8.6663, Time: 0.070s\n",
      "  Batch 660/765, Loss: 7.7166, Time: 0.070s\n",
      "  Batch 670/765, Loss: 7.9642, Time: 0.071s\n",
      "  Batch 680/765, Loss: 8.4812, Time: 0.069s\n",
      "  Batch 690/765, Loss: 8.5380, Time: 0.071s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 4: HYPERPARAMETER OPTIMIZATION\n",
    "# ============================================================\n",
    "\n",
    "# Run ADE optimization to find best hyperparameters\n",
    "print(\"Running ADE optimization for hyperparameters...\")\n",
    "best_params = optimize_hyperparameters(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    feature_cols=expanded_feature_cols\n",
    ")\n",
    "\n",
    "print(\"Optimized hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"- {param}: {value}\")\n",
    "    \n",
    "print(\"BLOCK 4 COMPLETED: Hyperparameter optimization successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BLOCK 5: MODEL TRAINING WITH OPTIMIZED PARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "# Now recreate data loaders with optimized batch size\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=1,  # Usually keep test batch size at 1 for detailed evaluation\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Create model with optimized parameters\n",
    "model = TemporalFusionTransformer(\n",
    "    num_features=len(expanded_feature_cols),\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_heads=1,  # Fixed as per requirements\n",
    "    dropout=0.1,\n",
    "    forecast_horizon=24,\n",
    "    hidden_layers=best_params['hidden_layers'] \n",
    ")\n",
    "\n",
    "# Train model with optimized parameters\n",
    "print(f\"Training optimized TFT model...\")\n",
    "model, train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    epochs=20,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "print(\"BLOCK 5 COMPLETED: Model training successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 6: LOSS VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss - LA Downtown')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('training_validation_loss.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"BLOCK 6 COMPLETED: Loss visualization successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 7: MODEL EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "# Evaluate model\n",
    "print(f\"Evaluating model on LA Downtown...\")\n",
    "rmse, mae, r2, station_metrics = evaluate_model(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    station_ids=all_stations,\n",
    "    regions=regions\n",
    ")\n",
    "\n",
    "print(f\"Overall metrics - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    regions=regions,\n",
    "    season=\"All Seasons - LA Downtown\"\n",
    ")\n",
    "\n",
    "print(\"BLOCK 7 COMPLETED: Model evaluation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 8: SEASONAL ANALYSIS - DATASET CREATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nCreating seasonal datasets...\")\n",
    "seasonal_datasets = create_seasonal_datasets(df, target_days, all_stations, feature_cols)\n",
    "\n",
    "# Visualize predictions for each season\n",
    "for season, season_loader in seasonal_datasets.items():\n",
    "    print(f\"Visualizing predictions for {season}...\")\n",
    "    visualize_predictions(\n",
    "        model=model,\n",
    "        data_loader=season_loader,\n",
    "        station_ids=all_stations,\n",
    "        regions=regions,\n",
    "        season=season\n",
    "    )\n",
    "\n",
    "print(\"BLOCK 8 COMPLETED: Seasonal datasets creation successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 9: SEASONAL PERFORMANCE EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "# Evaluate for each season\n",
    "seasonal_results = {}\n",
    "for season, season_loader in seasonal_datasets.items():\n",
    "    if len(season_loader) > 0:\n",
    "        print(f\"Evaluating model on {season} data...\")\n",
    "        rmse, mae, r2, _ = evaluate_model(\n",
    "            model=model,\n",
    "            data_loader=season_loader,\n",
    "            station_ids=all_stations,\n",
    "            regions=regions\n",
    "        )\n",
    "        seasonal_results[season] = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        }\n",
    "        print(f\"{season} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "print(\"BLOCK 9 COMPLETED: Seasonal performance evaluation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 10: SEASONAL MIRROR PLOTS\n",
    "# ============================================================\n",
    "\n",
    "# Function to unnormalize temperature data\n",
    "def unnormalize_temperature(temp_normalized, temp_min, temp_max):\n",
    "    \"\"\"\n",
    "    Convert normalized temperature back to original scale.\n",
    "    \"\"\"\n",
    "    return temp_normalized * (temp_max - temp_min) + temp_min\n",
    "\n",
    "# Get the temperature normalization parameters\n",
    "temp_min = df['Temperature_C'].min()\n",
    "temp_max = df['Temperature_C'].max()\n",
    "print(f\"Temperature normalization range: min={temp_min:.2f}, max={temp_max:.2f}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for season, seasonal_data in seasonal_datasets.items():\n",
    "    print(f\"Processing {season} forecast...\")\n",
    "    \n",
    "    try:\n",
    "        # This is a list with a single data point\n",
    "        (X_temporal, X_static), y = seasonal_data[0]\n",
    "        \n",
    "        # Move to device\n",
    "        X_temporal = X_temporal.to(device)\n",
    "        X_static = X_static.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model((X_temporal, X_static))\n",
    "        \n",
    "        # Move to CPU for plotting\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        y_np = y.numpy()\n",
    "        \n",
    "        # Unnormalize the temperature data\n",
    "        outputs_unnorm = unnormalize_temperature(outputs, temp_min, temp_max)\n",
    "        y_np_unnorm = unnormalize_temperature(y_np, temp_min, temp_max)\n",
    "        \n",
    "        # Create the mirror plot with unnormalized data\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "        \n",
    "        hours = np.arange(24)\n",
    "        \n",
    "        # Left plot - Actual temperatures\n",
    "        ax1.plot(hours, y_np_unnorm[0, 0, :], 'b-o', linewidth=2)\n",
    "        ax1.set_title(f\"Actual Temperature - {season}\", fontsize=14)\n",
    "        ax1.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax1.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "        ax1.set_xlim(0, 23)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.invert_xaxis()  # Invert x-axis for mirror effect\n",
    "        \n",
    "        # Right plot - Predicted temperatures\n",
    "        ax2.plot(hours, outputs_unnorm[0, 0, :], 'r-o', linewidth=2)\n",
    "        ax2.set_title(f\"Predicted Temperature - {season}\", fontsize=14)\n",
    "        ax2.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax2.set_xlim(0, 23)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add overall title\n",
    "        plt.suptitle(f\"{all_stations[0]} ({regions.get(all_stations[0], 'Unknown')}) - {season} Day Comparison\", \n",
    "                    fontsize=16, y=1.05)\n",
    "        \n",
    "        # Tight layout with minimal space between plots\n",
    "        fig.tight_layout()\n",
    "        plt.subplots_adjust(wspace=0.05)  # Reduce space between subplots\n",
    "        \n",
    "        # Add a legend to explain colors\n",
    "        ax1.plot([], [], 'b-o', label='Actual Temperature')\n",
    "        ax2.plot([], [], 'r-o', label='Predicted Temperature')\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "        \n",
    "        # Save and display\n",
    "        plt.savefig(f\"{season}_mirror_comparison.png\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {season} forecast: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"BLOCK 10 COMPLETED: Seasonal mirror plots created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 11: FINAL ANALYSIS AND RESULTS SAVING\n",
    "# ============================================================\n",
    "\n",
    "# Analyze seasonal performance\n",
    "if seasonal_results:\n",
    "    analyze_seasonal_performance(seasonal_results)\n",
    "\n",
    "print(\"Skipping topographic analysis since we only have data for LA Downtown\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame([{\n",
    "    'Station': 'LA Downtown',\n",
    "    'Region': 'urban',\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R2': r2\n",
    "}])\n",
    "\n",
    "results_df.to_csv('la_downtown_results.csv', index=False)\n",
    "print(\"Results saved to la_downtown_results.csv\")\n",
    "\n",
    "print(\"BLOCK 11 COMPLETED: Final analysis and results saving successful.\")\n",
    "print(\"=\" * 70)\n",
    "print(\"ALL BLOCKS EXECUTED SUCCESSFULLY! Analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7096681,
     "sourceId": 11342745,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7114409,
     "sourceId": 11365940,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7114641,
     "sourceId": 11366236,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7114978,
     "sourceId": 11366666,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7124793,
     "sourceId": 11379404,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
