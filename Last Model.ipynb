{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"GNN Model.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1pSO-VEyn5Cywjw9sXKn2fjXdVbI3hAsH\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Untitled2.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/193StgLnr4doKklAxwBiQsVX3njEfb1oa\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\hu4227mo-s\\OneDrive - Lund University\\Merged_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vmdpy\n",
    "from vmdpy import VMD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/vmdpy')  # Adjust the path if needed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Check for GPU and set device appropriately for Kaggle T4\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set optimized CUDA options for T4 GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False  # Better performance but less reproducible\n",
    "    # Set to float16 precision for faster computation on T4 GPU\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# ============================================================\n",
    "# MODIFIED DATA LOADING AND PROCESSING FOR TARGETED SEASONAL ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load weather data, clean missing values, and filter to San Jose only.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine file type and read\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    # Convert timestamp\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    elif 'DATE' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['DATE'])\n",
    "        df = df.rename(columns={'DATE': 'date_original'})\n",
    "\n",
    "    # Ensure time-based ordering before interpolation\n",
    "    df = df.sort_values(by='timestamp')\n",
    "\n",
    "    # Add hour of day feature - sine/cosine encoding for cyclical pattern\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "\n",
    "    # Add day of year feature - sine/cosine encoding for cyclical pattern\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n",
    "\n",
    "    # Define region mapping for LA Downtown only\n",
    "    region_mapping = {\n",
    "        'San Jose': 'urban'\n",
    "    }\n",
    "\n",
    "    # Filter to keep only LA Downtown\n",
    "    if 'station_id' in df.columns:\n",
    "        # Filter to LA Downtown only\n",
    "        df = df[df['station_id'] == 'San Jose']\n",
    "        \n",
    "        # Add region information\n",
    "        df['region'] = df['station_id'].map(region_mapping)\n",
    "\n",
    "        # Convert region to numerical encoding\n",
    "        region_to_num = {region: i for i, region in enumerate(df['region'].unique())}\n",
    "        df['region_code'] = df['region'].map(region_to_num)\n",
    "\n",
    "        # Add elevation for LA Downtown\n",
    "        elevation_mapping = {\n",
    "            'San Jose': 93         # meters\n",
    "        }\n",
    "        df['elevation'] = df['station_id'].map(elevation_mapping)\n",
    "        # Normalize elevation (will be constant for single station)\n",
    "        df['elevation_norm'] = 0.0  # Since we only have one station, just use 0 as normalized value\n",
    "\n",
    "        print(f\"Filtered data to San Jose only\")\n",
    "\n",
    "    # Interpolate missing values along the time dimension\n",
    "    df.interpolate(method='linear', limit_direction='both', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# VMD DECOMPOSITION\n",
    "# ============================================================\n",
    "\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def perform_vmd_decomposition(signal, alpha=2000, tau=0, K=10, DC=0, init=1, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Perform VMD decomposition on a signal.\n",
    "    \"\"\"\n",
    "    # Run VMD\n",
    "    u, u_hat, omega = VMD(signal, alpha, tau, K, DC, init, tol)\n",
    "    \n",
    "    # Calculate reconstruction residual\n",
    "    recon = np.sum(u, axis=0)\n",
    "    rres = np.sqrt(np.mean((signal - recon)**2)) / np.sqrt(np.mean(signal**2)) * 100\n",
    "    \n",
    "    return u, rres\n",
    "\n",
    "def find_optimal_k(signal, k_range=range(6, 15)):\n",
    "   \"\"\"\n",
    "   Find optimal K where rres < 3% with no sharp drop.\n",
    "   \"\"\"\n",
    "   results = []\n",
    "   \n",
    "   print(\"Starting VMD mode selection process...\")\n",
    "   print(f\"Target reconstruction error threshold: 3.0%\")\n",
    "   print(\"-\" * 40)\n",
    "   \n",
    "   for k in k_range:\n",
    "       print(f\"Testing K={k}\")\n",
    "       _, rres = perform_vmd_decomposition(signal, K=k)\n",
    "       results.append((k, rres))\n",
    "       print(f\"  K={k}, rres={rres:.2f}%\")\n",
    "       \n",
    "       if rres < 3.0:\n",
    "           print(f\"✓ Found optimal K={k} with rres={rres:.2f}% (below 3.0% threshold)\")\n",
    "           print(\"-\" * 40)\n",
    "           return k, rres\n",
    "   \n",
    "   # If no K achieves rres < 3%, return the K with lowest rres\n",
    "   best_k, best_rres = min(results, key=lambda x: x[1])\n",
    "   print(f\"! No K value achieved the target threshold.\")\n",
    "   print(f\"✓ Selected best K={best_k} with lowest rres={best_rres:.2f}%\")\n",
    "   print(\"-\" * 40)\n",
    "   return best_k, best_rres\n",
    "\n",
    "def decompose_station_data(data, feature_name, station_id, cache_dir='vmd_cache'):\n",
    "    \"\"\"\n",
    "    Decompose time series data for a single station.\n",
    "    \"\"\"\n",
    "    print(f\"Starting decomposition for {station_id} - {feature_name}\")\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a unique cache filename\n",
    "    cache_file = f\"{cache_dir}/vmd_{station_id}_{feature_name}.joblib\"\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading cached VMD decomposition for {station_id} - {feature_name}\")\n",
    "        return joblib.load(cache_file)\n",
    "    \n",
    "    # Get the time series to decompose\n",
    "    print(f\"  Extracting signal for {station_id}\")\n",
    "    signal = data[feature_name].values\n",
    "    \n",
    "    # Normalize signal\n",
    "    print(f\"  Normalizing signal\")\n",
    "    signal_norm = (signal - np.mean(signal)) / np.std(signal)\n",
    "    \n",
    "    # Find optimal K\n",
    "    print(f\"  Finding optimal K\")\n",
    "    k_opt, rres = find_optimal_k(signal_norm)\n",
    "    print(f\"Station {station_id}, feature {feature_name}: Optimal K={k_opt}, rres={rres:.2f}%\")\n",
    "    \n",
    "    # Perform VMD with optimal K\n",
    "    print(f\"  Performing VMD with K={k_opt}\")\n",
    "    u, _ = perform_vmd_decomposition(signal_norm, K=k_opt)\n",
    "    \n",
    "    # Denormalize modes\n",
    "    print(f\"  Denormalizing modes\")\n",
    "    u_denorm = u * np.std(signal) + np.mean(signal) / k_opt\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"  Saving to cache: {cache_file}\")\n",
    "    joblib.dump(u_denorm, cache_file)\n",
    "    \n",
    "    print(f\"Completed decomposition for {station_id} - {feature_name}\")\n",
    "    return u_denorm\n",
    "\n",
    "def parallel_vmd_decomposition(df, feature_cols_to_decompose=['Temperature_C'] ):\n",
    "    \"\"\"\n",
    "    Apply VMD decomposition to multiple stations serially to avoid multiprocessing issues.\n",
    "    \"\"\"\n",
    "    # Get the unique station IDs \n",
    "    station_ids = ['San Jose']  # We only have LA Downtown in this code\n",
    "    \n",
    "    decomposed_data = {}\n",
    "    \n",
    "    for feature in feature_cols_to_decompose:\n",
    "        print(f\"Performing VMD decomposition for feature: {feature}\")\n",
    "        \n",
    "        # Process each station serially instead of in parallel\n",
    "        results = []\n",
    "        for station_id in station_ids:\n",
    "            print(f\"  Processing station: {station_id}\")\n",
    "            result = decompose_station_data(df, feature, station_id)\n",
    "            results.append(result)\n",
    "            \n",
    "        # Store results\n",
    "        for i, station_id in enumerate(station_ids):\n",
    "            if station_id not in decomposed_data:\n",
    "                decomposed_data[station_id] = {}\n",
    "            decomposed_data[station_id][feature] = results[i]\n",
    "    \n",
    "    return decomposed_data\n",
    "\n",
    "def extract_target_days(df):\n",
    "    \"\"\"\n",
    "    Extract the 4 specific target days (one per season) for predictions\n",
    "    \"\"\"\n",
    "    # Define target days\n",
    "    target_days = [\n",
    "        {'season': 'Spring', 'month': 4, 'day': 15},  # April 15\n",
    "        {'season': 'Summer', 'month': 7, 'day': 20},  # July 20\n",
    "        {'season': 'Fall', 'month': 10, 'day': 10},   # October 10\n",
    "        {'season': 'Winter', 'month': 1, 'day': 15}   # January 15\n",
    "    ]\n",
    "\n",
    "    # Filter for each target day\n",
    "    target_data = {}\n",
    "    for target in target_days:\n",
    "        # Filter by month and day\n",
    "        day_data = df[(df['timestamp'].dt.month == target['month']) &\n",
    "                       (df['timestamp'].dt.day == target['day'])]\n",
    "\n",
    "        # Get the most recent year that has data for this day\n",
    "        if not day_data.empty:\n",
    "            latest_year = day_data['timestamp'].dt.year.max()\n",
    "            target_day_data = day_data[day_data['timestamp'].dt.year == latest_year]\n",
    "            target_data[target['season']] = target_day_data\n",
    "            print(f\"Found {len(target_day_data)} records for {target['season']} target day ({target['month']}/{target['day']}/{latest_year})\")\n",
    "        else:\n",
    "            print(f\"WARNING: No data found for {target['season']} target day\")\n",
    "\n",
    "    return target_data\n",
    "\n",
    "def prepare_seasonal_training_data(df, target_days):\n",
    "    \"\"\"\n",
    "    For each target day, prepare all historical data for training\n",
    "    \"\"\"\n",
    "    training_sets = {}\n",
    "\n",
    "    for season, target_day_data in target_days.items():\n",
    "        if target_day_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Get the date of this target\n",
    "        sample_date = target_day_data['timestamp'].iloc[0]\n",
    "        target_year = sample_date.year\n",
    "\n",
    "        # Use all historical data prior to the target year\n",
    "        historical_data = df[df['timestamp'].dt.year < target_year]\n",
    "\n",
    "        training_sets[season] = historical_data\n",
    "        print(f\"{season} training set: {len(historical_data)} samples from all historical data\")\n",
    "\n",
    "    return training_sets\n",
    "\n",
    "def split_data_temporal(df):\n",
    "    \"\"\"\n",
    "    Custom temporal split:\n",
    "    - Train: Jan 2021 – Dec 2022\n",
    "    - Validation: Jan 2023 – Sep 2023\n",
    "    - Test: Oct 2023 – Dec 2024\n",
    "    \"\"\"\n",
    "    print(\"Custom temporal splitting...\")\n",
    "\n",
    "    train_df = df[(df['timestamp'] >= '2021-01-01') & (df['timestamp'] < '2023-01-01')]\n",
    "    val_df = df[(df['timestamp'] >= '2023-01-01') & (df['timestamp'] < '2023-10-01')]\n",
    "    test_df = df[df['timestamp'] >= '2023-10-01']\n",
    "\n",
    "    print(f\"Training: {len(train_df)} samples\")\n",
    "    print(f\"Validation: {len(val_df)} samples\")\n",
    "    print(f\"Testing: {len(test_df)} samples\")\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def normalize_features(train_df, val_df, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalize features using StandardScaler fitted on training data\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit on training data\n",
    "    scaler.fit(train_df[feature_cols])\n",
    "\n",
    "    # Transform datasets\n",
    "    train_scaled = scaler.transform(train_df[feature_cols])\n",
    "    val_scaled = scaler.transform(val_df[feature_cols])\n",
    "\n",
    "    # Convert back to DataFrames\n",
    "    train_norm = pd.DataFrame(train_scaled, columns=feature_cols, index=train_df.index)\n",
    "    val_norm = pd.DataFrame(val_scaled, columns=feature_cols, index=val_df.index)\n",
    "\n",
    "    return train_norm, val_norm, scaler\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for weather forecasting with sliding window approach.\n",
    "    Modified to work efficiently with single station San Jose.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, station_ids, feature_cols, seq_length=24, forecast_horizon=24):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with weather data (filtered to San Jose only)\n",
    "            station_ids: List containing only 'San Jose'\n",
    "            feature_cols: List of feature columns to use as input\n",
    "            seq_length: Length of input sequence (in hours)\n",
    "            forecast_horizon: How many hours ahead to predict\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.station_ids = station_ids\n",
    "        self.feature_cols = feature_cols\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.n_stations = len(station_ids)  # Should be 1\n",
    "\n",
    "        # Get unique timestamps (now all from one station)\n",
    "        self.timestamps = sorted(df['timestamp'].unique())\n",
    "\n",
    "        # Filter valid timestamps (those that have enough history and future data)\n",
    "        valid_idx = []\n",
    "        for i in range(len(self.timestamps) - (seq_length + forecast_horizon - 1)):\n",
    "            # Check if we have continuous data for this window\n",
    "            current_time = self.timestamps[i]\n",
    "            end_time = self.timestamps[i + seq_length + forecast_horizon - 1]\n",
    "            expected_duration = timedelta(hours=seq_length + forecast_horizon - 1)\n",
    "\n",
    "            if (end_time - current_time) == expected_duration:\n",
    "                valid_idx.append(i)\n",
    "\n",
    "        self.valid_indices = valid_idx\n",
    "\n",
    "        # Add a safety check to ensure we have at least one valid window\n",
    "        if len(self.valid_indices) == 0:\n",
    "            print(f\"WARNING: No valid windows found in dataset. Using reduced requirements.\")\n",
    "            # Fall back to allowing any windows where we have both input and output data\n",
    "            valid_idx = []\n",
    "            for i in range(len(self.timestamps) - seq_length):\n",
    "                if i + seq_length < len(self.timestamps):\n",
    "                    valid_idx.append(i)\n",
    "            self.valid_indices = valid_idx\n",
    "            self.fallback_mode = True\n",
    "            print(f\"Found {len(self.valid_indices)} windows with relaxed continuity requirements\")\n",
    "        else:\n",
    "            self.fallback_mode = False\n",
    "            print(f\"Created dataset with {len(self.valid_indices)} valid windows\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.valid_indices))  # Ensure length is at least 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.valid_indices) == 0:\n",
    "            # Return dummy data if no valid indices\n",
    "            X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))\n",
    "            y = np.zeros((self.n_stations, self.forecast_horizon))\n",
    "            static_features = np.zeros((self.n_stations, 2))  # region_code and elevation\n",
    "            return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n",
    "\n",
    "        # Get actual data when possible\n",
    "        start_idx = self.valid_indices[idx % len(self.valid_indices)]\n",
    "\n",
    "        # Get timestamps for input and output windows\n",
    "        input_timestamps = self.timestamps[start_idx:start_idx + self.seq_length]\n",
    "        output_timestamps = self.timestamps[start_idx + self.seq_length:\n",
    "                                            start_idx + self.seq_length + self.forecast_horizon]\n",
    "\n",
    "        # Handle potential shortfall in output window (fallback mode)\n",
    "        if self.fallback_mode and len(output_timestamps) < self.forecast_horizon:\n",
    "            # Pad with repetition of last timestamp if needed\n",
    "            last_time = output_timestamps[-1] if len(output_timestamps) > 0 else input_timestamps[-1]\n",
    "            padding = [last_time] * (self.forecast_horizon - len(output_timestamps))\n",
    "            output_timestamps = list(output_timestamps) + padding\n",
    "\n",
    "        # Initialize tensors - simpler now with just one station\n",
    "        X = np.zeros((len(self.feature_cols), 1, self.seq_length))  # Only one station\n",
    "        y = np.zeros((1, self.forecast_horizon))  # Only one station\n",
    "        static_features = np.zeros((1, 2))  # region_code and elevation\n",
    "\n",
    "        # LA Downtown is our only station\n",
    "        station_id = 'San Jose'\n",
    "        \n",
    "        # Input sequence\n",
    "        for t_idx, ts in enumerate(input_timestamps):\n",
    "            station_data = self.df[self.df['timestamp'] == ts]\n",
    "\n",
    "            if not station_data.empty:\n",
    "                for f_idx, feat in enumerate(self.feature_cols):\n",
    "                    X[f_idx, 0, t_idx] = station_data[feat].values[0]\n",
    "\n",
    "                # Store static features (same for all timestamps)\n",
    "                if t_idx == 0:\n",
    "                    static_features[0, 0] = station_data['region_code'].values[0]\n",
    "                    static_features[0, 1] = station_data['elevation_norm'].values[0]\n",
    "\n",
    "        # Target sequence (temperature only)\n",
    "        for t_idx, ts in enumerate(output_timestamps):\n",
    "            if t_idx < self.forecast_horizon:  # Safety check\n",
    "                station_data = self.df[self.df['timestamp'] == ts]\n",
    "\n",
    "                if not station_data.empty:\n",
    "                    y[0, t_idx] = station_data['Temperature_C'].values[0]\n",
    "\n",
    "        return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n",
    "# ============================================================\n",
    "# TEMPORAL FUSION TRANSFORMER IMPLEMENTATION\n",
    "# ============================================================\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention layer for temporal data.\n",
    "    Simplified from the original TFT paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=2, dropout=0.1):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        # Linear projections\n",
    "        queries = self.query(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = self.key(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        values = self.value(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "        # Final linear layer\n",
    "        return self.out(out)\n",
    "\n",
    "class GatedResidualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Residual Network as described in the TFT paper.\n",
    "    Simplified version with fewer layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
    "        super(GatedResidualNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # If input and output sizes are different, apply a skip connection\n",
    "        self.skip_layer = None\n",
    "        if input_size != output_size:\n",
    "            self.skip_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "        # Main layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.gate = nn.Linear(input_size + output_size, output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch\n",
    "        hidden = F.elu(self.fc1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "\n",
    "        # Skip connection\n",
    "        if self.skip_layer is not None:\n",
    "            skip = self.skip_layer(x)\n",
    "        else:\n",
    "            skip = x\n",
    "\n",
    "        # Gate mechanism\n",
    "        gate_input = torch.cat([x, hidden], dim=-1)\n",
    "        gate = torch.sigmoid(self.gate(gate_input))\n",
    "\n",
    "        # Combine using gate\n",
    "        output = gate * hidden + (1 - gate) * skip\n",
    "\n",
    "        # Layer normalization\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class VariableSelectionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Variable Selection Network for TFT.\n",
    "    Simplified version with fewer layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size_per_var, num_vars, hidden_size, output_size, dropout=0.1):\n",
    "        super(VariableSelectionNetwork, self).__init__()\n",
    "        self.input_size_per_var = input_size_per_var\n",
    "        self.num_vars = num_vars\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # GRN for variable weights\n",
    "        self.weight_grn = GatedResidualNetwork(\n",
    "            input_size=input_size_per_var * num_vars,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=num_vars,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # GRN for each variable\n",
    "        self.var_grns = nn.ModuleList([\n",
    "            GatedResidualNetwork(\n",
    "                input_size=input_size_per_var,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_vars)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_vars, input_size_per_var]\n",
    "        batch_size = x.size(0)\n",
    "        flat_x = x.view(batch_size, -1)\n",
    "\n",
    "        # Calculate variable weights\n",
    "        var_weights = self.weight_grn(flat_x)\n",
    "        var_weights = F.softmax(var_weights, dim=-1).unsqueeze(-1)  # [batch_size, num_vars, 1]\n",
    "\n",
    "        # Transform each variable\n",
    "        var_outputs = []\n",
    "        for i in range(self.num_vars):\n",
    "            var_outputs.append(self.var_grns[i](x[:, i]))\n",
    "\n",
    "        var_outputs = torch.stack(var_outputs, dim=1)  # [batch_size, num_vars, output_size]\n",
    "\n",
    "        # Weighted combination\n",
    "        outputs = torch.sum(var_outputs * var_weights, dim=1)  # [batch_size, output_size]\n",
    "\n",
    "        return outputs, var_weights\n",
    "\n",
    "class TemporalFusionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Fusion Transformer with single predictions (no quantiles).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_stations, hidden_size=64, num_heads=1, \n",
    "                 dropout=0.1, forecast_horizon=24, hidden_layers=2):\n",
    "        super(TemporalFusionTransformer, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_stations = num_stations\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Static variable processing\n",
    "        self.static_var_processor = GatedResidualNetwork(\n",
    "            input_size=2,  # region_code, elevation\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Variable selection for time-varying features\n",
    "        self.temporal_var_selection = VariableSelectionNetwork(\n",
    "            input_size_per_var=24,  # Sequence length per feature\n",
    "            num_vars=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # LSTM encoder layers\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size=hidden_size if i == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "                batch_first=True\n",
    "            ) for i in range(hidden_layers)\n",
    "        ])\n",
    "        \n",
    "        # Temporal self-attention\n",
    "        self.self_attention = TemporalSelfAttention(\n",
    "            d_model=hidden_size,\n",
    "            n_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Final output layer for forecasting (single prediction)\n",
    "        self.forecast_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, forecast_horizon)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Unpack inputs\n",
    "        temporal_features, static_features = inputs\n",
    "        batch_size = temporal_features.size(0)\n",
    "        \n",
    "        # [batch, features, stations, time] -> [batch*stations, features, time]\n",
    "        temporal_features = temporal_features.permute(0, 2, 1, 3)\n",
    "        temporal_features = temporal_features.reshape(batch_size * self.num_stations, self.num_features, -1)\n",
    "        \n",
    "        # Static features: [batch, stations, static_dims] -> [batch*stations, static_dims]\n",
    "        static_features = static_features.reshape(batch_size * self.num_stations, -1)\n",
    "        \n",
    "        # Process static features\n",
    "        static_embeddings = self.static_var_processor(static_features)\n",
    "        \n",
    "        # Process temporal features with variable selection\n",
    "        temporal_embeddings, temporal_weights = self.temporal_var_selection(temporal_features)\n",
    "        \n",
    "        # Reshape to [batch*stations, seq_len, hidden]\n",
    "        temporal_embeddings = temporal_embeddings.unsqueeze(1).expand(-1, 24, -1)\n",
    "        \n",
    "        # Add static embeddings to each timestep\n",
    "        temporal_embeddings = temporal_embeddings + static_embeddings.unsqueeze(1)\n",
    "        \n",
    "        # Pass through LSTM layers\n",
    "        lstm_out = temporal_embeddings\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            lstm_out, _ = lstm_layer(lstm_out)\n",
    "        \n",
    "        # Self-attention\n",
    "        attention_out = self.self_attention(lstm_out)\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = self.forecast_projection(attention_out)\n",
    "        \n",
    "        # Take the last timesteps for the forecast horizon\n",
    "        forecast = forecast[:, -self.forecast_horizon:, 0]\n",
    "        \n",
    "        # Reshape back to [batch, stations, horizon]\n",
    "        forecast = forecast.reshape(batch_size, self.num_stations, -1)\n",
    "        \n",
    "        return forecast\n",
    "\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "    \n",
    "def train_model(model, train_loader, val_loader, learning_rate=0.001, epochs=30, patience=6, use_checkpoint=True):\n",
    "    \"\"\"\n",
    "    Train the model with MSE loss and early stopping.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoint_path = 'checkpoint.pth'\n",
    "    # Add a model version identifier to prevent future mismatches\n",
    "    model_version = \"single_output_v1\"  # Change this when architecture changes\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Simple checkpoint loading\n",
    "    if os.path.exists(checkpoint_path) and use_checkpoint:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    \n",
    "    # Add mixed precision training with autocast\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f\"Starting training for {epochs} epochs with patience {patience}...\")\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move to device\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = F.mse_loss(outputs, targets)\n",
    "\n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "        avg_train_loss = train_loss / max(1, train_batches)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                # Move to device\n",
    "                if isinstance(inputs, tuple):\n",
    "                    inputs = tuple(x.to(device) for x in inputs)\n",
    "                elif isinstance(inputs, list):\n",
    "                    inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "                else:\n",
    "                    inputs = inputs.to(device)\n",
    "                    \n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = F.mse_loss(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / max(1, val_batches)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if use_checkpoint and avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'model_version': model_version\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  Saved best model with val loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "    \n",
    "    # Load best model\n",
    "    if use_checkpoint and os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded best model\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading best model: {e}\")\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, data_loader, station_ids, regions):\n",
    "    \"\"\"\n",
    "    Evaluate the model and calculate metrics with robust dimension handling.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            # Move to device - handle ALL possible input types\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision for evaluation\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            # Me i changed the median preds with preds instead \n",
    "            preds = outputs.detach().cpu().numpy()\n",
    "            targets_cpu = targets.numpy()\n",
    "            \n",
    "            # Append batch predictions and targets\n",
    "            all_predictions.append(preds)\n",
    "            all_actuals.append(targets_cpu)\n",
    "            \n",
    "            # Clear GPU cache periodically\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Process predictions in chunks to save memory\n",
    "    rmse_sum = 0\n",
    "    mae_sum = 0\n",
    "    r2_sum = 0\n",
    "    sample_count = 0\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    for preds, acts in zip(all_predictions, all_actuals):\n",
    "        # Flatten current batch\n",
    "        preds_flat = preds.flatten()  \n",
    "        acts_flat = acts.flatten()\n",
    "        \n",
    "        # Update metrics\n",
    "        rmse_sum += np.sum((preds_flat - acts_flat) ** 2)\n",
    "        mae_sum += np.sum(np.abs(preds_flat - acts_flat))\n",
    "        sample_count += len(preds_flat)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    rmse = np.sqrt(rmse_sum / sample_count)\n",
    "    mae = mae_sum / sample_count\n",
    "    \n",
    "    # For R², we need all data (this is an approximation)\n",
    "    all_preds_concat = np.concatenate([p.flatten() for p in all_predictions])\n",
    "    all_acts_concat = np.concatenate([a.flatten() for a in all_actuals])\n",
    "    r2 = r2_score(all_acts_concat, all_preds_concat)\n",
    "\n",
    "    print(f\"Overall Metrics - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # Since we only have one station San Jose, simplify the station metrics\n",
    "    station_metrics = {\n",
    "        station_ids[0]: {\n",
    "            'region': regions.get(station_ids[0], 'Unknown'),\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Station {station_ids[0]} ({regions.get(station_ids[0], 'Unknown')}) - \"\n",
    "          f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    return rmse, mae, r2, station_metrics\n",
    "\n",
    "def visualize_predictions(model, data_loader, station_ids, regions, season):\n",
    "    \"\"\"\n",
    "    Visualize predictions for each station.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if len(data_loader) == 0:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions\n",
    "    try:\n",
    "        for inputs, targets in data_loader:\n",
    "            # Only process one batch for visualization\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Move to CPU for plotting\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.numpy()\n",
    "            break\n",
    "\n",
    "        # Check if we have data to plot\n",
    "        if 'outputs' not in locals():\n",
    "            print(\"No data was loaded from the dataloader\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing visualization data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots for each station\n",
    "    fig, axes = plt.subplots(len(station_ids), 1, figsize=(12, 3*len(station_ids)))\n",
    "    if len(station_ids) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    hours = np.arange(24)\n",
    "\n",
    "    for i, station in enumerate(station_ids):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Plot actual vs predicted\n",
    "        ax.plot(hours, targets[0, i, :], 'b-', label='Actual')\n",
    "        ax.plot(hours, outputs[0, i, :], 'r--', label='Predicted')\n",
    "\n",
    "        ax.set_title(f\"{station} ({regions.get(station, 'Unknown')}) - {season}\")\n",
    "        ax.set_xlabel('Hour of Day')\n",
    "        ax.set_ylabel('Temperature (°C)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{season}_predictions.png\")\n",
    "    plt.show()  # Add this line to display the plot\n",
    "    plt.close()\n",
    "\n",
    "def predict_day_temperatures(model, day_data, feature_cols):\n",
    "    \"\"\"\n",
    "    Predict temperatures for a single day (24 hours).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained TFT model\n",
    "        day_data: DataFrame with data for the target day\n",
    "        feature_cols: List of feature columns used by the model\n",
    "        \n",
    "    Returns:\n",
    "        Predicted temperatures for 24 hours\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input data format (same as in create_seasonal_datasets)\n",
    "    try:\n",
    "        # Format data for model input\n",
    "        temporal_data = np.zeros((1, len(feature_cols), 1, 24))  # [batch=1, features, stations=1, time=24]\n",
    "        \n",
    "        # Fill temporal features\n",
    "        for f_idx, feat in enumerate(feature_cols):\n",
    "            if feat in day_data.columns:\n",
    "                temporal_data[0, f_idx, 0, :] = day_data[feat].values[:24]\n",
    "        \n",
    "        # Create static features\n",
    "        static_data = np.zeros((1, 1, 2))  # [batch=1, stations=1, static_features=2]\n",
    "        static_data[0, 0, 0] = day_data['region_code'].iloc[0] \n",
    "        static_data[0, 0, 1] = day_data['elevation_norm'].iloc[0]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_temporal = torch.FloatTensor(temporal_data).to(device)\n",
    "        X_static = torch.FloatTensor(static_data).to(device)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = model((X_temporal, X_static))\n",
    "            \n",
    "        # Return prediction as numpy array\n",
    "        return prediction.cpu().numpy()[0, 0, :]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting temperatures: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "def plot_temperature_prediction(actual, predicted, date_str=None):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted temperatures.\n",
    "    \n",
    "    Args:\n",
    "        actual: Numpy array of actual temperatures (24 hours)\n",
    "        predicted: Numpy array of predicted temperatures (24 hours)\n",
    "        date_str: String representation of the date\n",
    "    \"\"\"\n",
    "    hours = np.arange(24)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(hours, actual, 'b-o', label='Actual Temperature', linewidth=2)\n",
    "    plt.plot(hours, predicted, 'r-o', label='Predicted Temperature', linewidth=2)\n",
    "    \n",
    "    title = f\"Daily Temperature Forecast\" \n",
    "    if date_str:\n",
    "        title += f\" for {date_str}\"\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Hour of Day', fontsize=14)\n",
    "    plt.ylabel('Temperature (°C)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.xticks(hours)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"temperature_forecast_{date_str.replace(' ', '_').replace(':', '-')}.png\" if date_str else \"temperature_forecast.png\"\n",
    "    plt.show()\n",
    "\n",
    "def create_mirror_plot(model, data_loader, station_ids, regions, season):\n",
    "    \"\"\"\n",
    "    Create a mirror plot with actual temperature on left side and predicted on right side.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if len(data_loader) == 0:\n",
    "        print(f\"No data available for {season} mirror plot\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions\n",
    "    try:\n",
    "        for inputs, targets in data_loader:\n",
    "            # Move inputs to device - handle ALL possible input types\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs = tuple(x.to(device) for x in inputs)\n",
    "            elif isinstance(inputs, list):\n",
    "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Move to CPU for plotting\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            targets = targets.numpy()\n",
    "            break\n",
    "            \n",
    "        if 'outputs' not in locals():\n",
    "            print(f\"No data was loaded for {season} mirror plot\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing mirror plot data for {season}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print full error details\n",
    "        return\n",
    "    \n",
    "    # Create a figure for each station\n",
    "    for i, station in enumerate(station_ids):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "        \n",
    "        hours = np.arange(24)\n",
    "        \n",
    "        # Left plot - Actual temperatures\n",
    "        ax1.plot(hours, targets[0, i, :], 'b-o', linewidth=2)\n",
    "        ax1.set_title(f\"Actual Temperature - {season}\", fontsize=14)\n",
    "        ax1.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax1.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "        ax1.set_xlim(0, 23)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.invert_xaxis()  # Invert x-axis for mirror effect\n",
    "        \n",
    "        # Right plot - Predicted temperatures\n",
    "        ax2.plot(hours, outputs[0, i, :], 'r-o', linewidth=2)\n",
    "        ax2.set_title(f\"Predicted Temperature - {season}\", fontsize=14)\n",
    "        ax2.set_xlabel('Hour of Day', fontsize=12)\n",
    "        ax2.set_xlim(0, 23)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add overall title\n",
    "        plt.suptitle(f\"{station} ({regions.get(station, 'Unknown')}) - {season} Day Comparison\", \n",
    "                    fontsize=16, y=1.05)\n",
    "        \n",
    "        # Add a line in the middle\n",
    "        fig.tight_layout()\n",
    "        plt.subplots_adjust(wspace=0.05)  # Reduce space between subplots\n",
    "        \n",
    "        # Save and display\n",
    "        plt.savefig(f\"{season}_mirror_comparison.png\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def create_seasonal_datasets(df, target_days, all_stations, feature_cols):\n",
    "    \"\"\"\n",
    "    Create datasets for each seasonal day and visualize predictions.\n",
    "    \"\"\"\n",
    "    seasonal_datasets = {}\n",
    "    \n",
    "    for season, day_data in target_days.items():\n",
    "        if day_data.empty:\n",
    "            print(f\"No data available for {season}\")\n",
    "            continue\n",
    "            \n",
    "        # Get the date of this target day\n",
    "        sample_date = day_data['timestamp'].iloc[0]\n",
    "        print(f\"Creating dataset for {season}: {sample_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare the data in the format expected by the model\n",
    "            # Format should be: temporal_features [batch, features, stations, time], static_features [batch, stations, static_features]\n",
    "            \n",
    "            # Reshape features to match expected format\n",
    "            temporal_data = np.zeros((1, len(feature_cols), 1, 24))  # [batch=1, features, stations=1, time=24]\n",
    "            \n",
    "            # Fill in the temporal features\n",
    "            for f_idx, feat in enumerate(feature_cols):\n",
    "                if feat in day_data.columns:\n",
    "                    temporal_data[0, f_idx, 0, :] = day_data[feat].values[:24]  # Using first 24 records\n",
    "            \n",
    "            # Create static features (region_code and elevation)\n",
    "            static_data = np.zeros((1, 1, 2))  # [batch=1, stations=1, static_features=2]\n",
    "            static_data[0, 0, 0] = day_data['region_code'].iloc[0] \n",
    "            static_data[0, 0, 1] = day_data['elevation_norm'].iloc[0]\n",
    "            \n",
    "            # Create target (actual temperatures)\n",
    "            target_data = np.zeros((1, 1, 24))  # [batch=1, stations=1, time=24]\n",
    "            target_data[0, 0, :] = day_data['Temperature_C'].values[:24]  # Using first 24 records\n",
    "            \n",
    "            # Convert to tensors\n",
    "            X_temporal = torch.FloatTensor(temporal_data)\n",
    "            X_static = torch.FloatTensor(static_data)\n",
    "            y = torch.FloatTensor(target_data)\n",
    "            \n",
    "            # Store as a list containing a single data point\n",
    "            seasonal_datasets[season] = [((X_temporal, X_static), y)]\n",
    "            print(f\"Created {season} dataset with shapes: X_temporal={X_temporal.shape}, X_static={X_static.shape}, y={y.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating {season} dataset: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return seasonal_datasets\n",
    "\n",
    "\n",
    "def analyze_topographic_performance(station_metrics, regions):\n",
    "    \"\"\"\n",
    "    Analyze model performance across different topographic regions.\n",
    "    \"\"\"\n",
    "    # Group metrics by region\n",
    "    region_metrics = {}\n",
    "    for station, metrics in station_metrics.items():\n",
    "        region = regions.get(station, 'Unknown')\n",
    "        if region not in region_metrics:\n",
    "            region_metrics[region] = []\n",
    "        region_metrics[region].append(metrics)\n",
    "\n",
    "    # Calculate average metrics by region\n",
    "    region_avg_metrics = {}\n",
    "    for region, metrics_list in region_metrics.items():\n",
    "        avg_rmse = np.mean([m['rmse'] for m in metrics_list])\n",
    "        avg_mae = np.mean([m['mae'] for m in metrics_list])\n",
    "        avg_r2 = np.mean([m['r2'] for m in metrics_list])\n",
    "\n",
    "        region_avg_metrics[region] = {\n",
    "            'avg_rmse': avg_rmse,\n",
    "            'avg_mae': avg_mae,\n",
    "            'avg_r2': avg_r2\n",
    "        }\n",
    "\n",
    "        print(f\"Region {region} - Avg RMSE: {avg_rmse:.4f}, Avg MAE: {avg_mae:.4f}, Avg R²: {avg_r2:.4f}\")\n",
    "\n",
    "        if not station_metrics:\n",
    "            print(\"No station metrics available for analysis\")\n",
    "            return {}\n",
    "\n",
    "    # Create bar chart comparing regions\n",
    "    regions = list(region_avg_metrics.keys())\n",
    "    rmse_values = [region_avg_metrics[r]['avg_rmse'] for r in regions]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(regions, rmse_values)\n",
    "\n",
    "    # Add styling\n",
    "    plt.title('RMSE by Topographic Region', fontsize=16)\n",
    "\n",
    "    plt.title('RMSE by Topographic Region', fontsize=16)\n",
    "    plt.ylabel('RMSE (°C)', fontsize=14)\n",
    "    plt.xlabel('Region', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'{height:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig('region_performance.png')\n",
    "    plt.close()\n",
    "\n",
    "    return region_avg_metrics\n",
    "\n",
    "def analyze_seasonal_performance(seasonal_results):\n",
    "    \"\"\"\n",
    "    Compare model performance across different seasons.\n",
    "    \"\"\"\n",
    "    seasons = list(seasonal_results.keys())\n",
    "    rmse_values = [results['rmse'] for results in seasonal_results.values()]\n",
    "    mae_values = [results['mae'] for results in seasonal_results.values()]\n",
    "\n",
    "    if not seasonal_results:\n",
    "        print(\"No seasonal results available for analysis\")\n",
    "        return [], [], []\n",
    "\n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(seasons))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width/2, rmse_values, width, label='RMSE')\n",
    "    ax.bar(x + width/2, mae_values, width, label='MAE')\n",
    "\n",
    "    ax.set_title('Model Performance by Season', fontsize=16)\n",
    "    ax.set_ylabel('Error (°C)', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(seasons)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(rmse_values):\n",
    "        ax.text(i - width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "    for i, v in enumerate(mae_values):\n",
    "        ax.text(i + width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig('seasonal_performance.png')\n",
    "    plt.close()\n",
    "\n",
    "    return seasons, rmse_values, mae_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ed968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vmd_modes(decomposed_data, station_id='San Jose', feature='Temperature_C'):\n",
    "    \"\"\"\n",
    "    Visualize ALL VMD decomposition modes split into two columns.\n",
    "    Left column: First half of modes\n",
    "    Right column: Second half of modes\n",
    "    \"\"\"\n",
    "    if station_id not in decomposed_data or feature not in decomposed_data[station_id]:\n",
    "        print(f\"No VMD data available for station {station_id}, feature {feature}\")\n",
    "        return\n",
    "    \n",
    "    # Get the VMD modes\n",
    "    modes = decomposed_data[station_id][feature]\n",
    "    total_modes = modes.shape[0]\n",
    "    \n",
    "    # Split modes into two halves\n",
    "    half_point = (total_modes + 1) // 2  # Handles odd numbers of modes\n",
    "    modes_first_half = modes[:half_point, :]\n",
    "    modes_second_half = modes[half_point:, :]\n",
    "    \n",
    "    # Create figure with two columns\n",
    "    fig, axes = plt.subplots(max(len(modes_first_half), len(modes_second_half)), 2, \n",
    "                          figsize=(16, 2*max(len(modes_first_half), len(modes_second_half))))\n",
    "    \n",
    "    # If only one row, ensure axes is 2D array\n",
    "    if axes.ndim == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Plot first half in left column\n",
    "    for i in range(len(modes_first_half)):\n",
    "        ax = axes[i, 0]\n",
    "        ax.plot(modes_first_half[i, :], 'b-')\n",
    "        ax.set_title(f\"VMD Mode {i+1}\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if i == len(modes_first_half) // 2:\n",
    "            ax.set_ylabel(\"Amplitude\")\n",
    "    \n",
    "    # Plot second half in right column\n",
    "    for i in range(len(modes_second_half)):\n",
    "        ax = axes[i, 1]\n",
    "        ax.plot(modes_second_half[i, :], 'b-')\n",
    "        ax.set_title(f\"VMD Mode {i+1+half_point}\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if i == len(modes_second_half) // 2:\n",
    "            ax.set_ylabel(\"Amplitude\")\n",
    "    \n",
    "    # Hide empty subplots if odd number of modes\n",
    "    if len(modes_first_half) > len(modes_second_half):\n",
    "        axes[-1, 1].axis('off')\n",
    "    \n",
    "    # Add labels\n",
    "    for ax in axes[-1, :]:\n",
    "        ax.set_xlabel(\"Time Steps\")\n",
    "    \n",
    "    plt.suptitle(f\"VMD Decomposition - {station_id} - {feature}\\nLeft: Modes 1-{half_point} | Right: Modes {half_point+1}-{total_modes}\", \n",
    "                fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"vmd_modes_split_{station_id}_{feature}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary (unchanged from original)\n",
    "    print(f\"\\nVMD Decomposition Summary for {station_id} - {feature}:\")\n",
    "    print(f\"Total number of modes: {total_modes}\")\n",
    "    for i in range(total_modes):\n",
    "        mode_energy = np.sum(modes[i, :]**2)\n",
    "        print(f\"Mode {i+1} - Energy: {mode_energy:.2f}\")\n",
    "\n",
    "\n",
    "def run_temperature_prediction(data_file, target_date=None):\n",
    "    \"\"\"\n",
    "    Run the VMD-TFT temperature prediction pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_file: Path to weather data file\n",
    "        target_date: Optional datetime object for prediction target \n",
    "                     (if None, will use the latest available date in data)\n",
    "    \"\"\"\n",
    "    print(\"Starting VMD-TFT temperature prediction pipeline...\")\n",
    "    \n",
    "    df = load_data(data_file)\n",
    "    print(f\"Loaded data with {len(df)} records\")\n",
    "    if df.empty:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Split data by years\n",
    "    train_df, val_df, test_df = split_data_temporal(df)\n",
    "    \n",
    "    # Define feature columns for the model\n",
    "    feature_cols = [\n",
    "    'Temperature_C',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlyStationPressure',\n",
    "    'hour_sin', 'hour_cos',\n",
    "    'day_sin', 'day_cos'\n",
    "    ]\n",
    "    \n",
    "    # Perform VMD decomposition on temperature data\n",
    "    print(\"Performing VMD decomposition...\")\n",
    "    decomposed_data = parallel_vmd_decomposition(df, feature_cols_to_decompose=['Temperature_C'])\n",
    "    \n",
    "    # Visualize VMD modes for Temperature_C\n",
    "    print(\"\\n==== VMD Modes Visualization ====\")\n",
    "    visualize_vmd_modes(decomposed_data, 'San Jose', 'Temperature_C')\n",
    "    print(\"==== End of VMD Visualization ====\\n\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = WeatherDataset(train_df, ['San Jose'], feature_cols, seq_length=24, forecast_horizon=24)\n",
    "    val_dataset = WeatherDataset(val_df, ['San Jose'], feature_cols, seq_length=24, forecast_horizon=24)\n",
    "    test_dataset = WeatherDataset(test_df, ['San Jose'], feature_cols, seq_length=24, forecast_horizon=24)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    model = TemporalFusionTransformer(\n",
    "    num_features=len(feature_cols),\n",
    "    num_stations=1,  # Just San Jose\n",
    "    hidden_size=128,  # Increased from 64\n",
    "    num_heads=2,     # Increased from 1\n",
    "    dropout=0.1,\n",
    "    forecast_horizon=24,\n",
    "    hidden_layers=2\n",
    ")\n",
    "\n",
    "    model, train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    learning_rate=0.0003,\n",
    "    epochs=30,\n",
    "    patience=10,\n",
    "    use_checkpoint=True  # Turn off for this run only\n",
    ")\n",
    "        # Plot training and validation loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', marker='o')\n",
    "    plt.title(\"Training vs Validation Loss over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Evaluate model\n",
    "    rmse, mae, r2, station_metrics = evaluate_model(model, test_loader, ['San Jose'], {'San Jose': 'urban'})\n",
    "    \n",
    "    # Predict for target date\n",
    "    if target_date is None:\n",
    "        # Use last day in dataset\n",
    "        target_date = df['timestamp'].max().date()\n",
    "        print(f\"No target date specified. Using last day in dataset: {target_date}\")\n",
    "    \n",
    "    # Filter data for target day\n",
    "    target_day_data = df[df['timestamp'].dt.date == target_date].copy()\n",
    "    \n",
    "    if target_day_data.empty:\n",
    "        print(f\"No data available for date {target_date}\")\n",
    "        return\n",
    "        \n",
    "    # Ensure we have 24 hours of data\n",
    "    if len(target_day_data) < 24:\n",
    "        print(f\"Warning: Only {len(target_day_data)} hours available for {target_date}. Need 24 hours.\")\n",
    "        return\n",
    "        \n",
    "    # Make prediction\n",
    "    print(f\"Predicting temperatures for {target_date}...\")\n",
    "    predicted_temps = predict_day_temperatures(model, target_day_data, feature_cols)\n",
    "    \n",
    "    if predicted_temps is not None:\n",
    "        # Get actual temperatures\n",
    "        actual_temps = target_day_data['Temperature_C'].values[:24]\n",
    "        \n",
    "        # Plot results\n",
    "        plot_temperature_prediction(actual_temps, predicted_temps, str(target_date))\n",
    "        \n",
    "        # Print prediction results\n",
    "        print(\"\\nPrediction Results:\")\n",
    "        print(f\"{'Hour':<5} {'Actual':<10} {'Predicted':<10} {'Difference':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        for hour in range(24):\n",
    "            diff = predicted_temps[hour] - actual_temps[hour]\n",
    "            print(f\"{hour:<5} {actual_temps[hour]:<10.2f} {predicted_temps[hour]:<10.2f} {diff:<10.2f}\")\n",
    "            \n",
    "        # Calculate error metrics for this prediction\n",
    "        day_mse = np.mean((predicted_temps - actual_temps) ** 2)\n",
    "        day_rmse = np.sqrt(day_mse)\n",
    "        day_mae = np.mean(np.abs(predicted_temps - actual_temps))\n",
    "        \n",
    "        print(f\"\\nDay prediction errors - RMSE: {day_rmse:.2f}°C, MAE: {day_mae:.2f}°C\")\n",
    "\n",
    "\n",
    "# Execute the pipeline when run as a script\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import r2_score\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    # Set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Import VMD implementation (assumed to be available)\n",
    "    try:\n",
    "        from vmdpy import VMD\n",
    "    except ImportError:\n",
    "        print(\"WARNING: VMD package not found. Please install vmdpy package for VMD decomposition.\")\n",
    "        \n",
    "        # Define a simple stub for VMD function that returns random data for demo purposes\n",
    "        def VMD(signal, alpha, tau, K, DC, init, tol):\n",
    "            \"\"\"Stub implementation of VMD for testing\"\"\"\n",
    "            u = np.random.randn(K, len(signal))\n",
    "            u_hat = np.random.randn(K, len(signal))\n",
    "            omega = np.random.randn(K)\n",
    "            return u, u_hat, omega\n",
    "    \n",
    "    c = r\"C:\\Users\\hu4227mo-s\\OneDrive - Lund University\\Merged_Data.csv\"\n",
    "    run_temperature_prediction(c, datetime(2024, 10, 10).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ac42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f79f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748dcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
